journal,year,title,authors,abstract,url
Journal of Econometrics,2024,Testing specification of distribution in stochastic frontier analysis,"Ming-Yen Cheng, Shouxia Wang, Lucy Xia and Xibin Zhang","Stochastic frontier analysis is regularly used in empirical studies to evaluate the productivity and efficiency of companies. A typical stochastic frontier model involves a parametric frontier subject to a composite error term consisting of an inefficiency and a random error. We develop new tests for specification of distribution of the inefficiency. We focus on simultaneous relaxation of two common assumptions: (1) parametric frontier which may lead to false conclusions when misspecified, and (2) homoscedasticity which can be easily violated when working with real data. While these two issues have been extensively studied in prior research exploring the estimation of a stochastic frontier and inefficiencies, they have not been properly addressed in the considered testing problem. We propose novel bootstrap and asymptotic distribution-free tests with neither parametric frontier nor homoscedasticity assumptions, in both cross-sectional and panel settings. Our tests are asymptotically consistent, simple to implement and widely applicable. Their powers against general fixed alternatives tend to one as sample size increases, and they can detect root-n order local alternatives. We demonstrate their efficacies through extensive simulation studies. When applied to a banking panel dataset, our tests provide sound justification for the commonly used exponential specification for banking data. The findings also show that a new parametric frontier model is more plausible than the conventional translog frontier.",http://www.sciencedirect.com/science/article/pii/S0304407622000677
Journal of Econometrics,2024,Testing equality of several distributions in separable metric spaces: A maximum mean discrepancy based approach,"Jin-Ting Zhang, Jia Guo and Bu Zhou","A new test for equal distributions of several high-dimensional samples in separable metric spaces, with its test statistic constructed based on maximum mean discrepancy, is proposed and studied. The asymptotic null and alternative distributions of the test statistic are established under some mild conditions. The new test is implemented via a three-cumulant matched chi-square approximation with the associated approximation parameters consistently estimated from the data. A new data-adaptive Gaussian kernel width selection method is also suggested. Good performance of the new test is illustrated by intensive simulation studies and a real data example of Gini index curves.",http://www.sciencedirect.com/science/article/pii/S0304407622000859
Journal of Econometrics,2024,Asset splitting algorithm for ultrahigh dimensional portfolio selection and its theoretical property,"Zhanrui Cai, Changcheng Li, Jiawei Wen and Songshan Yang","The presence of a huge number of assets poses challenges to classical portfolio selection algorithms. Constrained l1 minimization approaches have been proposed to directly estimate effective parameters in the optimal portfolio. Linear programming method and alternating direction method of multiplier (ADMM) algorithm is used to solve the corresponding minimization problems. However, these two algorithms may fail due to the limitations of computing time and computing memory when a huge number of assets are considered in the portfolio optimization. This article proposes an asset splitting ADMM (AS-ADMM for short), a parallel computing algorithm, to tackle such challenges, and establishes the convergence property of the new algorithm. Furthermore, we develop a new regularization method for estimating the effective parameters with the folded-concave penalty and establish its oracle property. The local linear approximation (LLA) algorithm is used to redirect the new method to a weighted l1 regularization method. We conduct simulation studies to investigate the advantage of the proposed algorithm and regularized model in solving the high dimensional portfolio selection problems. A real data example is also included to demonstrate the applicability of the proposed algorithms and regularization methods.",http://www.sciencedirect.com/science/article/pii/S0304407622000902
Journal of Econometrics,2024,A Multi-Kink quantile regression model with common structure for panel data analysis,"Yan Sun, Chuang Wan, Wenyang Zhang and Wei Zhong","Stimulated by the analysis of a data set on financial portfolio returns, we propose a multi-kink quantile regression (MKQR) model with latent homogeneous structure for panel data analysis. The proposed model accounts for both homogeneity and heterogeneity among individuals and parameters in panel data analysis. From statistical modeling point of view, it well balances the risk of misspecification and the model parsimony. From practical point of view, it is able to reveal not only the impacts of covariates in the global sense, but also individual attributes. An estimation procedure is presented to estimate both the unknown parameters and the latent homogeneous structure in the proposed model. Computational issues with the implementation of the estimation procedure are also discussed. Asymptotic theory of the estimators is established. It shows the necessity of taking into account both homogeneity and heterogeneity in panel data analysis. Monte Carlo simulation studies are conducted to demonstrate the finite sample performance of the proposed estimation and the risk of ignoring the homogeneity or heterogeneity among individuals. Finally, we apply the proposed model and the estimation procedure to the data set which stimulates this work and reveal some interesting findings.",http://www.sciencedirect.com/science/article/pii/S0304407622001178
Journal of Econometrics,2024,Optimal covariance matrix estimation for high-dimensional noise in high-frequency data,"Jinyuan Chang, Qiao Hu, Cheng Liu and Cheng Yong Tang","We consider high-dimensional measurement errors with high-frequency data. Our objective is on recovering the high-dimensional cross-sectional covariance matrix of the random errors with optimality. In this problem, not all components of the random vector are observed at the same time and the measurement errors are latent variables, leading to major challenges besides high data dimensionality. We propose a new covariance matrix estimator in this context with appropriate localization and thresholding, and then conduct a series of comprehensive theoretical investigations of the proposed estimator. By developing a new technical device integrating the high-frequency data feature with the conventional notion of α-mixing, our analysis successfully accommodates the challenging serial dependence in the measurement errors. Our theoretical analysis establishes the minimax optimal convergence rates associated with two commonly used loss functions; and we demonstrate with concrete cases when the proposed localized estimator with thresholding achieves the minimax optimal convergence rates. Considering that the variances and covariances can be small in reality, we conduct a second-order theoretical analysis that further disentangles the dominating bias in the estimator. A bias-corrected estimator is then proposed to ensure its practical finite sample performance. We also extensively analyze our estimator in the setting with jumps, and show that its performance is reasonably robust. We illustrate the promising empirical performance of the proposed estimator with extensive simulation studies and a real data analysis.",http://www.sciencedirect.com/science/article/pii/S0304407622001543
Journal of Econometrics,2024,A generalized knockoff procedure for FDR control in structural change detection,"Jingyuan Liu, Ao Sun and Yuan Ke","Controlling false discovery rate (FDR) is crucial for variable selection, multiple testing, among other signal detection problems. In literature, there is certainly no shortage of FDR control strategies when selecting individual features, but the relevant works for structural change detection, such as profile analysis for piecewise constant coefficients and integration analysis with multiple data sources, are limited. In this paper, we propose a generalized knockoff procedure (GKnockoff) for FDR control under such problem settings. We prove that the GKnockoff possesses pairwise exchangeability, and is capable of controlling the exact FDR under finite sample sizes. We further explore GKnockoff under high dimensionality, by first introducing a new screening method to filter the high-dimensional potential structural changes. We adopt a data splitting technique to first reduce the dimensionality via screening and then conduct GKnockoff on the refined selection set. Furthermore, the powers of proposed methods are systematically studied. Numerical comparisons with other methods show the superior performance of GKnockoff, in terms of both FDR control and power. We also implement the proposed methods to analyze a macroeconomic dataset for detecting changes of driven effects of economic development on the secondary industry.",http://www.sciencedirect.com/science/article/pii/S0304407622001567
Journal of Econometrics,2024,Time-varying minimum variance portfolio,"Qingliang (Michael) Fan, Ruike Wu, Yanrong Yang and Wei Zhong","This paper proposes a new time-varying minimum variance portfolio (TV-MVP) in a large investment universe of assets. Our method extends the existing literature on minimum variance portfolios by allowing for time-varying factor loadings, which facilitates the capture of the dynamics of the covariance structure of asset returns (and hence, the optimal investment strategy in a dynamic setting). We also use a shrinkage estimation method based on a quasi-likelihood function to regularize the residual covariances further. We establish the desired theoretical properties of proposed time-varying covariance and the optimal portfolio estimators under a more realistic heavy-tailed distribution. Specifically, we provide consistency of the optimal Sharpe ratio of the TV-MVP and the sharp risk consistency. Moreover, we offer a test of constant covariance structure and show the asymptotic distribution of the test statistic. Simulation and empirical studies suggest that the performance of the proposed TV-MVP is superior, in terms of estimation accuracy and out-of-sample Sharpe ratio, compared with that of other popular contemporary methods.",http://www.sciencedirect.com/science/article/pii/S0304407622001646
Journal of Econometrics,2024,A latent class Cox model for heterogeneous time-to-event data,"Youquan Pei, Heng Peng and Jinfeng Xu","Credit risk plays a vital role in the era of digital finance and it is one of primary interests to identify customers with similar types of risk categories so that personalized financial services can be offered accordingly. Motivated by the bourgeoning need for default risk modeling in finance, we propose herein a latent class Cox model for heterogeneous time-to-event data. The proposed model naturally extends the Cox proportional hazards model to flexibly take into account the heterogeneity of covariate effects as often manifested in real data. Without a priori specification of the number of latent classes, it simultaneously incorporates the commonalities and disparities of individual customers’ risk behaviors and provides a more refined modeling technique than existing approaches. We further propose a penalized maximum likelihood approach to identify the number of latent classes and estimate the model parameters. A modified expectation–maximization algorithm is then developed for its numerical implementation. Simulation studies are conducted to assess the finite-sample performance of the proposed approach. Its illustration with a real credit card data set is also provided.",http://www.sciencedirect.com/science/article/pii/S0304407622001841
Journal of Econometrics,2024,A post-screening diagnostic study for ultrahigh dimensional data,"Yaowu Zhang, Yeqing Zhou and Liping Zhu","We propose a consistent lack-of-fit test to assess whether replacing the original ultrahigh dimensional covariates with a given number of linear combinations results in a loss of regression information. To attenuate the spurious correlations that may inflate type-I error rates in high dimensions, we suggest to randomly split the observations into two parts. In the first part, we screen out as many irrelevant covariates as possible. This screening step helps to reduce the ultrahigh dimensionality to a moderate scale. In the second part, we perform a lack-of-fit test for conditional independence in the context of sufficient dimension reduction. In case that some important covariates are missed with a non-ignorable probability in the first screening stage, we introduce a multiple splitting procedure. We further propose a new statistic to test for conditional independence, which is shown to be n-consistent under the null and root-n-consistent under the alternative. We develop a consistent bootstrap procedure to approximate the asymptotic null distribution. The performances of our proposal are evaluated through comprehensive simulations and an empirical analysis of GDP data.",http://www.sciencedirect.com/science/article/pii/S0304407622001877
Journal of Econometrics,2024,Mixed membership estimation for social networks,"Jiashun Jin, Zheng Tracy Ke and Shengming Luo","In economics and social science, network data are regularly observed, and a thorough understanding of the network community structure facilitates the comprehension of economic patterns and activities. Consider an undirected network with n nodes and K communities. We model the network using the Degree-Corrected Mixed-Membership (DCMM) model, where for each node i=1,2,…,n, there exists a membership vector πi=(πi(1),πi(2),…,πi(K))′, where πi(k) is the weight that node i puts on community k, 1≤k≤K. In comparison to the well-known stochastic block model (SBM), the DCMM permits both severe degree heterogeneity and mixed memberships, making it more realistic and general. We present an efficient approach, Mixed-SCORE, for estimating the mixed membership vectors of all nodes and the other DCMM parameters. This approach is inspired by the discovery of a delicate simplex structure in the spectral domain. We derive explicit error rates for the Mixed-SCORE algorithm and demonstrate that it is rate-optimal over a broad parameter space. Our findings provide a novel statistical tool for network community analysis, which can be used to understand network formations, extract nodal features, identify unobserved covariates in dyadic regressions, and estimate peer effects. We applied Mixed-SCORE to a political blog network, two trade networks, a co-authorship network, and a citee network, and obtained interpretable results.",http://www.sciencedirect.com/science/article/pii/S0304407622002081
Journal of Econometrics,2024,An autocovariance-based learning framework for high-dimensional functional time series,"Jinyuan Chang, Cheng Chen, Xinghao Qiao and Qiwei Yao","Many scientific and economic applications involve the statistical learning of high-dimensional functional time series, where the number of functional variables is comparable to, or even greater than, the number of serially dependent functional observations. In this paper, we model observed functional time series, which are subject to errors in the sense that each functional datum arises as the sum of two uncorrelated components, one dynamic and one white noise. Motivated from the fact that the autocovariance function of observed functional time series automatically filters out the noise term, we propose a three-step framework by first performing autocovariance-based dimension reduction, then formulating a novel autocovariance-based block regularized minimum distance estimation to produce block sparse estimates, and based on which obtaining the final functional sparse estimates. We investigate theoretical properties of the proposed estimators, and illustrate the proposed estimation procedure with the corresponding convergence analysis via three sparse high-dimensional functional time series models. We demonstrate via both simulated and real datasets that our proposed estimators significantly outperform their competitors.",http://www.sciencedirect.com/science/article/pii/S0304407623000167
Journal of Econometrics,2024,Mining the factor zoo: Estimation of latent factor models with sufficient proxies,"Runzhe Wan, Yingying Li, Wenbin Lu and Rui Song","Latent factor model estimation typically relies on either using domain knowledge to manually pick several observed covariates as factor proxies, or purely conducting multivariate analysis such as principal component analysis. However, the former approach may suffer from the bias while the latter cannot incorporate additional information. We propose to bridge these two approaches while allowing the number of factor proxies to diverge, and hence make the latent factor model estimation robust, flexible, and statistically more accurate. As a bonus, the number of factors is also allowed to grow. At the heart of our method is a penalized reduced rank regression to combine information. To further deal with heavy-tailed data, a computationally attractive penalized robust reduced rank regression method is proposed. We establish faster rates of convergence compared with the benchmark. Extensive simulations and real examples are used to illustrate the advantages.",http://www.sciencedirect.com/science/article/pii/S0304407623000179
Journal of Econometrics,2024,Robustifying Markowitz,"Alla Petukhina, Yegor Klochkov, Wolfgang Karl Härdle and Nikita Zhivotovskiy","Markowitz mean–variance portfolios with sample mean and covariance as input parameters feature numerous issues in practice. They perform poorly out of sample due to estimation error, they experience extreme weights together with high sensitivity to change in input parameters. The heavy-tail characteristics of financial time series are in fact the cause for these erratic fluctuations of weights that consequently create substantial transaction costs. In robustifying the weights we present a toolbox for stabilizing costs and weights for global minimum Markowitz portfolios. Utilizing a projected gradient descent (PGD) technique, we avoid the estimation and inversion of the covariance operator as a whole and concentrate on robust estimation of the gradient descent increment. Using modern tools of robust statistics we construct a computationally efficient estimator with almost Gaussian properties based on median-of-means uniformly over weights. This robustified Markowitz approach is confirmed by empirical studies on equity markets. We demonstrate that robustified portfolios reach the lowest turnover compared to shrinkage-based and constrained portfolios while preserving or slightly improving out-of-sample performance.",http://www.sciencedirect.com/science/article/pii/S0304407623000180
Journal of Econometrics,2024,"Spherical autoregressive models, with application to distributional and compositional time series",Changbo Zhu and Hans-Georg Müller,"We introduce a new class of autoregressive models for spherical time series. The dimension of the spheres on which the observations of the time series are situated may be finite-dimensional or infinite-dimensional, where in the latter case we consider the Hilbert sphere. Spherical time series arise in various settings. We focus here on distributional and compositional time series. Applying a square root transformation to the densities of the observations of a distributional time series maps the distributional observations to the Hilbert sphere, equipped with the Fisher–Rao metric. Likewise, applying a square root transformation to the components of the observations of a compositional time series maps the compositional observations to a finite-dimensional sphere, equipped with the geodesic metric on spheres. The challenge in modeling such time series lies in the intrinsic non-linearity of spheres and Hilbert spheres, where conventional arithmetic operations such as addition or scalar multiplication are no longer available. To address this difficulty, we consider rotation operators to map observations on the sphere. Specifically, we introduce a class of skew-symmetric operators such that the associated exponential operators are rotation operators that for each given pair of points on the sphere map the first point of the pair to the second point of the pair. We exploit the fact that the space of skew-symmetric operators is Hilbertian to develop autoregressive modeling of geometric differences that correspond to rotations of spherical and distributional time series. Differences expressed in terms of rotations can be taken between the Fréchet mean and the observations or between consecutive observations of the time series. We derive theoretical properties of the ensuing autoregressive models and showcase these approaches with several motivating data. These include a time series of yearly observations of bivariate distributions of the minimum/maximum temperatures for a period of 120 days during each summer for the years 1990-2018 at Los Angeles (LAX) and John F. Kennedy (JFK) international airports. A second data application concerns a compositional time series with annual observations of compositions of energy sources for power generation in the U.S..",http://www.sciencedirect.com/science/article/pii/S0304407623000209
Journal of Econometrics,2024,The nonparametric Box–Cox model for high-dimensional regression analysis,He Zhou and Hui Zou,"The mainstream theory for high-dimensional regression assumes that the underlying true model is a low-dimensional linear regression model. On the other hand, a standard technique in regression analysis, even in the traditional low-dimensional setting, is to employ the Box–Cox transformation for reducing anomalies such as non-additivity and heteroscedasticity in linear regression. In this paper, we propose a new high-dimensional regression method based on a nonparametric Box–Cox model with an unspecified monotone transformation function. Model fitting and computation become much more challenging than the usual penalized regression method, and a two-step method is proposed for the estimation of this model in high-dimensional settings. First, we propose a novel technique called composite probit regression (CPR) and use the folded concave penalized CPR for estimating the regression parameters. The strong oracle property of the estimator is established without knowing the nonparametric transformation function. Next, the nonparametric function is estimated by conducting univariate monotone regression. The computation is done efficiently by using a coordinate-majorization-descent algorithm. Extensive simulation studies show that the proposed method performs well in various settings. Our analysis of the supermarket data demonstrates the superior performance of the proposed method over the standard high-dimensional regression method.",http://www.sciencedirect.com/science/article/pii/S0304407623000568
Journal of Econometrics,2024,Stock co-jump networks,"Yi Ding, Yingying Li, Guoli Liu and Xinghua Zheng","We propose a Degree-Corrected Block Model with Dependent Multivariate Poisson edges (DCBM-DMP) to study stock co-jump dependence. To estimate the community structure, we extend the SCORE algorithm in Jin (2015) and develop a Spectral Clustering On Ratios-of-Eigenvectors for networks with Dependent Multivariate Poisson edges (SCORE-DMP) algorithm. We prove that SCORE-DMP enjoys strong consistency in community detection. Empirically, using high-frequency data of S&P 500 constituents, we construct two co-jump networks according to whether the market jumps and find that they exhibit different community features than GICS. We further show that the co-jump networks help in stock return prediction.",http://www.sciencedirect.com/science/article/pii/S030440762300057X
Journal of Econometrics,2024,High frequency market making: The role of speed,Yacine Ait-Sahalia and Mehmet Sağlam,"We propose a model where a strategic high frequency market maker exploits his speed and informational advantages to place quotes that interact with the market orders of low frequency traders. We characterize the optimal market making policy and the equilibrium that results. The model has testable implications regarding the impact of speed on the provision of liquidity. We test these implications by taking advantage of a natural experiment on the NYSE American stock exchange, which implemented an intentional delay in 2017 then removed it in 2019. We find broad agreement between the empirical evidence and the implications of the model.",http://www.sciencedirect.com/science/article/pii/S0304407623000581
Journal of Econometrics,2024,Inferential theory for generalized dynamic factor models,"Matteo Barigozzi, Marc Hallin, Matteo Luciani and Paolo Zaffaroni","We provide the asymptotic distributional theory for the so-called General or Generalized Dynamic Factor Model (GDFM), laying the foundations for an inferential approach in the GDFM analysis of high-dimensional time series. By exploiting the duality between common shocks and dynamic loadings, we derive the asymptotic distribution and associated standard errors for a class of estimators for common shocks, dynamic loadings, common components, and impulse response functions. We present an empirical application aimed at constructing a “core” inflation indicator for the U.S. economy, which demonstrates the superiority of the GDFM-based indicator over the most common approaches, particularly the one based on Principal Components.",http://www.sciencedirect.com/science/article/pii/S0304407623000593
Journal of Econometrics,2024,Realized regression with asynchronous and noisy high frequency and high dimensional data,"Dachuan Chen, Per A. Mykland and Lan Zhang","We develop regression for high frequency data. This regression is novel in that it can be for both fixed and increasing dimension. Also, the data may have microstructure noise, and observations (trades, or quotes) can be asynchronous, (i.e., the observations do not need to be synchronized across dimensions). As is customary for high-frequency inference methods, we refer to our method as “realized” regression.",http://www.sciencedirect.com/science/article/pii/S030440762300132X
Journal of Econometrics,2024,Power enhancement for testing multi-factor asset pricing models via Fisher’s method,"Xiufan Yu, Jiawei Yao and Lingzhou Xue","Testing multi-factor asset pricing models is instrumental for asset pricing theory and practice. However, due to the accumulation of errors in estimating high-dimensional parameters, traditional quadratic-form tests such as the Wald test perform poorly against the sparse alternative hypothesis, i.e., a few mispriced assets. Fan et al. (2015b) introduced a powerful testing procedure by adding a power enhancement component to the Wald test statistic and proved power enhancement properties. To provide an alternative to their methodology, we first instantiate the power enhancement component by introducing a new maximum-form test statistic and then study the asymptotic joint distribution of the Wald test statistic and the maximum test statistic. We prove that these two test statistics are asymptotically independent. Given their asymptotic independence, we propose a new power-enhanced testing procedure to combine their respective power based on Fisher’s method (Fisher, 1925). Theoretically, we prove that the new power-enhanced test retains the desired nominal significance level and achieves asymptotically consistent power against more general alternatives. Furthermore, we demonstrate the finite-sample performance of our proposed power-enhanced test in both simulation studies and an empirical study of testing market efficiency using asset returns of the Russel-2000 portfolio.",http://www.sciencedirect.com/science/article/pii/S0304407623001525
Journal of Econometrics,2024,Retire: Robust expectile regression in high dimensions,"Rebeka Man, Kean Ming Tan, Zian Wang and Wen-Xin Zhou","High-dimensional data can often display heterogeneity due to heteroscedastic variance or inhomogeneous covariate effects. Penalized quantile and expectile regression methods offer useful tools to detect heteroscedasticity in high-dimensional data. The former is computationally challenging due to the non-smooth nature of the check loss, and the latter is sensitive to heavy-tailed error distributions. In this paper, we propose and study (penalized) robust expectile regression (retire), with a focus on iteratively reweighted ℓ1-penalization which reduces the estimation bias from ℓ1-penalization and leads to oracle properties. Theoretically, we establish the statistical properties of the retire estimator under two regimes: (i) low-dimensional regime in which d≪n; (ii) high-dimensional regime in which s≪n≪d with s denoting the number of significant predictors. In the high-dimensional setting, we thoroughly analyze the statistical properties of the solution path of iteratively reweighted ℓ1-penalized retire estimation, adapted from the local linear approximation algorithm for folded-concave regularization. Under a mild minimum signal strength condition, we demonstrate that with as few as log(logd) iterations, the final iterate of our proposed approach achieves the oracle convergence rate. At each iteration, we solve the weighted ℓ1-penalized convex program using a semismooth Newton coordinate descent algorithm. Numerical studies demonstrate the promising performance of the proposed procedure in comparison to both non-robust and quantile regression based alternatives.",http://www.sciencedirect.com/science/article/pii/S0304407623001537
Journal of Econometrics,2024,Inference on the best policies with many covariates,"Waverly Wei, Yuqing Zhou, Zeyu Zheng and Jingshen Wang","Understanding the impact of the most effective policies or treatments on a response variable of interest is desirable in many empirical works in economics, statistics and other disciplines. Due to the widespread winner’s curse phenomenon, conventional statistical inference assuming that the top policies are chosen independent of the random sample may lead to overly optimistic evaluations of the best policies. In recent years, given the increased availability of large datasets, such an issue can be further complicated when researchers include many covariates to estimate the policy or treatment effects in an attempt to control for potential confounders. In this manuscript, to simultaneously address the above-mentioned issues, we propose a resampling-based procedure that not only lifts the winner’s curse in evaluating the best policies observed in a random sample, but also is robust to the presence of many covariates. The proposed inference procedure yields accurate point estimates and valid frequentist confidence intervals that achieve the exact nominal level as the sample size goes to infinity for multiple best policy effect sizes. We illustrate the finite-sample performance of our approach through Monte Carlo experiments and two empirical studies, evaluating the most effective policies in charitable giving and the most beneficial group of workers in the National Supported Work program.",http://www.sciencedirect.com/science/article/pii/S0304407623001549
Journal of Econometrics,2024,Bipartite network influence analysis of a two-mode network,"Yujia Wu, Wei Lan, Xinyan Fan and Kuangnan Fang","A two-mode network contains two types of nodes, and edges exist only between any two nodes that are associated with different entities. Owing to the network connections (i.e., edges) between the two types of network nodes, nodal responses are unlikely to be independently and identically distributed, resulting in possible nodal heterogeneity across the two types of nodes. This study proposes a novel bipartite network influence model (BNIM) to evaluate nodal heterogeneity from the perspective of nodal influence. To make the model estimable, we parameterize the influence indices with a set of nodal attributes through a prespecified link function, and employ the quasi-maximum likelihood approach to estimate the unknown parameters. Score tests are presented to examine the heterogeneity of nodal influences across the two types of nodes. To assess the adequacy of the link function, we carry out a quasi-likelihood ratio test and establish its asymptotic properties under the appropriate conditions. Simulation studies and the real data analysis of a fund-stock network are studied to assess the finite-sample performance of BNIM.",http://www.sciencedirect.com/science/article/pii/S0304407623002786
Journal of Econometrics,2024,Dynamic modeling for multivariate functional and longitudinal data,"Siteng Hao, Shu-Chin Lin, Jane-Ling Wang and Qixian Zhong","Dynamic interactions among several stochastic processes are common in many scientific fields. It is crucial to model these interactions to understand the dynamic relationship of the corresponding multivariate processes with their derivatives and to improve predictions. In reality, full observations of the multivariate processes are not feasible as measurements can only be taken at discrete locations or time points, and often only sparingly and intermittently in longitudinal studies. This results in multivariate longitudinal data that are measured at different times for different subjects. We propose a time-dynamic model to handle multivariate longitudinal data by modeling the derivatives of multivariate processes using the values of these processes. Starting with a linear concurrent model, we develop methods to estimate the regression coefficient functions, which can accommodate irregularly measured longitudinal data that are possibly contaminated with noise. Our approach can also be applied to settings when the observational times are the same for all subjects. We establish the convergence rates of our estimators with phase transitions and further illustrate our model through a simulation study and a real data application.",http://www.sciencedirect.com/science/article/pii/S0304407623002890
Journal of Econometrics,2024,Volatility prediction comparison via robust volatility proxies: An empirical deviation perspective,"Weichen Wang, Ran An and Ziwei Zhu","Volatility forecasting is crucial to risk management and portfolio construction. One particular challenge of assessing volatility forecasts is how to construct a robust proxy for the unknown true volatility. In this work, we show that the empirical loss comparison between two volatility predictors hinges on the deviation of the volatility proxy from the true volatility. We then establish non-asymptotic deviation bounds for three robust volatility proxies, two of which are based on clipped data, and the third of which is based on exponentially weighted Huber loss minimization. In particular, in order for the Huber approach to adapt to non-stationary financial returns, we propose to solve a tuning-free weighted Huber loss minimization problem to jointly estimate the volatility and the optimal robustification parameter at each time point. We then inflate this robustification parameter and use it to update the volatility proxy to achieve optimal balance between the bias and variance of the global empirical loss. We also extend this Huber method to construct volatility predictors. Finally, we exploit the proposed robust volatility proxy to compare different volatility predictors on the Bitcoin market data and calibrated synthetic data. It turns out that when the sample size is limited, applying the robust volatility proxy gives more consistent and stable evaluation of volatility forecasts.",http://www.sciencedirect.com/science/article/pii/S0304407623003494
Journal of Econometrics,2024,Reprint: Statistical inference for linear mediation models with high-dimensional mediators and application to studying stock reaction to COVID-19 pandemic,"Xu Guo, Runze Li, Jingyuan Liu and Mudong Zeng","Mediation analysis draws increasing attention in many research areas such as economics, finance and social sciences. In this paper, we propose new statistical inference procedures for high dimensional mediation models, in which both the outcome model and the mediator model are linear with high dimensional mediators. Traditional procedures for mediation analysis cannot be used to make statistical inference for high dimensional linear mediation models due to high-dimensionality of the mediators. We propose an estimation procedure for the indirect effects of the models via a partially penalized least squares method, and further establish its theoretical properties. We further develop a partially penalized Wald test on the indirect effects, and prove that the proposed test has a χ2 limiting null distribution. We also propose an F-type test for direct effects and show that the proposed test asymptotically follows a χ2-distribution under null hypothesis and a noncentral χ2-distribution under local alternatives. Monte Carlo simulations are conducted to examine the finite sample performance of the proposed tests and compare their performance with existing ones. We further apply the newly proposed statistical inference procedures to study stock reaction to COVID-19 pandemic via an empirical analysis of studying the mediation effects of financial metrics that bridge company’s sector and stock return.",http://www.sciencedirect.com/science/article/pii/S0304407623003664
Journal of Econometrics,2024,Reprint: Hypothesis testing on high dimensional quantile regression,"Zhao Chen, Vivian Xinyi Cheng and Xu Liu","Quantile regression has been an important analytical tool in econometrics since it was proposed in 1970s. Many advantages make it still popular in the era of big data. This paper focuses on the testing problems of high-dimensional quantile regression, supplementing to robust methods in the literature of high-dimensional hypothesis testing. We first construct a new test statistic based on the quantile regression score function. The new test statistic avoids the inverse operation of the covariance matrix, and hence becomes applicable to high-dimensional or even ultrahigh-dimensional settings. The proposed method retains robustness for non-Gaussian and heavy-tailed distributions. We then derive the limiting distributions of the proposed test statistic under both the null and the alternative hypotheses. We further investigate the case where the design matrix follows an elliptical distribution. We examined the finite sample performance of our proposed method through Monte Carlo simulations. The numerical comparisons exhibit that our proposed tests outperform some existing methods in terms of controlling Type I error rate and power, when the data deviate from the Gaussian assumptions or are heavy-tailed. We illustrate our proposed high-dimensional quantile testing in financial econometrics, through an empirical analysis of Chinese stock market data.",http://www.sciencedirect.com/science/article/pii/S0304407623003676
Journal of Econometrics,2024,Beyond RCP8.5: Marginal mitigation using quasi-representative concentration pathways,J. Miller and William A. Brock,"Assessments of decreases in economic damages from climate change mitigation typically rely on climate output from computationally expensive pre-computed runs of general circulation models under a handful of scenarios with discretely varying targets, such as the four representative concentration pathways for CO2 and other anthropogenically emitted gases. Although such analyses are valuable in informing scientists and policymakers about massive multilateral mitigation goals, we add to the literature by considering potential outcomes from more modest policy changes that may not be represented by any well-known concentration pathway. Specifically, we construct computationally efficient Quasi-representative Concentration Pathways (QCPs) to leverage concentration pathways of existing peer-reviewed scenarios. Computational efficiency allows for bootstrapping to assess uncertainty. We illustrate our methodology by considering the impact on the relative risk of mortality from heat stress in London from the United Kingdom’s net zero emissions goal. More than half of our interval estimate for the business-as-usual scenario covers an annual risk at least that of a COVID-19-like mortality event by 2100. Success of the UK’s policy alone would do little to mitigate the risk.",http://www.sciencedirect.com/science/article/pii/S0304407621001792
Journal of Econometrics,2024,Modelling cycles in climate series: The fractional sinusoidal waveform process,Tommaso Proietti and Federico Maddanu,"The paper proposes a novel model for time series displaying persistent stationary cycles, the fractional sinusoidal waveform process. The underlying idea is to allow the parameters that regulate the amplitude and phase to evolve according to fractional noise processes. Its advantages with respect to popular alternative specifications, such as the Gegenbauer process, are twofold: the autocovariance function is available in closed form, which opens the way to exact maximum likelihood estimation; secondly, the model encompasses deterministic cycles, so that discrete spectra arise as a limiting case. A generalization of the process, featuring multiple components, an additive ‘red noise’ component and exogenous variables, provides the basic model for climate time series with mixed spectra. Our illustrations deal with the change in amplitude and phase of the intra-annual component of carbon dioxide concentrations in Mauna Loa, and with the estimation and the quantification of the contribution of orbital cycles to the variability of paleoclimate time series.",http://www.sciencedirect.com/science/article/pii/S0304407622000987
Journal of Econometrics,2024,Sieve bootstrap inference for linear time-varying coefficient models,Marina Friedrich and Yicong Lin,"We propose a sieve bootstrap framework to conduct pointwise and simultaneous inference for time-varying coefficient regression models based on a local linear estimator. The asymptotic validity of the sieve bootstrap in the presence of autocorrelation is established. The bootstrap automatically produces a consistent estimate of nuisance parameters, both at the interior and boundary points. In addition, we develop a bootstrap-based test for parameter constancy and examine its asymptotic properties. An extensive simulation study demonstrates a good finite sample performance of our methods. The proposed methods are applied to assess the price development of CO2 certificates in the European Emissions Trading System. We find evidence of time variation in the relationship between allowance prices and their fundamental price drivers. The time variation might offer an explanation for previous contradicting findings using linear regression models with constant coefficients.",http://www.sciencedirect.com/science/article/pii/S0304407622001701
Journal of Econometrics,2024,The validity of bootstrap testing for threshold autoregression,"Simone Giannerini, Greta Goracci and Anders Rahbek","We consider bootstrap-based testing for threshold effects in non-linear threshold autoregressive (TAR) models. It is well-known that classic tests based on asymptotic theory tend to be biased in case of small, or even moderate sample sizes, especially when the estimated parameters indicate non-stationarity, or in presence of heteroskedasticity, as often witnessed in the analysis of financial or climate data. To address the issue we propose a supremum Lagrange Multiplier test statistic (sLM), where the null hypothesis specifies a linear autoregressive (AR) model against the alternative of a TAR model. We consider both the classical recursive residual i.i.d. bootstrap (sLMi) and a wild bootstrap (sLMw), applied to the sLM statistic, and establish their validity under the null hypothesis. The framework is new, and requires the proof of non-standard results for bootstrap analysis in time series models; this includes a uniform bootstrap law of large numbers and a bootstrap functional central limit theorem. The Monte Carlo evidence shows that the bootstrap tests have correct empirical size even for small samples; the wild bootstrap version (sLMw) is also robust against the presence of heteroskedasticity. Moreover, there is no loss of empirical power when compared to the asymptotic test and the size of the tests is not affected if the order of the tested model is selected through AIC. Finally, we use our results to analyse the time series of the Greenland ice sheet mass balance. We find a significant threshold effect and an appropriate specification that manages to reproduce the main non-linear features of the series, such as the asymmetric seasonal cycle, the main periodicities, and the multimodality of the probability density function.",http://www.sciencedirect.com/science/article/pii/S0304407623000040
Journal of Econometrics,2024,Modelling circular time series,"Andrew Harvey, Stan Hurn, Dario Palumbo and Stephen Thiele","Circular variables often play an important role in the construction of models for analysing and forecasting the consequences of climate change and its impact on the environment. Such variables pose special problems for time series modelling. This article shows how the score-driven approach, developed primarily in econometrics, provides a natural solution to the difficulties and leads to a coherent and unified methodology for estimation, model selection and testing. The new methods are illustrated with data on wind direction.",http://www.sciencedirect.com/science/article/pii/S0304407623001446
Journal of Econometrics,2024,Common volatility shocks driven by the global carbon transition,Susana Campos-Martins and David Hendry,"We propose a novel approach to measure the global effects of climate change news on financial markets. For that purpose, we first calculate the global common volatility of the oil and gas industry. Then we project it on climate-related shocks constructed using text-based proxies of climate change news. We show that rising concerns about the energy transition make oil and gas share prices move at the global scale, controlling for shocks to the oil price, US and world stock markets. Despite the clear exposure of oil and gas companies to carbon transition risk, not all geoclimatic shocks are alike. The signs and magnitudes of the impacts differ across climate risk drivers. Regarding sentiment, climate change news tends to create turmoil only when the news is negative. Moreover, the adverse effect is amplified by oil price movements but weakened by stock market shocks. Finally, our findings point out climate news materialises when it reaches the global scale, supporting the relevance of modelling geoclimatic volatility.",http://www.sciencedirect.com/science/article/pii/S0304407623001665
Journal of Econometrics,2024,Long monthly temperature series and the Vector Seasonal Shifting Mean and Covariance Autoregressive model,"Changli He, Jian Kang, Annastiina Silvennoinen and Timo Teräsvirta","We consider a vector version of the Shifting Seasonal Mean Autoregressive model. The model is used for describing dynamic behaviour of and contemporaneous dependence between a number of long monthly temperature series for 20 cities in Europe, extending from the second half of the 18th century until mid-2010s. The results indicate strong warming in the winter months, February excluded, and cooling followed by warming during the summer months. Error variances are mostly constant over time, but for many series there is systematic decrease between 1820 and 1850 in April. Error correlations are considered by selecting two small sets of series and modelling correlations within these sets. Some correlations do change over time, but a large majority remains constant. Not surprisingly, the correlations generally decrease with the distance between cities, but the precise geographical location also plays a role.",http://www.sciencedirect.com/science/article/pii/S0304407623002105
Journal of Econometrics,2024,On model selection criteria for climate change impact studies,"Xiaomeng Cui, Bulat Gafarov, Dalia Ghanem and Todd Kuffner","Climate change impact studies inform policymakers on the estimated damages of future climate change on economic, health and other outcomes. In most studies, an annual outcome variable is observed, e.g. agricultural yield, along with a higher-frequency regressor, e.g. daily temperature. Applied researchers then face a problem of selecting a model to characterize the nonlinear relationship between the outcome and the high-frequency regressor to make a policy recommendation based on the model-implied damage function. We show that existing model selection criteria are only suitable for the policy objective if one of the models under consideration nests the true model. If all models are seen as imperfect approximations of the true nonlinear relationship, the model that performs well in the historical climate conditions is not guaranteed to perform well at the projected climate. We therefore propose a new criterion, the proximity-weighted mean squared error (PWMSE) that directly targets precision of the damage function at the projected future climate. To make this criterion feasible, we assign higher weights to historical years that can serve as “weather analogs” to the projected future climate when evaluating competing models using the PWMSE. We show that our approach selects the best approximate regression model that has the smallest weighted squared error of predicted impacts for a projected future climate. A simulation study and an application revisiting the impact of climate change on agricultural production illustrate the empirical relevance of our theoretical analysis.",http://www.sciencedirect.com/science/article/pii/S0304407623002270
Journal of Econometrics,2024,Sparse generalized Yule–Walker estimation for large spatio-temporal autoregressions with an application to NO2 satellite data,Hanno Reuvers and Etienne Wijler,"We consider a high-dimensional model in which variables are observed over time and space. The model consists of a spatio-temporal regression containing a time lag and a spatial lag of the dependent variable. Unlike classical spatial autoregressive models, we do not rely on a predetermined spatial interaction matrix, but infer all spatial interactions from the data. Assuming sparsity, we estimate the spatial and temporal dependence fully data-driven by penalizing a set of Yule–Walker equations. This regularization can be left unstructured, but we also propose customized shrinkage procedures when observations originate from spatial grids (e.g. satellite images). Finite sample error bounds are derived and estimation consistency is established in an asymptotic framework wherein the sample size and the number of spatial units diverge jointly. Exogenous variables can be included as well. A simulation exercise shows strong finite sample performance compared to competing procedures. As an empirical application, we model satellite measured nitrogen dioxide (NO2) concentrations in London. Our approach delivers forecast improvements over a competitive benchmark and we discover evidence for strong spatial interactions.",http://www.sciencedirect.com/science/article/pii/S0304407623002361
Journal of Econometrics,2024,Testing for coefficient distortion due to outliers with an application to the economic impacts of climate change,"Xiyu Jiao, Felix Pretis and Moritz Schwarz","Outlying observations can bias regression estimates, requiring the use of outlier-robust estimators. Comparing robust estimates to those obtained using ordinary least squares (OLS) is a common robustness check, however, such comparisons have been mostly informal due to the lack of available tests. Here we introduce a formal test for coefficient distortion due to outliers in regression models. Our proposed test is based on the difference between OLS and robust estimates obtained using a class of Huber-skip M-type estimators (such as Impulse Indicator Saturation or Robustified Least Squares). We show that our distortion test has an asymptotic chi-squared distribution by establishing the asymptotics of the corresponding Huber-skip M-estimators using an empirical process Central Limit Theorem recently developed in the literature. The test is valid for cross-sectional, as well as panel, and stationary or deterministically-trending time series models. To improve finite sample performance and to alleviate concerns on distributional assumptions, we explore several bootstrap testing schemes. We apply our outlier distortion test to estimates of the macro-economic impacts of climate change allowing for adaptation.",http://www.sciencedirect.com/science/article/pii/S0304407623002634
Journal of Econometrics,2024,"Reprint of: When will Arctic sea ice disappear? Projections of area, extent, thickness, and volume","Francis Diebold, Glenn Rudebusch, Maximilian Göbel, Philippe Goulet Coulombe and Boyuan Zhang","Rapidly diminishing Arctic summer sea ice is a strong signal of the pace of global climate change. We provide point, interval, and density forecasts for four measures of Arctic sea ice: area, extent, thickness, and volume. Importantly, we enforce the joint constraint that these measures must simultaneously arrive at an ice-free Arctic. We apply this constrained joint forecast procedure to models relating sea ice to atmospheric carbon dioxide concentration and models relating sea ice directly to time. The resulting “carbon-trend” and “time-trend” projections are mutually consistent and predict a nearly ice-free summer Arctic Ocean by the mid-2030s with an 80% probability. Moreover, the carbon-trend projections show that global adoption of a lower carbon path would likely delay the arrival of a seasonally ice-free Arctic by only a few years.",http://www.sciencedirect.com/science/article/pii/S0304407623003615
Journal of Econometrics,2024,A residual bootstrap for conditional Value-at-Risk,"Eric Beutner, Alexander Heinemann and Stephan Smeekes","A fixed-design residual bootstrap method is proposed for the two-step estimator of Francq and Zakoïan(2015) associated with the conditional Value-at-Risk. The bootstrap’s consistency is proven for a general class of volatility models and intervals are constructed for the conditional Value-at-Risk. A simulation study reveals that the equal-tailed percentile bootstrap interval tends to fall short of its nominal value. In contrast, the reversed-tails bootstrap interval yields accurate coverage. We also compare the theoretically analyzed fixed-design bootstrap with the recursive-design bootstrap. It turns out that the fixed-design bootstrap performs equally well in terms of average coverage, yet leads on average to shorter intervals in smaller samples. An empirical application illustrates the interval estimation.",http://www.sciencedirect.com/science/article/pii/S0304407623002701
Journal of Econometrics,2024,Profiling the plight of disconnected youth in America,"Thomas MaCurdy, David Glick, Sonam Sherpa and Sriniketh Nagavarapu","In a successful transition from youth to adulthood, individuals pass through a sequence of roles involving school, work, and family formation that culminate in their becoming self-sufficient adults. However, some “disconnected” youth spend extended periods of time outside of any role that constitutes an element of the pathway towards adult independence. Assisting these youth requires a systematic understanding of what “disconnection” means, how many disconnected youth there are, who these youth are, and how the scale of the problem has evolved over time. Using the National Longitudinal Surveys of Youth for 1997 and 1979, we address these issues by creating concrete definitions of “disconnection spells” using rich data on youths’ enrollment, work, and personal histories. We estimate a multi-state duration model to account for right censoring and to understand differences across salient sub-groups. Our estimates imply that in the early 2000s, almost 19% and 25% of young men and young women, respectively, experienced a disconnection spell by age 23 using our basic definition. These rates are substantially higher for certain sub-groups defined by race/ethnicity, parental education, and government aid receipt, rising as high as 30+% by age 23. Approximately 60% of youth with a disconnection spell have it last longer than a year, and close to 10% have it last longer than 4 years. However, once reconnected, a majority of youth go at least three years without a re-disconnection spell. Patterns of initial disconnection changed markedly from the 1980s to the 2000s, as young women saw a 12 percentage point decline over time. Moreover, the Black-White gap in disconnection has fallen for women, but increased for men. Our profile of disconnection experiences provides a starting point for government agencies aiming to understand where, how, and with whom to intervene to prevent lengthy disconnection spells.",http://www.sciencedirect.com/science/article/pii/S0304407623002737
Journal of Econometrics,2024,Sharp bounds in the latent index selection model,Philip Marx,"A fundamental question underlying the literature on partial identification is: what can we learn about parameters that are relevant for policy but not necessarily point-identified by the exogenous variation we observe? This paper provides an answer in terms of sharp, analytic characterizations and bounds for an important class of policy-relevant treatment effects, consisting of marginal treatment effects and linear functionals thereof, in the latent index selection model as formalized in Vytlacil (2002). The sharp bounds use the full content of identified marginal distributions, and analytic derivations rely on the theory of stochastic orders. The proposed methods also make it possible to sharply incorporate new auxiliary assumptions on distributions into the latent index selection framework. Empirically, I apply the methods to study the effects of Medicaid on emergency room utilization in the Oregon Health Insurance Experiment, showing that the predictions from extrapolations based on a distribution assumption (rank similarity) differ substantively and consistently from existing extrapolations based on a parametric mean assumption (linearity). This underscores the value of utilizing the model’s full empirical content in combination with auxiliary assumptions.",http://www.sciencedirect.com/science/article/pii/S0304407623002774
Journal of Econometrics,2024,An information–Theoretic approach to partially identified auction models,Sung Jae Jun and Joris Pinkse,"We consider a situation in which we have data from ascending auctions with symmetric bidders, independent private values, and exogenous entry in which the bidders’ value distribution is partially identified. Focusing on the case in which the seller intends to use a second price auction, we discuss how to determine an optimal reserve price. We justify the use of maximum entropy, explore the properties of the estimand, determine the asymptotic properties of our maximum entropy estimator, evaluate its behavior in a simulation study, and demonstrate its use in a modest application. As an extension, we propose a maxmin decision rule with entropy regularization, which includes Aryal and Kim (2013) and the maximum entropy solution as extreme cases.",http://www.sciencedirect.com/science/article/pii/S0304407623002828
Journal of Econometrics,2024,Identification and estimation of sequential games of incomplete information with multiple equilibria,Jangsu Yoon,"This paper discusses the identification and estimation of game-theoretic models, mainly focusing on sequential games of incomplete information. In most empirical games, researchers cannot observe the exact order of actions played in the game and rely on the assumption of simultaneous actions. My structural modeling generalizes an empirical game to encompass simultaneous and sequential actions as special cases. I specify a sequential game allowing for multiple players in each stage and multiple Perfect Bayesian Nash Equilibria, showing that the structural parameters, including the payoff function parameters, the order of actions, and the equilibrium selection mechanism, are separately identified. The various exclusion restrictions in the finite mixture literature help attain point identification of structural parameters and provide a testable method to verify identification conditions. Next, I consider a Sieve Minimum Distance (SMD) estimator of Ai and Chen (2003) for the structural parameters and verify its asymptotic properties. The Monte Carlo simulations evaluate the performance of the proposed estimator and provide numerical evidence of potential bias under the misspecified order of actions. The empirical application of Walmart and Kmart’s entry game demonstrates that retailers compete sequentially in a significant portion of markets.",http://www.sciencedirect.com/science/article/pii/S0304407623002853
Journal of Econometrics,2024,Unconditional effects of general policy interventions,"Julian Martinez-Iriarte, Gabriel Montes-Rojas and Yixiao Sun","This paper studies the unconditional effects of a general policy intervention, which includes location-scale shifts and simultaneous shifts as special cases. The location-scale shift is intended to study a counterfactual policy aimed at changing not only the mean or location of a covariate but also its dispersion or scale. The simultaneous shift refers to the situation where shifts in two or more covariates take place simultaneously. For example, a shift in one covariate is compensated at a certain rate by a shift in another covariate. Not accounting for these possible scale or simultaneous shifts will result in an incorrect assessment of the potential policy effects on an outcome variable of interest. The unconditional policy parameters are estimated with simple semiparametric estimators, for which asymptotic properties are studied. Monte Carlo simulations are implemented to study their finite sample performances. The proposed approach is applied to a Mincer equation to study the effects of changing years of education on wages and to study the effect of smoking during pregnancy on birth weight.",http://www.sciencedirect.com/science/article/pii/S0304407623002865
Journal of Econometrics,2024,Role models and revealed gender-specific costs of STEM in an extended Roy model of major choice,"Marc Henry, Romuald Méango and Ismaël Mourifié","We derive sharp bounds on the non consumption utility component in an extended Roy model of sector selection. We interpret this non consumption utility component as a compensating wage differential. The bounds are derived under the assumption that potential utilities in each sector are (jointly) stochastically monotone with respect to an observed selection shifter. The research is motivated by the analysis of women’s choice of university major, their under representation in mathematics intensive fields, and the impact of role models on choices and outcomes. To illustrate our methodology, we investigate the cost of STEM fields with data from a German graduate survey, and using the mother’s education level and the proportion of women on the STEM faculty at the time of major choice as selection shifters.",http://www.sciencedirect.com/science/article/pii/S0304407623002877
Journal of Econometrics,2024,Estimation of complier expected shortfall treatment effects with a binary instrumental variable,"Bo Wei, Kean Ming Tan and Xuming He","Estimating the causal effect of a treatment or exposure for a subpopulation is of great interest in many biomedical and economical studies. Expected shortfall, also referred to as the super-quantile, is an attractive effect-size measure that can accommodate data heterogeneity and aggregate local information of effect over a certain region of interest of the outcome distribution. In this article, we propose the ComplieRExpected Shortfall Treatment Effect (CRESTE) model under an instrumental variable framework to quantity the CRESTE for a binary endogenous treatment variable. By utilizing the special characteristics of a binary instrumental variable and a specific formulation of Neyman-orthogonalization, we propose a two-step estimation procedure, which can be implemented by simply solving weighted least-squares regression and weighted quantile regression with estimated weights. We develop the asymptotic properties for the proposed estimator and use numerical simulations to confirm its validity and robust finite-sample performance. An illustrative analysis of a National Job Training Partnership Act study is presented to show the practical utility of the proposed method.",http://www.sciencedirect.com/science/article/pii/S0304407623002889
Journal of Econometrics,2024,Nested Pseudo likelihood estimation of continuous-time dynamic discrete games,Jason Blevins and Minhae Kim,"We introduce a sequential estimator for continuous time dynamic discrete choice models (single-agent models and games) by adapting the nested pseudo likelihood (NPL) estimator of Aguirregabiria and Mira (2002, 2007), developed for discrete time models with discrete time data, to the continuous time case with data sampled either discretely (i.e., uniformly-spaced snapshot data) or continuously. We establish conditions for consistency and asymptotic normality of the estimator, a local convergence condition, and, for single agent models, a zero Jacobian property assuring local convergence. We carry out a series of Monte Carlo experiments using an entry-exit game with five heterogeneous firms to confirm the large-sample properties and demonstrate finite-sample bias reduction via iteration. In our simulations we show that the convergence issues documented for the NPL estimator in discrete time models are less likely to affect comparable continuous-time models. We also show that there can be large bias in economically-relevant parameters, such as the competitive effect and entry cost, from estimating a misspecified discrete time model when in fact the data generating process is a continuous time model.",http://www.sciencedirect.com/science/article/pii/S0304407623002920
Journal of Econometrics,2024,What leads to measurement errors? Evidence from reports of program participation in three surveys,"Pablo Celhay, Bruce D. Meyer and Nikolas Mittag","Measurement errors are often a large source of bias in survey data. Lack of knowledge of the determinants of such errors makes it difficult to reduce the extent of errors when collecting data and to assess the validity of analyses using the data. We study the determinants of reporting error using high quality administrative data on government transfers linked to three major U.S. surveys. Our results support several theories of misreporting: Errors are related to event recall, forward and backward telescoping, salience of receipt, the stigma of reporting participation in welfare programs and respondent's degree of cooperation with the survey overall. We provide evidence on how survey design choices affect reporting errors. Our findings help survey users to gauge the reliability of their data and to devise estimation strategies that can correct for systematic errors, such as instrumental variable approaches. Understanding survey errors allows researchers collecting survey data to reduce them by improving survey design. Our results indicate that survey design should take into account that higher response rates as well as collecting more detailed information may have negative effects on survey accuracy.",http://www.sciencedirect.com/science/article/pii/S030440762300297X
Journal of Econometrics,2024,Quantile analysis of “hazard-rate” game models,Andreea Enache and Jean-Pierre Florens,"This paper consists of an econometric analysis of a broad class of games of incomplete information. In these games, a player’s action depends both on her unobservable characteristic (the private information), as well as on the ratio of the distribution of the unobservable characteristic and its density function (which we call the ”hazard-rate”). The goal is to use data on players’ actions to recover the distribution of private information. We show that the structural parameter (the quantile of the unobservable characteristic, denoted by L) can be related to the reduced form parameter (the quantile of the data, denoted by Q) through a differential equation Q(α)=a(L(α),αL′(α)). We analyse the local and global identification of this equation in L. At the same time, under suitable assumptions, we establish the local and global well-posedness of the inverse problem Q=T(L). We solve the differential equation and, as a consequence of the well-posedness, we estimate nonparametrically the quantile and the c.d.f. function of the unobserved variables at a root-n speed of convergence (conditional on the quantile of the data being estimated at root-n). Moreover as the transformation of Q in L is continuous with respect to a topology that implies also the derivatives, we estimate nonparametrically the quantile density and the density the unobserved variables (that are continuous functions with respect to Q) at root-n speed of convergence. We provide also several generalisations of the model accounting for exogenuous variables, implicit expression of the strategy function and integral expression of the strategy function. Our results have several policy applications, including better design of auctions, regulation models, estimation of war of attrition in patent races, and public good contracts.",http://www.sciencedirect.com/science/article/pii/S0304407623002981
Journal of Econometrics,2024,A conditional linear combination test with many weak instruments,"Dennis Lim, Wenjie Wang and Yichong Zhang","We consider a linear combination of jackknife Anderson-Rubin (AR), jackknife Lagrangian multiplier (LM), and orthogonalized jackknife LM tests for inference in IV regressions with many weak instruments and heteroskedasticity. Following I.Andrews (2016), we choose the weights in the linear combination based on a decision-theoretic rule that is adaptive to the identification strength. Under both weak and strong identifications, the proposed test controls asymptotic size and is admissible among certain class of tests. Under strong identification, our linear combination test has optimal power against local alternatives among the class of invariant or unbiased tests which are constructed based on jackknife AR and LM tests. Simulations and an empirical application to Angrist and Krueger’s (1991) dataset confirm the good power properties of our test.",http://www.sciencedirect.com/science/article/pii/S0304407623003184
Journal of Econometrics,2024,Kolmogorov–Smirnov type testing for structural breaks: A new adjusted-range based self-normalization approach,"Yongmiao Hong, Oliver Linton, Brendan McCabe, Jiajing Sun and Shouyang Wang","A popular self-normalization (SN) approach in time series analysis uses the variance of a partial sum as a self-normalizer. This is known to be sensitive to irregularities such as persistent autocorrelation, heteroskedasticity, unit roots and outliers. We propose a novel SN approach based on the adjusted-range of a partial sum, which is robust to these aforementioned irregularities. We develop an adjusted-range based Kolmogorov–Smirnov type test for structural breaks for both univariate and multivariate time series, and consider testing parameter constancy in a time series regression setting. Our approach can rectify the well-known power decrease issue associated with existing self-normalized KS tests without having to use backward and forward summations as in Shao and Zhang (2010), and can alleviate the “better size but less power” phenomenon when the existing SN approaches (Shao, 2010; Zhang et al., 2011; Wang and Shao, 2022) are used. Moreover, our proposed tests can cater for more general alternatives. Monte Carlo simulations and empirical studies demonstrate the merits of our approach.",http://www.sciencedirect.com/science/article/pii/S0304407623003196
Journal of Econometrics,2024,An identification and testing strategy for proxy-SVARs with weak proxies,"Giovanni Angelini, Giuseppe Cavaliere and Luca Fanelli","When proxies (external instruments) used to identify target structural shocks are weak, inference in proxy-SVARs (SVAR-IVs) is nonstandard and the construction of asymptotically valid confidence sets for the impulse responses of interest requires weak-instrument robust methods. In the presence of multiple target shocks, test inversion techniques require extra restrictions on the proxy-SVAR parameters other than those implied by the proxies that may be difficult to interpret and test. We show that frequentist asymptotic inference in these situations can be conducted through Minimum Distance estimation and standard asymptotic methods if the proxy-SVAR can be identified by using ‘strong’ instruments for the non-target shocks; i.e., the shocks which are not of primary interest in the analysis. The suggested identification strategy hinges on a novel pre-test for the null of instrument relevance, based on bootstrap resampling, which is not subject to pre-testing issues. Specifically, the validity of post-test asymptotic inferences remains unaffected by the test outcomes due to an asymptotic independence result between the bootstrap and non-bootstrap statistics. The test is robust to conditionally heteroskedastic and/or zero-censored proxies, is computationally straightforward and applicable regardless of the number of shocks being instrumented. Some illustrative examples show the empirical usefulness of the suggested identification and testing strategy.",http://www.sciencedirect.com/science/article/pii/S0304407623003202
Journal of Econometrics,2024,Estimation and variable selection for high-dimensional spatial dynamic panel data models,"Li Hou, Baisuo Jin and Yuehua Wu","Spatiotemporal modeling of networks is of great practical importance, with modern applications in epidemiology and social network analysis. Despite rapid methodological advances, how to effectively and efficiently estimate the parameters of spatial dynamic panel models remains a challenging problem. To tackle this problem, we construct consistent complex least-squares estimators by the eigendecomposition of a spatial weight matrix method originally proposed for undirected networks. We no longer require all eigenvalues and eigenvectors to be real, which is a remarkable achievement as it implies that the proposed method is now applicable to spatiotemporal data modeling of directed networks. Under mild, interpretable conditions, we show that the proposed parameter estimators are consistent and asymptotically normally distributed. We also present a complex orthogonal greedy algorithm for variable selection and rigorously investigate its convergence properties. Moreover, we incorporate fixed effects into the spatial dynamic panel models and provide a model transformation so that the proposed method can also be applied to the transformed model. Extensive simulation studies and data examples demonstrate the effectiveness of the proposed method.",http://www.sciencedirect.com/science/article/pii/S0304407623003214
Journal of Econometrics,2024,Tail behavior of ACD models and consequences for likelihood-based estimation,"Giuseppe Cavaliere, Thomas Mikosch, Anders Rahbek and Frederik Vilandt","We establish new results for estimation and inference in financial durations models, where events are observed over a given time span, such as a trading day, or a week. For the classical autoregressive conditional duration (ACD) models by Engle and Russell (1998), we show that the large sample behavior of likelihood estimators is highly sensitive to the tail behavior of the financial durations. In particular, even under stationarity, asymptotic normality breaks down for tail indices smaller than one or, equivalently, when the clustering behavior of the observed events is such that the unconditional distribution of the durations has no finite mean. Instead, we find that estimators are mixed Gaussian and have non-standard rates of convergence. The results are based on exploiting the crucial fact that for duration data the number of observations within any given time span is random. Our results apply to general econometric models where the number of observed events is random.",http://www.sciencedirect.com/science/article/pii/S0304407623003299
Journal of Econometrics,2024,High-dimensional IV cointegration estimation and inference,Peter Phillips and Igor L. Kheifets,"A semiparametric triangular systems approach shows how multicointegrating linkages occur naturally in an I(1) cointegrated regression model when the long run error variance matrix in the system is singular. Under such singularity, cointegrated I(1) systems embody a multicointegrated structure that makes them useful in many empirical settings. Earlier work shows that such systems may be analyzed and estimated without appealing to the associated I(2) system but with suboptimal convergence rates and potential asymptotic bias. The present paper develops a robust approach to estimation and inference of such systems using high dimensional IV methods that have appealing asymptotic properties like those known to apply in the optimal estimation of cointegrated systems (Phillips, 1991). The approach uses an extended version of high-dimensional trend IV (Phillips, 2006, 2014) estimation with deterministic orthonormal instruments. The methods and derivations involve new results on high-dimensional IV techniques and matrix normalization in the limit theory that are of independent interest. Wald tests of general linear restrictions are constructed using a fixed-b long run variance estimator that leads to robust pivotal HAR inference in both cointegrated and multicointegrated cases. Simulations show good properties of the estimation and inferential procedures in finite samples. An empirical illustration to housing stocks, starts and completions is provided.",http://www.sciencedirect.com/science/article/pii/S030440762300338X
Journal of Econometrics,2024,The fixed-b limiting distribution and the ERP of HAR tests under nonstationarity,Alessandro Casini,"We show that the limiting distribution of HAR test statistics under fixed-b asymptotics is not pivotal when the data are nonstationary (i.e., time-varying autocovariance structure). It takes the form of a complicated function of Gaussian processes and depends on the second moments of the relevant series (e.g., of the regressors and errors for the case of the linear regression model). Hence, fixed-b inference methods based on stationarity are not theoretically valid in general. The nuisance parameters entering the fixed-b limiting distribution can be consistently estimated under small-b asymptotics but only with nonparametric rate of convergence. We show that the error in rejection probability (ERP) is an order of magnitude larger than that under stationarity and can be also larger than that of HAR tests based on HAC estimators under conventional asymptotics. These theoretical results reconcile with recent finite-sample evidence showing that existing fixed-b HAR tests can perform poorly when the data are nonstationary. They can be conservative under the null hypothesis and have non-monotonic power under the alternative hypothesis irrespective of how large the sample size is. Based on the new nonstationary fixed-b distribution, we propose a feasible method that controls the null rejection rates well regardless of whether the data are stationary or not and of the strength of the serial dependence as verified for some representative data-generating processes in a simple location model.",http://www.sciencedirect.com/science/article/pii/S030440762300341X
Journal of Econometrics,2024,Robust testing for explosive behavior with strongly dependent errors,"Yiu Lim Lui, Peter Phillips and Jun Yu",A heteroskedasticity-autocorrelation robust (HAR) test statistic is proposed to test for the presence of explosive roots in financial or real asset prices when the equation errors are strongly dependent. Limit theory for the test statistic is developed and extended to heteroskedastic models. The new test has stable size properties unlike conventional test statistics that typically lead to size distortion and inconsistency in the presence of strongly dependent equation errors. The new procedure can be used to consistently time-stamp the origination and termination of an explosive episode under similar conditions of long memory errors. Simulations are conducted to assess the finite sample performance of the proposed test and estimators. An empirical application to the S&P 500 index highlights the usefulness of the proposed procedures in practical work.,http://www.sciencedirect.com/science/article/pii/S0304407623003421
Journal of Econometrics,2024,Distributed estimation and inference for spatial autoregression model with large scale networks,"Yimeng Ren, Zhe Li, Xuening Zhu, Yuan Gao and Hansheng Wang","The rapid growth of online network platforms generates large-scale network data and it poses great challenges for statistical analysis using the spatial autoregression (SAR) model. In this work, we develop a novel distributed estimation and statistical inference framework for the SAR model on a distributed system. We first propose a distributed network least squares approximation (DNLSA) method. This enables us to obtain a one-step estimator by taking a weighted average of local estimators on each worker. Afterwards, a refined two-step estimation is designed to further reduce the estimation bias. For statistical inference, we utilize a random projection method to reduce the expensive communication cost. Theoretically, we show the consistency and asymptotic normality of both the one-step and two-step estimators. In addition, we provide theoretical guarantee of the distributed statistical inference procedure. The theoretical findings and computational advantages are validated by several numerical simulations implemented on the Spark system. Lastly, an experiment on the Yelp dataset further illustrates the usefulness of the proposed methodology.",http://www.sciencedirect.com/science/article/pii/S0304407623003457
Journal of Econometrics,2024,Autoregressive conditional betas,"F. Blasques, Christian Francq and Sébastien Laurent","This paper introduces an autoregressive conditional beta (ACB) model that allows regressions with dynamic betas (or slope coefficients) and residuals with GARCH conditional volatility. The model fits in the (quasi) score-driven approach recently proposed in the literature, and it is semi-parametric in the sense that the distributions of the innovations are not necessarily specified. The time-varying betas are allowed to depend on past shocks and exogenous variables. We establish the existence of a stationary solution for the ACB model, the invertibility of the score-driven filter for the time-varying betas, and the asymptotic properties of one-step and multistep QMLEs for the new ACB model. The finite sample properties of these estimators are studied by means of an extensive Monte Carlo study. Finally, we also propose a strategy to test for the constancy of the conditional betas. In a financial application, we find evidence for time-varying conditional betas and highlight the empirical relevance of the ACB model in a portfolio and risk management empirical exercise.",http://www.sciencedirect.com/science/article/pii/S0304407623003469
Journal of Econometrics,2024,The likelihood ratio test for structural changes in factor models,"Jushan Bai, Jiangtao Duan and Xu Han","A factor model with a break in its factor loadings is observationally equivalent to a model without changes in the loadings but with a change in the variance of its factors. This approach effectively transforms a high-dimensional structural change problem into a low-dimensional problem. This paper considers the likelihood ratio (LR) test for a variance change in the estimated factors. The LR test implicitly explores a special feature of the estimated factors: the pre-break and post-break variances can be a singular matrix under the alternative hypothesis, making the LR test diverging faster and thus more powerful than Wald-type tests. The better power property of the LR test is also confirmed by simulations. We also consider mean changes and multiple breaks. We apply this procedure to the factor modeling of the US employment and study the structural change problem using monthly industry-level data.",http://www.sciencedirect.com/science/article/pii/S0304407623003470
Journal of Econometrics,2024,Bellman filtering and smoothing for state–space models,Rutger-Jan Lange,"This paper presents a new filter for state–space models based on Bellman’s dynamic-programming principle, allowing for nonlinearity, non-Gaussianity and degeneracy in the observation and/or state-transition equations. The resulting Bellman filter is a direct generalisation of the (iterated and extended) Kalman filter, enabling scalability to higher dimensions while remaining computationally inexpensive. It can also be extended to enable smoothing. Under suitable conditions, the Bellman-filtered states are stable over time and contractive towards a region around the true state at every time step. Static (hyper)parameters are estimated by maximising a filter-implied pseudo log-likelihood decomposition. In univariate simulation studies, the Bellman filter performs on par with state-of-the-art simulation-based techniques at a fraction of the computational cost. In two empirical applications, involving up to 150 spatial dimensions or highly degenerate/nonlinear state dynamics, the Bellman filter outperforms competing methods in both accuracy and speed.",http://www.sciencedirect.com/science/article/pii/S0304407623003482
Journal of Econometrics,2024,Advances in nowcasting economic activity: The role of heterogeneous dynamics and fat tails,"Juan Antolín-Díaz, Thomas Drechsel and Ivan Petrella","A key question for households, firms, and policy makers is: how is the economy doing now? This paper develops a Bayesian dynamic factor model that allows for nonlinearities, heterogeneous lead–lag patterns and fat tails in macroeconomic data. Explicitly modeling these features changes the way that different indicators contribute to the real-time assessment of the state of the economy, and substantially improves the out-of-sample performance of this class of models. In a formal evaluation, our nowcasting framework beats benchmark econometric models and professional forecasters at predicting US GDP growth in real time.",http://www.sciencedirect.com/science/article/pii/S0304407623003500
Journal of Econometrics,2024,Observation-driven filtering of time-varying parameters using moment conditions,"Drew Creal, Siem Jan Koopman, Andre Lucas and Marcin Zamojski","We develop a new and flexible semi-parametric approach for time-varying parameter models when the true dynamics are unknown. The time-varying parameters are estimated using a recursive updating scheme that is driven by the influence function of a conditional moments-based criterion. We show that the updates ensure local improvements of the conditional criterion function in expectation. The dynamics are observation driven, which yields a computationally efficient methodology that does not require advanced simulation techniques for estimation. We illustrate the new approach using both simulated and real empirical data and derive new, robust filters for time-varying scales based on characteristic functions.",http://www.sciencedirect.com/science/article/pii/S0304407623003512
Journal of Econometrics,2024,Identification of heterogeneous elasticities in gross-output production functions,Tong Li and Yuya Sasaki,"This paper presents the identification of heterogeneous elasticities in gross-output production functions with non-separable unobserved productivity. We propose that the ex-ante flexible input cost shares identify the heterogeneous output elasticities with respect to flexible inputs for each firm. Applying the proposed method to a panel of firms in the food production industry in Chile, we find that the extent of heterogeneity in the output elasticities with respect to intermediate inputs has significantly declined from 1985 to 1995, with its distribution tending to concentrate toward zero as time progresses.",http://www.sciencedirect.com/science/article/pii/S0304407623003536
Journal of Econometrics,2024,Estimation and inference by stochastic optimization,Jean-Jacques Forneron,"In non-linear estimations, it is common to assess sampling uncertainty by bootstrap inference. For complex models, this can be computationally intensive. This paper combines optimization with resampling: turning stochastic optimization into a fast resampling device. Two methods are introduced: a resampled Newton–Raphson (rnr) and a resampled quasi-Newton (rqn) algorithm. Both produce draws that can be used to compute consistent estimates, confidence intervals, and standard errors in a single run. The draws are generated by a gradient and Hessian (or an approximation) computed from batches of data that are resampled at each iteration. The proposed methods transition quickly from optimization to resampling when the objective is smooth and strictly convex. Simulated and empirical applications illustrate the properties of the methods on large scale and computationally intensive problems. Comparisons with frequentist and Bayesian methods highlight the features of the algorithms.",http://www.sciencedirect.com/science/article/pii/S0304407623003548
Journal of Econometrics,2024,Nonparametric estimation of stochastic frontier models with weak separability,Samuele Centorrino and Christopher F. Parmeter,"We propose a robust and versatile approach to estimate the stochastic frontier model which avoids parametric assumptions. Our approach requires a single continuous covariate which monotonically influences the conditional mean of inefficiency. Subject to these conditions, the frontier and the conditional mean of inefficiency can be estimated nonparametrically. The estimator we propose uses local least squares and marginal integration making it easy to implement across statistical software. A range of Monte Carlo simulations suggests that when our main identification condition holds, our proposed estimator outperforms other proposals that currently exist. Finally, we provide an application to the study of undercounting COVID-19 cases across the United States. Whereas our method indicates significant undercounting, consistent with existing evidence, other nonparametric methods suggest far less undercounting.",http://www.sciencedirect.com/science/article/pii/S0304407623003573
Journal of Econometrics,2024,Semiparametric Bayesian estimation of dynamic discrete choice models,Andriy Norets and Kenichi Shimizu,"We propose a tractable semiparametric estimation method for structural dynamic discrete choice models. The distribution of additive utility shocks in the proposed framework is modeled by location-scale mixtures of extreme value distributions with varying numbers of mixture components. Our approach exploits the analytical tractability of extreme value distributions in the multinomial choice settings and the flexibility of the location-scale mixtures. We implement the Bayesian approach to inference using Hamiltonian Monte Carlo and an approximately optimal reversible jump algorithm. In our simulation experiments, we show that the standard dynamic logit model can deliver misleading results, especially about counterfactuals, when the shocks are not extreme value distributed. Our semiparametric approach delivers reliable inference in these settings. We develop theoretical results on approximations by location-scale mixtures in an appropriate distance and posterior concentration of the set identified utility parameters and the distribution of shocks in the model.",http://www.sciencedirect.com/science/article/pii/S0304407623003585
Journal of Econometrics,2024,Systematic staleness,"Federico M. Bandi, Davide Pirino and Roberto Renò","Asset prices are stale. We define a measure of systematic (market-wide) staleness as the percentage of small price adjustments across multiple assets. A notion of idiosyncratic (asset-specific) staleness is also established. For both systematic and idiosyncratic staleness, we provide a limit theory based on joint asymptotics relying on increasingly-frequent observations over a fixed time span and an increasing number of assets. Using systematic and idiosyncratic staleness as moment conditions, we introduce novel structural estimates of systematic and idiosyncratic measures of liquidity obtained from transaction prices only. The economic signal contained in the structural estimates is assessed by virtue of suitable metrics.",http://www.sciencedirect.com/science/article/pii/S0304407623002385
Journal of Econometrics,2024,Hypothesis testing on high dimensional quantile regression,"Zhao Chen, Vivian Xinyi Cheng and Xu Liu","Quantile regression has been an important analytical tool in econometrics since it was proposed in 1970s. Many advantages make it still popular in the era of big data. This paper focuses on the testing problems of high-dimensional quantile regression, supplementing to robust methods in the literature of high-dimensional hypothesis testing. We first construct a new test statistic based on the quantile regression score function. The new test statistic avoids the inverse operation of the covariance matrix, and hence becomes applicable to high-dimensional or even ultrahigh-dimensional settings. The proposed method retains robustness for non-Gaussian and heavy-tailed distributions. We then derive the limiting distributions of the proposed test statistic under both the null and the alternative hypotheses. We further investigate the case where the design matrix follows an elliptical distribution. We examined the finite sample performance of our proposed method through Monte Carlo simulations. The numerical comparisons exhibit that our proposed tests outperform some existing methods in terms of controlling Type I error rate and power, when the data deviate from the Gaussian assumptions or are heavy-tailed. We illustrate our proposed high-dimensional quantile testing in financial econometrics, through an empirical analysis of Chinese stock market data.",http://www.sciencedirect.com/science/article/pii/S0304407623002415
Journal of Econometrics,2024,High-dimensional low-rank tensor autoregressive time series modeling,"Di Wang, Yao Zheng and Guodong Li","Modern technological advances have enabled an unprecedented amount of structured data with complex temporal dependence, urging the need for new methods to efficiently model and forecast high-dimensional tensor-valued time series. This paper provides a new modeling framework to accomplish this task via autoregression (AR). By considering a low-rank Tucker decomposition for the transition tensor, the proposed tensor AR can flexibly capture the underlying low-dimensional tensor dynamics, providing both substantial dimension reduction and meaningful multi-dimensional dynamic factor interpretations. For this model, we first study several nuclear-norm-regularized estimation methods and derive their non-asymptotic properties under the approximate low-rank setting. In particular, by leveraging the special balanced structure of the transition tensor, a novel convex regularization approach based on the sum of nuclear norms of square matricizations is proposed to efficiently encourage low-rankness of the coefficient tensor. To further improve the estimation efficiency under exact low-rankness, a non-convex estimator is proposed with a gradient descent algorithm, and its computational and statistical convergence guarantees are established. Simulation studies and an empirical analysis of tensor-valued time series data from multi-category import-export networks demonstrate the advantages of the proposed approach.",http://www.sciencedirect.com/science/article/pii/S0304407623002609
Journal of Econometrics,2024,Simultaneously Incomplete and Incoherent (SII) Dynamic LDV Models: With an Application to Financing Constraints and Firms’ Decision to Innovate,Vassilis Hajivassiliou and Frédérique Savignac,"We develop novel methods for establishing coherency and completeness conditions in Static and Dynamic Limited Dependent Variables (LDV) Models. We characterize the two distinct problems as “empty-region” incoherency and “overlap-region” incoherency or incompleteness and show that the two properties can co-exist. We focus on the class of models that can be Simultaneously Incomplete and Incoherent (SII). We propose estimation strategies based on Conditional Maximum Likelihood Estimation (CMLE) for simultaneous dynamic LDV models without imposing recursivity. Point identification is achieved through sign-restrictions on parameters or other prior assumptions that complete the underlying data process. Using as modelling framework the Panel Bivariate Probit model with State Dependence, we analyse the impact of financing constraints on innovation: ceteris paribus, a firm facing binding finance constraints is substantially less likely to undertake innovation, while the probability that a firm encounters a binding finance constraint more than doubles if the firm is innovative. In addition, a strong role for state dependence in dynamic versions of our models is established.",http://www.sciencedirect.com/science/article/pii/S0304407623002622
Journal of Econometrics,2024,Optimal nonparametric range-based volatility estimation,"Tim Bollerslev, Jia Li and Qiyuan Li","We present a general framework for optimal nonparametric spot volatility estimation based on intraday range data, comprised of the first, highest, lowest, and last price over a given time-interval. We rely on a decision-theoretic approach together with a coupling-type argument to directly tailor the form of the nonparametric estimator to the specific volatility measure of interest and relevant loss function. The resulting new optimal estimators offer substantial efficiency gains compared to existing commonly used range-based procedures.",http://www.sciencedirect.com/science/article/pii/S0304407623002646
Journal of Econometrics,2024,Local linearization based subvector inference in moment inequality models,Xinyue Bei,"This paper introduces a bootstrap-based profiling inference method for subvectors in moment inequality models following insights from Bugni et al. (2017). Compared to their paper, the new method calculates the critical value by searching over a local neighborhood of a pre-estimator, instead of the whole null parameter space, to profile out nuisance parameters. In this way, non-linear moment conditions are simplified by linear expansion and the bootstrap iterates over quadratic programming problems, which significantly simplifies and accelerates computation. This method controls asymptotic size uniformly over a large class of data generating processes. In the Monte Carlo simulations, the new procedure improves upon the computing time of Bugni et al. (2017) and Kaido et al. (2019) significantly. I provide an empirical illustration estimating an airline entry game.",http://www.sciencedirect.com/science/article/pii/S0304407623002658
Journal of Econometrics,2024,"Binary choice with misclassification and social interactions, with an application to peer effects in attitude",Zhongjian Lin and Yingyao Hu,"The interaction of economic agents is one of the most important elements in economic analyses. Social interactions on subjective outcomes, behavior, or decisions, are inherently difficult to identify and estimate because these variables are prone to misclassification errors. This paper puts forth a binary choice model with misclassification and social interactions to rectify the misclassification problems in social interactions studies. We achieve the identification of the conditional choice probability of the latent dependent variable by the technique of repeated measurements and a monotonicity condition. We construct the complete likelihood function from the two repeated measurements and propose a nested pseudo likelihood algorithm for estimation. Consistency and asymptotic normality results are shown for the proposed estimation method. We illustrate the finite sample performance of the model and the estimation method by three Monte Carlo experiments and an application to the study of peer effects among students in their attitudes towards learning.",http://www.sciencedirect.com/science/article/pii/S0304407623002671
Journal of Econometrics,2024,Detecting identification failure in moment condition models,Jean-Jacques Forneron,"This paper develops an approach to detect identification failure in moment condition models. This is achieved by introducing a quasi-Jacobian matrix computed as the slope of a linear approximation of the moments on an estimate of the identified set. It is asymptotically singular when local and/or global identification fails, and equivalent to the usual Jacobian matrix which has full rank when the model is globally and locally identified. Building on this property, a simple test with chi-squared critical values is introduced to conduct subvector inferences allowing for strong, semi-strong, and weak identification without a priori knowledge about the underlying identification structure. Monte-Carlo simulations and an empirical application to the Long-Run Risks model illustrate the results.",http://www.sciencedirect.com/science/article/pii/S0304407623002683
Journal of Econometrics,2024,Inference in models with partially identified control functions,Andres Aradillas-Lopez,"In multiple contributions to the literature, James L. Powell and coauthors have developed estimators for semiparametric models where sample selectivity and/or endogeneity can be handled through a “control function”. Their methods rely on pairwise comparisons of observations which match (asymptotically) the control functions. Conditional on this matching, a moment condition can identify the parameters of the model. However, there exist instances where the control functions are unobserved, but we have bounds for them which depend on observable covariates. These bounds can arise directly from the nature of the data available (e.g, with interval data), or they can be derived from an economic model. The inability to observe the control functions precludes the matching proposed in Powell’s methods. In this paper we show that, under certain conditions, testable implications can still be obtained through pairwise comparisons of observations for which the control-function bounds are disjoint. Testable implications now take the form of pairwise functional inequalities. We propose an inferential procedure based on these pairwise inequalities and we analyze its properties.",http://www.sciencedirect.com/science/article/pii/S0304407623002695
Journal of Econometrics,2024,Causal inference of general treatment effects using neural networks with a diverging number of confounders,"Xiaohong Chen, Ying Liu, Shujie Ma and Zheng Zhang","Semiparametric efficient estimation of various multi-valued causal effects, including quantile treatment effects, is important in economic, biomedical, and other social sciences. Under the unconfoundedness condition, adjustment for confounders requires estimating the nuisance functions relating outcome or treatment to confounders nonparametrically. This paper considers a generalized optimization framework for efficient estimation of general treatment effects using artificial neural networks (ANNs) to approximate the unknown nuisance function of growing-dimensional confounders. We establish a new approximation error bound for the ANNs to the nuisance function belonging to a mixed smoothness class without a known sparsity structure. We show that the ANNs can alleviate the “curse of dimensionality” under this circumstance. We establish the root-n consistency and asymptotic normality of the proposed general treatment effects estimators, and apply a weighted bootstrap procedure for conducting inference. The proposed methods are illustrated via simulation studies and a real data application.",http://www.sciencedirect.com/science/article/pii/S0304407623002713
Journal of Econometrics,2024,Bounding program benefits when participation is misreported,Denni Tommasi and Lina Zhang,"Instrumental variables (IV) are commonly used to estimate treatment effects in case of noncompliance. However, program participation is often misreported in survey data and standard techniques are not sufficient to point identify and consistently estimate the effects of interest. In this paper, we show that the identifiable IV estimand that ignores treatment misclassification is a weighted average of local average treatment effects with weights that can also be negative. This is troublesome because it may fail to deliver a correct causal interpretation, and this is true even if all the weights are non-negative. Therefore, we provide three IV strategies to bound the program benefits when both noncompliance and misreporting are present. We demonstrate the gain of identification power achieved by leveraging multiple exogenous variations when discrete or multiple-discrete IVs are available. At last, we use our new Stata command, ivbounds, to study the benefits of participating in the 401(k) pension plan on savings.",http://www.sciencedirect.com/science/article/pii/S0304407623002725
Journal of Econometrics,2024,Estimation and bootstrapping under spatiotemporal models with unobserved heterogeneity,"Xingdong Feng, Wenyu Li and Qianqian Zhu","Proposed herein is a novel spatiotemporal model to characterize the unobserved heterogeneity across individuals using quantile-function-based correlated random effects and heteroscedastic innovations in a general framework. This model can be used to explore the influence of space-specific factors on latent effects at different quantile levels by controlling for spatiotemporal effects. A two-stage estimation procedure is introduced in which (i) the method of moments is used to estimate spatiotemporal effects then (ii) quantile regression is used for individual effects. A hybrid double bootstrapping procedure is then proposed to approximate the asymptotic distributions of coefficient estimators. The validity of the estimation and bootstrapping is established theoretically and then confirmed by simulation studies, and the usefulness of the proposed model is demonstrated with a real example involving city air quality.",http://www.sciencedirect.com/science/article/pii/S0304407623002750
Journal of Econometrics,2024,Nonparametric Gini-Frisch bounds,Karim Chalak,"The Gini-Frisch bounds partially identify the constant slope coefficient in a linear equation when the explanatory variable suffers from classical measurement error. This paper generalizes these quintessential bounds to accommodate nonparametric heterogeneous effects. It provides suitable conditions under which the main insights that underlie the Gini-Frisch bounds apply to partially identify the average marginal effect of an error-laden variable in a nonparametric nonseparable equation. To this end, the paper puts forward a nonparametric analogue to the standard “forward” and “reverse” linear regression bounds. The nonparametric forward regression bound generalizes the linear regression “attenuation bias” due to classical measurement error.",http://www.sciencedirect.com/science/article/pii/S0304407623002762
Journal of Econometrics,2024,Identification of multi-valued treatment effects with unobserved heterogeneity,Koki Fusejima,"In this paper, we establish sufficient conditions for identifying treatment effects on continuous outcomes in endogenous and multi-valued discrete treatment settings with unobserved heterogeneity. We employ the monotonicity assumption for multi-valued discrete treatments and instruments, and our identification condition has a clear economic interpretation. In addition, we identify the local treatment effects in multi-valued treatment settings and derive closed-form expressions of the identified treatment effects. We provide examples to illustrate the usefulness of our result.",http://www.sciencedirect.com/science/article/pii/S0304407623002798
Journal of Econometrics,2024,Population interference in panel experiments,"Kevin Han, Guillaume Basse and Iavor Bojinov","The phenomenon of population interference, where a treatment assigned to one experimental unit affects another experimental unit’s outcome, has received considerable attention in standard randomized experiments. The complications produced by population interference in this setting are now readily recognized, and partial remedies are well known. Less understood is the impact of population interference in panel experiments where treatment is sequentially randomized in the population, and the outcomes are observed at each time step. This paper proposes a general framework for studying population interference in panel experiments and presents new finite population estimation and inference results. Our findings suggest that, under mild assumptions, the addition of a temporal dimension to an experiment alleviates some of the challenges of population interference for certain estimands. In contrast, we show that the presence of carryover effects — that is, when past treatments may affect future outcomes — exacerbates the problem. Our results are illustrated through both an empirical analysis and an extensive simulation study.",http://www.sciencedirect.com/science/article/pii/S0304407623002816
Journal of Econometrics,2024,Endogeneity in weakly separable models without monotonicity,"Songnian Chen, Shakeeb Khan and Xun Tang","We identify and estimate treatment effects when potential outcomes are weakly separable with a binary endogenous treatment. Vytlacil and Yildiz (2007) proposed an identification strategy that exploits the mean of observed outcomes, but their approach requires a monotonicity condition. In comparison, we exploit full information in the entire outcome distribution, instead of just its mean. As a result, our method does not require monotonicity and is also applicable to general settings with multiple indices. We provide examples where our approach can identify treatment effect parameters of interest whereas existing methods would fail. These include models where potential outcomes depend on multiple unobserved disturbance terms, such as a Roy model, a multinomial choice model, as well as a model with endogenous random coefficients. We establish consistency and asymptotic normality of our estimators.",http://www.sciencedirect.com/science/article/pii/S030440762300283X
Journal of Econometrics,2024,Tuning parameter-free nonparametric density estimation from tabulated summary data,"Ji Hyung Lee, Yuya Sasaki, Alexis Akira Toda and Yulong Wang","Administrative data are often easier to access as tabulated summaries than in the original format due to confidentiality concerns. Motivated by this practical feature, we propose a novel nonparametric density estimation method from tabulated summary data based on maximum entropy and prove its strong uniform consistency. Unlike existing kernel-based estimators, our estimator is free from tuning parameters and admits a closed-form density that is convenient for post-estimation analysis. We apply the proposed method to the tabulated summary data of the U.S. tax returns to estimate the income distribution.",http://www.sciencedirect.com/science/article/pii/S0304407623002841
Journal of Econometrics,2024,Asset pricing with neural networks: Significance tests,"Hasan Fallahgoul, Vincentius Franstianto and Xin Lin","This study proposes a novel hypothesis test for evaluating the statistical significance of input variables in multi-layer perceptron (MLP) regression models. Theoretical foundations are established through consistency results and estimation rate analysis using the sieves method. To validate the test’s performance in complex and realistic settings, an extensive Monte Carlo simulation is conducted. Results of the simulation reveal that the test has a high power and low rate of false positives, making it a powerful tool for detecting true effects in data. The test is further applied to identify the most influential predictors of equity risk premiums, with results indicating that only a small number of characteristics have statistical significance and all macroeconomic predictors are insignificant at the 1% level. These findings are consistent across a variety of neural network architectures.",http://www.sciencedirect.com/science/article/pii/S0304407623002907
Journal of Econometrics,2024,Maximum Likelihood Estimation for Non-Stationary Location Models with Mixture of Normal Distributions,"Francisco Blasques, Janneke van Brummelen, Paolo Gorgi and Siem Jan Koopman",We consider an observation-driven location model where the unobserved location variable is modeled as a random walk process and where the error variable is from a mixture of normal distributions. The time-varying location can be extended with a stationary process to account for cyclical and/or higher order autocorrelation. The mixed normal distribution can accurately approximate many continuous error distributions. We obtain a flexible modeling framework for the robust filtering and forecasting based on time-series models with non-stationary and nonlinear features. We provide sufficient conditions for strong consistency and asymptotic normality of the maximum likelihood estimator of the parameter vector in the specified model. The asymptotic properties are valid under correct model specification and can be generalized to allow for potential misspecification of the model. A simulation study is carried out to monitor the forecast accuracy improvements when extra mixture components are added to the model. In an empirical study we show that our approach is able to outperform alternative observation-driven location models in forecast accuracy for a time-series of electricity spot prices.,http://www.sciencedirect.com/science/article/pii/S0304407623002919
Journal of Econometrics,2024,Semi-parametric single-index predictive regression models with cointegrated regressors,"Weilun Zhou, Jiti Gao, David Harris and Hsein Kew","This paper considers the estimation of a semi-parametric single-index regression model that allows for nonlinear predictive relationships. This model is useful for predicting financial asset returns, whose observed behaviour resembles a stationary process, if the multiple nonstationary predictors are cointegrated. The presence of cointegrated regressors imposes a single-index structure in the model, and this structure not only balances the nonstationarity properties of the multiple predictors with the stationarity properties of asset returns but also avoids the curse of dimensionality associated with nonparametric regression function estimation. An orthogonal series expansion is used to approximate the unknown link function for the single-index component. We consider the constrained nonlinear least squares estimator of the single-index (or the cointegrating) parameters and the plug-in estimator of the link function, and derive their asymptotic properties. In an empirical application, we find some evidence of in-sample nonlinear predictability of U.S. stock returns using cointegrated predictors. We also find that the single-index model in general produces better out-of-sample forecasts than both the historical average benchmark and the linear predictive regression model.",http://www.sciencedirect.com/science/article/pii/S0304407623002932
Journal of Econometrics,2024,Rank-based max-sum tests for mutual independence of high-dimensional random vectors,"Hongfei Wang, Binghui Liu, Long Feng and Yanyuan Ma","We consider the problem of testing mutual independence of high-dimensional random vectors, and propose a series of high-dimensional rank-based max-sum tests, which are suitable for high-dimensional data and can be robust to distribution types of the variables, form of the dependence between variables and the sparsity of correlation coefficients. Further, we demonstrate the application of some representative members of the proposed tests on testing cross-sectional independence of the error vectors under fixed effects panel data regression models. We establish the asymptotic properties of the proposed tests under the null and alternative hypotheses, respectively, and then demonstrate the superiority of the proposed tests through extensive simulations, which suggest that they combine the advantages of both the max-type and sum-type high-dimensional rank-based tests. Finally, a real panel data analysis is performed to illustrate the application of the proposed tests.",http://www.sciencedirect.com/science/article/pii/S0304407623002944
Journal of Econometrics,2024,Matching points: Supplementing instruments with covariates in triangular models,Junlong Feng,"Models with a discrete endogenous variable and an instrument that takes on fewer values are common in economics. This paper presents a new method that matches pairs of covariates and instruments to restore the order condition in this scenario and to achieve point-identification of the outcome function. The outcome function must be monotonic in a scalar disturbance, but it can be nonseparable. The first stage for the discrete endogenous variable needs to have a multi-index structure but allows for multidimensional heterogeneity. This paper also provides estimators of the outcome function. Two empirical examples of the return to education and of selection into Head Start illustrate the usefulness and limitations of the method.",http://www.sciencedirect.com/science/article/pii/S0304407623002956
Journal of Econometrics,2024,"Mental health and abortions among young women: time-varying unobserved heterogeneity, health behaviors, and risky decisions",Lena Janys and Bettina Siflinger,"In this paper, we provide causal evidence on abortions and risky health behaviors as determinants of mental health development among young women. Using administrative in- and outpatient records from Sweden, we apply a novel grouped fixed-effects estimator proposed by Bonhomme and Manresa (2015) to allow for time-varying unobserved heterogeneity. We show that the positive association obtained from standard estimators shrinks to zero once we control for grouped unobserved heterogeneity that varies across ages. We estimate the group profiles of unobserved heterogeneity, which reflect differences in unobserved risk to be diagnosed with a mental health condition. In addition, we analyze mental health development and risky health behaviors other than unintended pregnancies across groups. Our results suggest that these are determined by the same type of unobserved heterogeneity, which we attribute to the same unobserved process of decision-making. We develop and estimate a theoretical model of risky choices and mental health that is consistent with our empirical results. In the model, mental health disparity across groups is generated by different degrees of self-control problems. Our findings imply that mental health concerns cannot be used to justify restrictive abortion policies.",http://www.sciencedirect.com/science/article/pii/S0304407623002968
