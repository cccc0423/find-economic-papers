journal,year,title,authors,abstract,url
Journal of Econometrics,2023,Volatility measurement with pockets of extreme return persistence,"Torben Andersen, Yingying Li, Viktor Todorov and Bo Zhou","Increasing evidence points towards the episodic emergence of pockets with extreme return persistence. This notion refers to intraday periods of non-trivial duration, for which stock returns are highly positively autocorrelated. Such episodes include, but are not limited to, gradual jumps and prolonged bursts in the drift component. In this paper, we develop a family of integrated volatility estimators, labeled differenced-return volatility (DV) estimators, which provide robustness to these types of Itô semimartingale violations. Specifically, we show that, by using differences in consecutive high-frequency returns, our DV estimators can reduce the non-trivial bias that all commonly-used estimators exhibit during such periods of apparent short-term intraday return predictability. A Monte Carlo study demonstrates the reliability of the newly developed volatility estimators in finite samples. In our empirical volatility forecasting application to S&P 500 index futures and individual equities, our DV-based Heterogeneous Autoregressive (HAR) model performs well relative to existing procedures according to standard out-of-sample MSE and QLIKE criteria.",http://www.sciencedirect.com/science/article/pii/S0304407620303924
Journal of Econometrics,2023,Two-stage weighted least squares estimator of the conditional mean of observation-driven time series models,Abdelhakim Aknouche and Christian Francq,"General parametric forms are assumed for the conditional mean λt(θ0) and variance υt of a time series. These conditional moments can for instance be derived from count time series, Autoregressive Conditional Duration or Generalized Autoregressive Score models. In this paper, our aim is to estimate the conditional mean parameter θ0, trying to be as agnostic as possible about the conditional distribution of the observations. Quasi-Maximum Likelihood Estimators (QMLEs) based on the linear exponential family fulfill this goal, but they may be inefficient and have complicated asymptotic distributions when θ0 contains boundary coefficients. We thus study alternative Weighted Least Square Estimators (WLSEs), which enjoy the same consistency property as the QMLEs when the conditional distribution is misspecified, but have simpler asymptotic distributions when components of θ0 are null and gain in efficiency when υt is well specified. We compare the asymptotic properties of the QMLEs and WLSEs, and determine a data driven strategy for finding an asymptotically optimal WLSE. Simulation experiments and illustrations on realized volatility forecasting are presented.",http://www.sciencedirect.com/science/article/pii/S030440762100213X
Journal of Econometrics,2023,Dynamic conditional eigenvalue GARCH,"Simon Hetland, Rasmus Søndergaard Pedersen and Anders Rahbek","In this paper we introduce a multivariate generalized autoregressive conditional heteroskedastic (GARCH) class of models with time-varying conditional eigenvalues. The dynamics of the eigenvalues is derived for the cases with underlying Gaussian and Student’s t-distributed innovations based on the general theory of dynamic conditional score models by Creal, Koopman and Lucas (2013) and Harvey (2013). The resulting time-varying eigenvalue GARCH models – labeled ‘λ-GARCH’ models – differ for the two cases of innovations, similar to, and generalizing, univariate linear Gaussian GARCH and Student’s t-based Beta-t-GARCH models. Asymptotic theory is provided for the Gaussian-based quasi-maximum likelihood estimator (QMLE). In addition, and in order to test for the number of (linear combinations of) the time-varying eigenvalues, we consider testing and inference under the hypothesis of reduced rank of the GARCH loading matrices. The conditional Gaussian and Student’s t distributed λ-GARCH models are applied to US return data, and it is found that the eigenvalue structure for the sample considered indeed satisfies the hypothesis of reduced rank. Specifically, it is possible to disentangle time-varying linear combinations of the eigenvalues, or factors, from time-invariant factors which drive the dynamics of the conditional covariance.",http://www.sciencedirect.com/science/article/pii/S0304407621002141
Journal of Econometrics,2023,A dynamic conditional score model for the log correlation matrix,Christian M. Hafner and Linqi Wang,"This paper proposes a new model for the dynamics of correlation matrices, where the dynamics are driven by the likelihood score with respect to the matrix logarithm of the correlation matrix. In analogy to the exponential GARCH model for volatility, this transformation ensures that the correlation matrices remain positive definite, even in high dimensions. For the conditional distribution of returns, we assume a student-t copula to explain the dependence structure and univariate student-t for the marginals with potentially different degrees of freedom. The separation into volatility and correlation parts allows for a two-step estimation, which facilitates estimation in high dimensions. We derive estimation theory for one-step and two-step estimation. In an application to a set of six asset indices including financial and alternative assets we show that the model performs well in terms of diagnostics, specification tests, and out-of-sample forecasting.",http://www.sciencedirect.com/science/article/pii/S0304407621002153
Journal of Econometrics,2023,Beta observation-driven models with exogenous regressors: A joint analysis of realized correlation and leverage effects,P. Gorgi and Siem Jan Koopman,"We consider a general class of observation-driven models with exogenous regressors for double bounded data that are based on the beta distribution. We obtain a stationary and ergodic beta observation-driven process subject to a contraction condition on the stochastic dynamic model equation. We derive conditions for strong consistency and asymptotic normality of the maximum likelihood estimator. The general results are used to study the properties of a beta autoregressive process with threshold effects and to establish the asymptotic properties of the maximum likelihood estimator. We employ beta autoregressive models to describe leverage effects in realized correlations for several pairs of stocks. We find that the impact of past values of realized correlation on future values is significantly higher when stock or market returns are negative rather than positive. The results also indicate that the size of the leverage depends on the magnitude of the market news as measured by daily market returns. These findings support the conjecture that correlation between stock returns tends to be higher when stock prices are falling, and lower when there is a surge in stock prices. Finally, we conduct an out-of-sample study that shows that the models with leverage effects can enhance the accuracy of point and density forecasts of realized correlations.",http://www.sciencedirect.com/science/article/pii/S0304407621002165
Journal of Econometrics,2023,Comparing forecasting performance in cross-sections,"Ritong Qu, Allan Timmermann and Yinchu Zhu","This paper develops new methods for pairwise comparisons of predictive accuracy with cross-sectional data. Using a common factor setup, we establish conditions on cross-sectional dependencies in forecast errors which allow us to test the null of equal predictive accuracy on a single cross-section of forecasts. We consider both unconditional tests of equal predictive accuracy as well as tests that condition on the realization of common factors and show how to decompose forecast errors into exposures to common factors and idiosyncratic components. An empirical application compares the predictive accuracy of financial analysts’ short-term earnings forecasts across six brokerage firms.",http://www.sciencedirect.com/science/article/pii/S0304407621002256
Journal of Econometrics,2023,Evaluating forecast performance with state dependence,"Florens Odendahl, Barbara Rossi and Tatevik Sekhposyan","We propose a novel forecast evaluation methodology to assess models’ absolute and relative forecasting performance when it is a state-dependent function of economic variables. In our framework, the forecasting performance, measured by a forecast error loss function, is modeled via a hard or smooth threshold model with unknown threshold values. Existing tests either assume a constant out-of-sample forecast performance or use non-parametric techniques robust to time-variation; consequently, they may lack power against a state-dependent performance. Our tests can be applied to relative forecast comparisons, forecast encompassing, forecast efficiency, and, more generally, moment-based tests of forecast evaluation. Monte Carlo results suggest that our proposed tests perform well in finite samples and have better power than existing tests in selecting the best forecast or assessing its efficiency in the presence of state dependence. Our tests uncover “pockets of predictability” in U.S. equity premia; although the term spread is not a useful predictor on average over the sample, it forecasts significantly better than the benchmark forecast when real GDP growth is low. In addition, we find that leading indicators, such as measures of vacancy postings and new orders for durable goods, improve the forecasts of U.S. industrial production when financial conditions are tight.",http://www.sciencedirect.com/science/article/pii/S0304407621002657
Journal of Econometrics,2023,CRPS learning,Jonathan Berrisch and Florian Ziel,"Combination and aggregation techniques can significantly improve forecast accuracy. This also holds for probabilistic forecasting methods where predictive distributions are combined. There are several time-varying and adaptive weighting schemes such as Bayesian model averaging (BMA). However, the quality of different forecasts may vary not only over time but also within the distribution. For example, some distribution forecasts may be more accurate in the center of the distributions, while others are better at predicting the tails. Therefore, we introduce a new weighting method that considers the differences in performance over time and within the distribution. We discuss pointwise combination algorithms based on aggregation across quantiles that optimize with respect to the continuous ranked probability score (CRPS). After analyzing the theoretical properties of pointwise CRPS learning, we discuss B- and P-Spline-based estimation techniques for batch and online learning, based on quantile regression and prediction with expert advice. We prove that the proposed fully adaptive Bernstein online aggregation (BOA) method for pointwise CRPS online learning has optimal convergence properties. They are confirmed in simulations and a probabilistic forecasting study for European emission allowance (EUA) prices.",http://www.sciencedirect.com/science/article/pii/S0304407621002724
Journal of Econometrics,2023,Extensions to IVX methods of inference for return predictability,"Matei Demetrescu, Iliyan Georgiev, Paulo Rodrigues and Robert Taylor","The contribution of this paper is threefold. First, we demonstrate that, provided either a suitable bootstrap implementation is employed or heteroskedasticity-consistent standard errors are used, the IVX-based predictability tests of Kostakis et al. (2015) retain asymptotically valid inference under the null hypothesis under considerably weaker assumptions on the innovations than are required by Kostakis et al. (2015). Second, under the same assumptions, we develop asymptotically valid bootstrap implementations of the IVX tests. Monte Carlo simulations show that the bootstrap tests deliver considerably more accurate finite sample inference than the asymptotic implementations of the tests under certain problematic parameter constellations, most notably for one-sided testing, and where multiple predictors are included. Third, we show how sub-sample implementations of the IVX approach can be used to develop asymptotically valid one-sided and two-sided tests for the presence of temporary windows of predictability.",http://www.sciencedirect.com/science/article/pii/S0304407622000586
Journal of Econometrics,2023,Dynamic clustering of multivariate panel data,"Igor Custodio João, Andre Lucas, Julia Schaumburg and Bernd Schwaab","We propose a dynamic clustering model for uncovering latent time-varying group structures in multivariate panel data. The model is dynamic in three ways. First, the cluster location and scale matrices are time-varying to track gradual changes in cluster characteristics over time. Second, all units can transition between clusters based on a Hidden Markov model (HMM). Finally, the HMM’s transition matrix can depend on lagged time-varying cluster distances as well as economic covariates. Monte Carlo experiments suggest that the units can be classified reliably in a variety of challenging settings. Incorporating dynamics in the cluster composition proves empirically important in a study of 299 European banks between 2008Q1 and 2018Q2. We find that approximately 3% of banks transition per quarter on average. Transition probabilities are in part explained by differences in bank profitability, suggesting that factors contributing to low profitability for some banks can lead to long-lasting changes in financial industry structure.",http://www.sciencedirect.com/science/article/pii/S0304407622000689
Journal of Econometrics,2023,Optimal model averaging based on forward-validation,Xiaomeng Zhang and Xinyu Zhang,"In this paper, noting that the prediction of time series follows the temporal order of data, we propose a frequentist model averaging method based on forward-validation. Our method also considers the uncertainty of the window size in estimation, i.e., we allow the sample size to vary among candidate models. We establish the asymptotic optimality of our method in the sense of achieving the lowest possible squared prediction risk. We also prove that if there exists one or more correctly specified models, our method will automatically assign all the weights to them. The promising performance of our method for finite samples is demonstrated by simulations and an empirical example of predicting the equity premium.",http://www.sciencedirect.com/science/article/pii/S030440762200094X
Journal of Econometrics,2023,Machine learning panel data regressions with heavy-tailed dependent data: Theory and application,"Andrii Babii, Ryan T. Ball, Eric Ghysels and Jonas Striaukas","The paper introduces structured machine learning regressions for heavy-tailed dependent panel data potentially sampled at different frequencies. We focus on the sparse-group LASSO regularization. This type of regularization can take advantage of the mixed frequency time series panel data structures and improve the quality of the estimates. We obtain oracle inequalities for the pooled and fixed effects sparse-group LASSO panel data estimators recognizing that financial and economic data can have fat tails. To that end, we leverage on a new Fuk–Nagaev concentration inequality for panel data consisting of heavy-tailed τ-mixing processes.",http://www.sciencedirect.com/science/article/pii/S0304407622001282
Journal of Econometrics,2023,Transformed regression-based long-horizon predictability tests,"Matei Demetrescu, Paulo Rodrigues and Robert Taylor","We propose new tests for long-horizon predictability based on IVX estimation of a transformed regression which explicitly accounts for the over-lapping nature of the dependent variable in the long-horizon regression arising from temporal aggregation. To improve efficiency, we moreover incorporate the residual augmentation approach recently used in the context of short-horizon predictability testing by Demetrescu and Rodrigues (2022). Our proposed tests improve on extant tests in the literature in a number of ways. First, they allow practitioners to remain ambivalent over the strength of the persistence of the predictors. Second, they are valid under much weaker conditions on the innovations than extant long-horizon predictability tests; in particular, we allow for general forms of conditional and unconditional heteroskedasticity in the innovations, neither of which are tied to a parametric model. Third, unlike the popular Bonferroni-based methods in the literature, our proposed tests can handle multiple predictors, and can be easily implemented as either one or two-sided hypotheses tests. Monte Carlo analysis suggests that our preferred tests offer improved finite sample properties compared to the leading tests in the literature. We report results from an empirical application investigating the use of real exchange rates for predicting nominal exchange rates and inflation.",http://www.sciencedirect.com/science/article/pii/S0304407622001294
Journal of Econometrics,2023,On the aggregation of probability assessments: Regularized mixtures of predictive densities for Eurozone inflation and real interest rates,"Francis Diebold, Minchul Shin and Boyuan Zhang","We propose methods for constructing regularized mixtures of density forecasts. We explore a variety of objectives and regularization penalties, and we use them in a substantive exploration of Eurozone inflation and real interest rate density forecasts. All individual inflation forecasters (even the ex post best forecaster) are outperformed by our regularized mixtures. From the Great Recession onward, the optimal regularization tends to move density forecasts’ probability mass from the centers to the tails, correcting for overconfidence.",http://www.sciencedirect.com/science/article/pii/S0304407622001464
Journal of Econometrics,2023,Semiparametric modeling of multiple quantiles,Leopoldo Catania and Alessandra Luati,"We develop a semiparametric model to track a large number of quantiles of a time series. The model satisfies the condition of non-crossing quantiles and the defining property of fixed quantiles. A key feature of the specification is that the updating scheme for time-varying quantiles at each probability level is based on the gradient of the check loss function. Theoretical properties of the proposed model are derived such as weak stationarity of the quantile process and consistency of the estimators of the fixed parameters. The model can be applied for filtering and prediction. We also illustrate a number of possible applications such as: (i) semiparametric estimation of dynamic moments of the observables, (ii) density prediction, and (iii) quantile predictions.",http://www.sciencedirect.com/science/article/pii/S0304407622002044
Journal of Econometrics,2023,A flexible predictive density combination for large financial data sets in regular and crisis periods,"Roberto Casarin, Stefano Grassi, Francesco Ravazzolo and Herman van Dijk","A flexible predictive density combination is introduced for large financial data sets which allows for model set incompleteness. Dimension reduction procedures that include learning allocate the large sets of predictive densities and combination weights to relatively small subsets. Given the representation of the probability model in extended nonlinear state-space form, efficient simulation-based Bayesian inference is proposed using parallel dynamic clustering as well as nonlinear filtering, implemented on graphics processing units. The approach is applied to combine predictive densities based on a large number of individual US stock returns of daily observations over a period that includes the Covid-19 crisis period. Evidence on dynamic cluster composition, weight patterns and model set incompleteness gives valuable signals for improved modelling. This enables higher predictive accuracy and better assessment of uncertainty and risk for investment fund management.",http://www.sciencedirect.com/science/article/pii/S0304407622002093
Journal of Econometrics,2023,Predictive quantile regression with mixed roots and increasing dimensions: The ALQR approach,"Rui Fan, Ji Hyung Lee and Youngki Shin","In this paper we propose the adaptive lasso for predictive quantile regression (ALQR). Reflecting empirical findings, we allow predictors to have various degrees of persistence and exhibit different signal strengths. The number of predictors is allowed to grow with the sample size. We study regularity conditions under which stationary, local unit root, and cointegrated predictors are present simultaneously. We next show the convergence rates, model selection consistency, and asymptotic distributions of ALQR. We apply the proposed method to the out-of-sample quantile prediction problem of stock returns and find that it outperforms the existing alternatives. We also provide numerical evidence from additional Monte Carlo experiments, supporting the theoretical results.",http://www.sciencedirect.com/science/article/pii/S0304407622002111
Journal of Econometrics,2023,Uniform predictive inference for factor models with instrumental and idiosyncratic betas,"Mingmian Cheng, Yuan Liao and Xiye Yang","This paper investigates the impact of allowing for characteristic-based time-varying factor betas on the diffusion-index type forecasts. The factor beta consists of two distinct components: the “instrumental beta” is a function of some observable characteristics, while the “idiosyncratic beta” captures more volatile residual movements. To estimate these characteristic-based time-varying betas and the corresponding factors, we apply the projected principal component analysis (P-PCA) method on high-frequency returns data. The primary advantage of this method is that it refines the estimators of latent factors, which shall be used in the forecasting models. We show that various leading components of the conditional mean forecast error are all asymptotically normal and pairwise independent. Extensive simulation studies show the good finite-sample properties of the P-PCA estimators and demonstrate the advantage of the P-PCA method relative to the classic PCA method in forecasting. In our empirical experiments of volatility prediction, we find that the factor-augmented model associated with the P-PCA method is more parsimonious and achieves better performance for a wide variety of target assets. We also find evidence on different levels of variation over time in the idiosyncratic beta, which necessitates our uniform predictive inference procedure.",http://www.sciencedirect.com/science/article/pii/S0304407622002123
Journal of Econometrics,2023,Dynamic factor copula models with estimated cluster assignments,Dong Hwan Oh and Andrew J. Patton,"This paper proposes a dynamic multi-factor copula for use in high-dimensional time series applications. A novel feature of our model is that the assignment of individual variables to groups is estimated from the data, rather than being pre-assigned using SIC industry codes, market capitalization ranks, or other ad hoc methods. We adapt the k-means clustering algorithm for use in our application and show that it has excellent finite-sample properties. Applying the new model to returns on 110 US equities, we find around 20 clusters to be optimal. In out-of-sample forecasts, we find that a model with as few as five estimated clusters significantly outperforms an otherwise identical model with 21 clusters formed using two-digit SIC codes.",http://www.sciencedirect.com/science/article/pii/S0304407622002135
Journal of Econometrics,2023,A penalized two-pass regression to predict stock returns with time-varying risk premia,"Gaetan Bakalli, Stéphane Guerrier and Olivier Scaillet","We develop a penalized two-pass regression with time-varying factor loadings. The penalization in the first pass enforces sparsity for the time-variation drivers while also maintaining compatibility with the no-arbitrage restrictions by regularizing appropriate groups of coefficients. The second pass delivers risk premia estimates to predict equity excess returns. Our Monte Carlo results and our empirical results on a large cross-sectional data set of US individual stocks show that penalization without grouping can yield to nearly all estimated time-varying models violating the no-arbitrage restrictions. Moreover, our results demonstrate that the proposed method reduces the prediction errors compared to a penalized approach without appropriate grouping or a time-invariant factor model.",http://www.sciencedirect.com/science/article/pii/S0304407622002147
Journal of Econometrics,2023,Taking stock of long-horizon predictability tests: Are factor returns predictable?,"Alexandros Kostakis, Tassos Magdalinos and Michalis P. Stamatogiannis","This study provides a critical assessment of long-horizon return predictability tests using highly persistent regressors. We show that the commonly used statistics are typically oversized, leading to spurious inference. Instead, we propose a Wald statistic, which accommodates multiple predictors of (unknown) arbitrary persistence degree within the I(0)-I(1) range. The test statistic, based on an adaptation of the IVX procedure to a long-horizon regression framework, is shown to have a standard chi-squared asymptotic distribution (regardless of the stochastic properties of the regressors used as predictors) and to exhibit excellent finite-sample size and power properties. Employing this test statistic, we find evidence of predictability for “old” and “new” pricing factors with monthly returns, but this becomes weaker as the predictive horizon increases. The predictability evidence substantially weakens with annual data. Overall, we question the incremental value of using long-horizon predictive regressions.",http://www.sciencedirect.com/science/article/pii/S0304407623000052
Journal of Econometrics,2023,Time-varying forecast combination for high-dimensional data,Bin Chen and Kenwin Maung,"In this paper, we propose a new nonparametric estimator of time-varying forecast combination weights. When the number of individual forecasts is small, we study the asymptotic properties of the local linear estimator. When the number of candidate forecasts exceeds or diverges with the sample size, we consider penalized local linear estimation with the group SCAD penalty. We show that the estimator exhibits the oracle property and correctly selects relevant forecasts with probability approaching one. Simulations indicate that the proposed estimators outperform existing combination schemes when structural changes exist. An empirical application on inflation and unemployment forecasting highlights the merits of our approach relative to other popular methods in the literature.",http://www.sciencedirect.com/science/article/pii/S0304407623000556
Journal of Econometrics,2023,Are bond returns predictable with real-time macro data?,"Dashan Huang, Fuwei Jiang, Kunpeng Li, Guoshi Tong and Guofu Zhou","We investigate the predictability of bond returns using real-time macro variables and consider the possibility of a nonlinear predictive relationship and the presence of weak factors. To address these issues, we propose a scaled sufficient forecasting (sSUFF) method and analyze its asymptotic properties. Using both the existing and the new method, we find empirically that real-time macro variables have significant forecasting power both in-sample and out-of-sample. Moreover, they generate sizable economic values, and their predictability is not spanned by the yield curve. We also observe that the forecasted bond returns are countercyclical, and the magnitude of predictability is stronger during economic recessions, which lends empirical support to well-known macro finance theories.",http://www.sciencedirect.com/science/article/pii/S0304407623001161
Journal of Econometrics,2023,Time-Varying Parameters in Econometrics: The editor’s foreword,"F. Blasques, Andrew Harvey, Siem Jan Koopman and Andre Lucas","The Themed Issue Time-Varying Parameters in Econometrics consists of eight papers where theoretical, methodological and empirical developments are reported. Particular attention is given to observation-driven time-varying parameter models of which score-driven models is a sub-class. We will introduce the Themed Issue and review its contributions.",http://www.sciencedirect.com/science/article/pii/S0304407623001173
Journal of Econometrics,2023,Business-cycle consumption risk and asset prices,Federico M. Bandi and Andrea Tamoni,"Aggregation is routinely employed in asset pricing to capture frequency-specific effects. We formalize the theoretical mapping between aggregates of time series and their frequency-specific components as well as the mapping between factor loadings obtained upon aggregation of returns and factors and frequency-specific factor loadings. We show that business-cycle consumption, a component of the consumption growth process with cycles between 4 and 8 years, provides valuable pricing signal. In agreement with the implications of theory, we document that consumption growth aggregated over a 4-year horizon (4-year consumption) has analogous pricing ability, cross-sectionally and in the time series, to business-cycle consumption.",http://www.sciencedirect.com/science/article/pii/S0304407623001410
Journal of Econometrics,2023,Score-driven models for realized volatility,Andrew Harvey and Dario Palumbo,"This paper sets up a statistical framework for modeling realized volatility (RV) using a Dynamic Conditional Score (DCS) model. It first shows how, for a dataset on stock indices, a preliminary analysis of RV, based on fitting a linear Gaussian model to its logarithm, suggests the use of a two component dynamic specification. It also indicates a departure from normality, a weekly pattern in the data and the presence of heteroscedasticity. Fitting the two component DCS specification with leverage and a day of the week effect is then carried out directly on RV with a Generalized Beta of the Second Kind (GB2) conditional distribution or, equivalently, on the logarithm of RV with an Exponential Generalized Beta of the Second Kind (EGB2) distribution. The forecasting performance of this model, with and without heteroscedasticity, is compared with that of the Heterogeneous Autoregression (HAR), some extensions of it and some other models. Overall there is a clear gain from using the GB2-DCS model, even when the HAR model uses additional information, such as realized semi-variance. When the aim is to forecast tail behavior, the fat-tailed GB2 model performs much better than models with thin-tailed distributions. A further exercise uses a dataset on exchange rates to compare GB2-DCS models with models that use realized quarticity. Again the additional information offers no forecasting advantage. Overall the GB2-DCS models are transparent, provide a comprehensive description of the properties of RV, and are difficult to beat for forecasting.",http://www.sciencedirect.com/science/article/pii/S0304407623001422
Journal of Econometrics,2023,Score-driven asset pricing: Predicting time-varying risk premia based on cross-sectional model performance,Dennis Umlandt,This paper proposes a new parametric approach for estimating linear factor pricing models with dynamic risk premia. Time-varying risk prices and exposures follow an observation-driven updating scheme that reduces the one-step-ahead prediction error from a cross-sectional factor model at the current observation. This agnostic approach is particularly useful in situations where predictors are unknown or of uncertain quality. Updating schemes for elliptically distributed returns are derived and propose cross-sectional regression errors as driving sequence for the parameter dynamics. Estimation and inference are performed by likelihood maximization. A simulation study confirms that the novel method is capable of filtering and predicting substantial risk price movements. The empirical performance of the method is illustrated by an application to a panel of equity portfolios.,http://www.sciencedirect.com/science/article/pii/S0304407623001641
Journal of Econometrics,2023,"Better bunching, nicer notching","Marinho Bertanha, Andrew H. McCallum and Nathan Seegert","This paper studies the bunching identification strategy for an elasticity parameter that summarizes agents’ responses to changes in slope (kink) or intercept (notch) of a schedule of incentives. We show that current bunching methods may be very sensitive to implicit assumptions in the literature about unobserved individual heterogeneity. We overcome this sensitivity concern with new non- and semi-parametric estimators. Our estimators allow researchers to show how bunching elasticities depend on different identifying assumptions and when elasticities are robust to them. We follow the literature and derive our methods in the context of the iso-elastic utility model and an income tax schedule that creates a piece-wise linear budget constraint. We demonstrate bunching behavior provides robust estimates for self-employed and not-married taxpayers in the context of the U.S. Earned Income Tax Credit. In contrast, estimates for self-employed and married taxpayers depend on specific identifying assumptions, which highlight the value of our approach. We provide the Stata package bunching to implement our procedures.",http://www.sciencedirect.com/science/article/pii/S0304407623002282
Journal of Econometrics,2023,What is a standard error? (And how should we compute it?),Jeffrey Wooldridge,"I review the definition of a standard error from a frequentist perspective, including both exact analysis and asymptotic analysis. Using the linear model for illustration, I discuss the model-based, design-based, and sampling-based approaches to uncertainty in obtaining standard errors. The model-based approach is widely applicable and produces reasonable measures of estimator precision in many settings. In some situations, particularly in the context of clustering, the model-based approach can suffer from ambiguity, and can lead to standard errors that are systematically biased. A combination of the design-based and sampling-based approaches requires the researcher to think about the variation in key explanatory variables when computing standard errors, and it can even apply to cases where the entire population is observed.",http://www.sciencedirect.com/science/article/pii/S0304407623002336
Journal of Econometrics,2023,Instrument validity for heterogeneous causal effects,Zhenting Sun,"This paper provides a general framework for testing instrument validity in heterogeneous causal effect models. The generalization includes the cases where the treatment can be multivalued ordered or unordered. Based on a series of testable implications, we propose a nonparametric test which is proved to be asymptotically size controlled and consistent. Compared to the tests in the literature, our test can be applied in more general settings and may achieve power improvement. Refutation of instrument validity by the test helps detect invalid instruments that may yield implausible results on causal effects. Evidence that the test performs well on finite samples is provided via simulations. We revisit the empirical study on return to schooling to demonstrate application of the proposed test in practice. An extended continuous mapping theorem and an extended delta method, which may be of independent interest, are provided to establish the asymptotic distribution of the test statistic under null.",http://www.sciencedirect.com/science/article/pii/S0304407623002397
Journal of Econometrics,2023,Some impossibility results for inference with cluster dependence with large clusters,Denis Kojevnikov and Kyungchul Song,"This paper focuses on a setting with observations having a cluster dependence structure and presents two main impossibility results. First, we show that when there is only one large cluster, i.e., the researcher does not have any knowledge on the dependence structure of the observations, it is not possible to consistently discriminate the mean. When within-cluster observations satisfy the uniform central limit theorem, we also show that a sufficient condition for consistent n-discrimination of the mean is that we have at least two large clusters. This result shows some limitations for inference when we lack information on the dependence structure of observations. Our second result provides a necessary and sufficient condition for the cluster structure that the long run variance is consistently estimable. Our result implies that when there is at least one large cluster, the long run variance is not consistently estimable.",http://www.sciencedirect.com/science/article/pii/S0304407623002403
Journal of Econometrics,2023,Semiparametric estimation of long-term treatment effects,Jiafeng Chen and David M. Ritzwoller,"Long-term outcomes of experimental evaluations are necessarily observed after long delays. We develop semiparametric methods for combining the short-term outcomes of experiments with observational measurements of short-term and long-term outcomes, in order to estimate long-term treatment effects. We characterize semiparametric efficiency bounds for various instances of this problem. These calculations facilitate the construction of several estimators. We analyze the finite-sample performance of these estimators with a simulation calibrated to data from an evaluation of the long-term effects of a poverty alleviation program.",http://www.sciencedirect.com/science/article/pii/S0304407623002610
Journal of Econometrics,2023,Identification and estimation of spillover effects in randomized experiments,Gonzalo Vazquez-Bare,"I study identification, estimation and inference for spillover effects in experiments where units’ outcomes may depend on the treatment assignments of other units within a group. I show that the commonly-used reduced-form linear-in-means regression identifies a weighted sum of spillover effects with some negative weights, and that the difference in means between treated and controls identifies a combination of direct and spillover effects entering with different signs. I propose nonparametric estimators for average direct and spillover effects that overcome these issues and are consistent and asymptotically normal under a precise relationship between the number of parameters of interest, the total sample size and the treatment assignment mechanism. These findings are illustrated using data from a conditional cash transfer program and with simulations. The empirical results reveal the potential pitfalls of failing to flexibly account for spillover effects in policy evaluation: the estimated difference in means and the reduced-form linear-in-means coefficients are all close to zero and statistically insignificant, whereas the nonparametric estimators I propose reveal large, nonlinear and significant spillover effects.",http://www.sciencedirect.com/science/article/pii/S0304407621003067
Journal of Econometrics,2023,Identification of mixtures of dynamic discrete choices,Ayden Higgins and Koen Jochmans,"This paper provides new identification results for finite mixtures of Markov processes. Our arguments yield identification from knowledge of the cross-sectional distribution of three (or more) effective time-series observations under simple conditions. We explain how our approach and results are different from those in previous work by Kasahara and Shimotsu (2009) and Hu and Shum (2012). Most notably, outside information, such as monotonicity restrictions that link conditional distributions to latent types, is not needed.",http://www.sciencedirect.com/science/article/pii/S0304407623001562
Journal of Econometrics,2023,Under-identification of structural models based on timing and information set assumptions,"Daniel Ackerberg, Garth Frazer, Kyoo il Kim, Yao Luo and Yingjun Su","We revisit identification based on timing and information set assumptions in structural models, which have been used in the context of production functions, demand equations, and hedonic pricing models (e.g. Olley and Pakes, 1996, Blundell and Bond, 2000). First, we demonstrate a general under-identification problem using these assumptions in a simple version of the Blundell–Bond dynamic panel model. In particular, the basic moment conditions can yield multiple discrete solutions: one at the persistence parameter in the main equation and another at the persistence parameter governing the regressor. We then propose a possible solution in the simple setting by enforcing an assumed sign restriction and discuss more general practical advice for empirical researchers using these methods.",http://www.sciencedirect.com/science/article/pii/S0304407623001574
Journal of Econometrics,2023,Inference under covariate-adaptive randomization with imperfect compliance,Federico A. Bugni and Mengsi Gao,"This paper studies inference in a randomized controlled trial (RCT) with covariate-adaptive randomization (CAR) and imperfect compliance of a binary treatment. In this context, we study inference on the local average treatment effect (LATE), i.e., the average treatment effect conditional on individuals that always comply with the assigned treatment. As in Bugni et al. (2018, 2019), CAR refers to randomization schemes that first stratify according to baseline covariates and then assign treatment status so as to achieve “balance” within each stratum. In contrast to these papers, however, we allow participants of the RCT to endogenously decide to comply or not with the assigned treatment status.",http://www.sciencedirect.com/science/article/pii/S0304407623002130
Journal of Econometrics,2023,Linear panel regressions with two-way unobserved heterogeneity,Hugo Freeman and Martin Weidner,"We study linear panel regression models in which the unobserved error term is an unknown smooth function of two-way unobserved fixed effects. In standard additive or interactive fixed effect models the individual specific and time specific effects are assumed to enter with a known functional form (additive or multiplicative). In this paper, we allow for this functional form to be more general and unknown. We discuss two different estimation approaches that allow consistent estimation of the regression parameters in this setting as the number of individuals and the number of time periods grow to infinity. The first approach uses the interactive fixed effect estimator in Bai (2009), which is still applicable here, as long as the number of factors in the estimation grows asymptotically. The second approach first discretizes the two-way unobserved heterogeneity (similar to what Bonhomme et al., 2021 are doing for one-way heterogeneity) and then estimates a simple linear fixed effect model with additive two-way grouped fixed effects. For both estimation methods we obtain asymptotic convergence results, perform Monte Carlo simulations, and employ the estimators in an empirical application to UK house price data.",http://www.sciencedirect.com/science/article/pii/S0304407623002142
Journal of Econometrics,2023,Stable outcomes and information in games: An empirical framework,Paul S. Koh,"Empirically, many strategic settings are characterized by stable outcomes in which players’ decisions are publicly observed, yet no player takes the opportunity to deviate. To analyze such situations in the presence of incomplete information, we build an empirical framework by introducing a novel solution concept that we call Bayes stable equilibrium and computationally tractable approaches for estimation and inference. Our framework allows the researcher to be agnostic about players’ information and the equilibrium selection rule. In an application, we study the strategic entry decisions of McDonald’s and Burger King in the US. While the Bayes stable equilibrium identified set is always (weakly) tighter than the Bayes correlated equilibrium identified set, our results show that the former can be substantially tighter in practice. In a counterfactual experiment, we examine the impact of increasing access to healthy food on the market structures in Mississippi food deserts.",http://www.sciencedirect.com/science/article/pii/S0304407623002154
Journal of Econometrics,2023,Econometric inference on a large Bayesian game with heterogeneous beliefs,Denis Kojevnikov and Kyungchul Song,"Econometric models of strategic interactions among people or firms have received a great deal of attention in the literature. Less attention has been paid to the role of the underlying assumptions about the way agents form beliefs about other agents. We focus on a single large Bayesian game with idiosyncratic strategic neighborhoods and develop an approach of empirical modeling that relaxes the assumption of rational expectations and allows the players to form beliefs differently. By drawing on the main intuition of Kalai (2004), we introduce the notion of hindsight regret, which measures each player’s ex-post value of other players’ type information, and obtain the belief-free bound for the hindsight regret. Using this bound, we derive testable implications and develop a bootstrap inference procedure for the structural parameters. Our inference method is uniformly valid regardless of the size of strategic neighborhoods and tends to exhibit high power when the neighborhoods are large. We demonstrate the finite sample performance of the method through Monte Carlo simulations.",http://www.sciencedirect.com/science/article/pii/S030440762300218X
Journal of Econometrics,2023,Penetrating sporadic return predictability,Yundong Tu and Xinling Xie,"Return predictability has been one of the central research questions in finance for many decades. This paper proposes a predictive regression with multiple structural changes to capture the sporadic predictive ability of potential predictors for the return series. An adaptive group Lasso procedure, augmented with a forward regression for break screening, is adopted to efficiently and consistently identify the structural breaks in the predictive regression, with predictors exhibiting low signal strength and heterogeneous degrees of persistence. To enhance the prediction accuracy, adaptive Lasso is further used to eliminate the irrelevant predictors and is shown to achieve the oracle property. Simulation studies demonstrate the effectiveness of the proposed methods in break detection and predictor selection, and further show that ignoring structural breaks could abate predictability. The application to predicting U.S. equity premium illustrates the practical merits of our methodology in revealing return predictability that changes over time.",http://www.sciencedirect.com/science/article/pii/S0304407623002257
Journal of Econometrics,2023,A new generalized exponentially weighted moving average quantile model and its statistical inference,Ke Zhu,"The exponentially weighting scheme is a simple and pragmatic approach to compute the value at risk (VaR). However, the existing exponentially weighting methods lack a sound statistical inference procedure. To circumvent this deficiency, this paper proposes a new generalized exponentially weighted moving average (GEWMA) quantile model, which allows a much broader weighting scheme than the benchmark one used in “Risk Metrics” document. For the GEWMA quantile model, a systematic statistical inference procedure is provided, including the weighted estimators for the weighting parameters, a t-test for the stability of the conditional quantile, another t-test for the mean invariance of the conditional quantile, a unit root test for the absence of intercept term, and several dynamic quantile tests for the model checking. Under mild conditions, the asymptotics of all proposed estimators and tests are established. Simulations show that all proposed estimators and tests have good finite-sample performances. Applications to four major exchange rates demonstrate that the weighting scheme suggested by “Risk Metrics” document is inappropriate, and the GEWMA quantile model delivers better VaR predictions than its many competitive methods. As an extension, the asymmetric GEWMA quantile model is also studied.",http://www.sciencedirect.com/science/article/pii/S0304407623002269
Journal of Econometrics,2023,Estimation and inference in a high-dimensional semiparametric Gaussian copula vector autoregressive model,"Yanqin Fan, Fang Han and Hyeonseok Park","This paper develops simple, robust estimation and inference methods for the transition matrix of a high-dimensional semiparametric Gaussian copula vector autoregressive process. Our estimator is based on rank estimators of the large variance and auto-covariance matrices of a transformed latent high-dimensional Gaussian process. We derive rates of convergence of our estimator based on which we develop de-biased inference for Granger causality. Numerical results demonstrate the efficacy of the proposed methods. Although our focus is on the observable process, by the nature of rank estimators, all the methods developed directly apply to the transformed latent process. In technical terms, our analysis relies heavily on newly developed exponential inequalities for (degenerate) U-statistics under α-mixing condition.",http://www.sciencedirect.com/science/article/pii/S0304407623002294
Journal of Econometrics,2023,Adaptive robust large volatility matrix estimation based on high-frequency financial data,"Minseok Shin, Donggyu Kim and Jianqing Fan","Several novel statistical methods have been developed to estimate large integrated volatility matrices based on high-frequency financial data. To investigate their asymptotic behaviors, they require a sub-Gaussian or finite high-order moment assumption for observed log-returns, which cannot account for the heavy-tail phenomenon of stock-returns. Recently, a robust estimator was developed to handle heavy-tailed distributions with some bounded fourth-moment assumption. However, we often observe that log-returns have heavier tail distribution than the finite fourth-moment and that the degrees of heaviness of tails are heterogeneous across asset and over time. In this paper, to deal with the heterogeneous heavy-tailed distributions, we develop an adaptive robust integrated volatility estimator that employs pre-averaging and truncation schemes based on jump-diffusion processes. We call this an adaptive robust pre-averaging realized volatility (ARP) estimator. We show that the ARP estimator has a sub-Weibull tail concentration with only finite 2α-th moments for any α>1. In addition, we establish matching upper and lower bounds to show that the ARP estimation procedure is optimal. To estimate large integrated volatility matrices using the approximate factor model, the ARP estimator is further regularized using the principal orthogonal complement thresholding (POET) method. The numerical study is conducted to check the finite sample performance of the ARP estimator.",http://www.sciencedirect.com/science/article/pii/S0304407623002300
Journal of Econometrics,2023,Identification of dynamic binary response models,"S. Khan, Maria Ponomareva and E. Tamer","We consider identification of parameters in dynamic binary response models with panel data under minimal assumptions. This model is prominent in empirical economics as it has been used to infer state dependence in the presence of unobserved heterogeneity. The main results in our paper are characterizations of the identified set under weak assumptions. The results generalize the existing literature in several directions: (1) we do not require any restrictions on the support of the observables; for example, we allow for time trends, time dummies, and/or only discrete covariates; (2) we only maintain that the idiosyncratic error terms are stationary over time conditional on the fixed effect and the covariates (without conditioning on initial conditions) and without imposing a parametric distribution on the distribution of these error terms; (3) we show that it is possible to get point identification in some cases even with T=2 (two time periods). We also construct examples of identified sets in some designs to illustrate the informational content of different assumptions.",http://www.sciencedirect.com/science/article/pii/S0304407623002312
Journal of Econometrics,2023,What is a standard error?,Andrew Gelman,,http://www.sciencedirect.com/science/article/pii/S0304407623002324
Journal of Econometrics,2023,What is uncertainty in today’s practice of data science?,Bin Yu,,http://www.sciencedirect.com/science/article/pii/S030440762300235X
Journal of Econometrics,2023,A structural analysis of simple contracts,"Yonghong An, Shengjie Hong and Daiqiang Zhang","This paper provides an econometric framework for analyzing two-period simple contracts where an agent chooses between a fixed-price option and a cost-reimbursement option provided by a principal in each contracting period. First, we propose a consistent procedure for testing the null hypothesis of a corresponding cost function being linear, which is widely assumed for tractability in the literature. Motivated by the rejection of such a null based on our empirical data, next we establish nonparametric identification, without restricting the cost function to be linear, for all model primitives conditioned on the agent exerting nonzero effort. These primitives include agent’s cost and disutility functions, distribution of agent efficiency type, and parameters that characterize agent’s bargaining power and intertemporal preference. Moreover, we propose a consistent procedure to implement the identification results for estimation. In our empirical study, we find strong evidence against linearity of the cost function. The importance of this empirical finding is further evidenced by a welfare analysis, which shows the welfare assessment to be sensitive to the specification of cost function.",http://www.sciencedirect.com/science/article/pii/S0304407623001501
Journal of Econometrics,2023,"Moments, shocks and spillovers in Markov-switching VAR models",Erik Kole and Dick van Dijk,"To investigate how economies, financial markets or institutions can deal with stress, we often analyze the effects of shocks conditional on being in a recession or a bear market. MSVAR models are perfectly suited for such analyses because they combine gradual movements with sudden regime switches. In this paper, we develop a comprehensive methodology to conduct these analyses. We derive first and second moments conditional only on the regime distribution and propose impulse response functions for both moments. By formulating the MSVAR as an extended linear non-Gaussian VAR, all results are available in closed-form. We illustrate our methods with an application to stock and bond return predictability. We show how forecasts of means, volatilities and (auto-)correlations depend on the regimes. The effect of shocks becomes highly nonlinear, and they propagate via different channels. During bear markets, shocks have stronger effects on means and volatilities and die out more slowly.",http://www.sciencedirect.com/science/article/pii/S0304407623001902
Journal of Econometrics,2023,Inference and forecasting for continuous-time integer-valued trawl processes,"Mikkel Bennedsen, Asger Lunde, Neil Shephard and Almut Veraart","This paper develops likelihood-based methods for estimation, inference, model selection, and forecasting of continuous-time integer-valued trawl processes. The full likelihood of integer-valued trawl processes is, in general, highly intractable, motivating the use of composite likelihood methods, where we consider the pairwise likelihood in lieu of the full likelihood. Maximizing the pairwise likelihood of the data yields an estimator of the parameter vector of the model, and we prove consistency and, in the short memory case, asymptotic normality of this estimator. When the underlying trawl process has long memory, the asymptotic behaviour of the estimator is more involved; we present some partial results for this case. The pairwise approach further allows us to develop probabilistic forecasting methods, which can be used to construct the predictive distribution of integer-valued time series. In a simulation study, we document the good finite sample performance of the likelihood-based estimator and the associated model selection procedure. Lastly, the methods are illustrated in an application to modelling and forecasting financial bid–ask spread data, where we find that it is beneficial to carefully model both the marginal distribution and the autocorrelation structure of the data.",http://www.sciencedirect.com/science/article/pii/S0304407623001926
Journal of Econometrics,2023,A solution to the global identification problem in DSGE models,Andrzej Kocięcki and Marcin Kolasa,"We develop an analytical framework to study global identification in structural models with forward-looking expectations. Our identification condition combines the similarity transformation linking the observationally equivalent state space systems with the constraints imposed on them by the model parameters. The key step of solving the identification problem then reduces to finding all roots of a system of polynomial equations. We show how it can be done using the concept of a Gröbner basis and recently developed algorithms to compute it analytically. In contrast to frameworks relying on numerical search, our approach can prove whether a model is identified or not at a given parameter point, explicitly delivering the complete set of observationally equivalent parameter vectors. We present the solution to the global identification problem for several popular dynamic stochastic general equilibrium (DSGE) models.",http://www.sciencedirect.com/science/article/pii/S0304407623001938
Journal of Econometrics,2023,Generalized linear models with structured sparsity estimators,Mehmet Caner,"In this paper, we introduce structured sparsity estimators for use in Generalized Linear Models. Structured sparsity estimators in the least squares loss are introduced by Stucky and van de Geer (2018). Their proofs exclusively depend on their use of fixed design and normal errors. We extend their results to debiased structured sparsity estimators with Generalized Linear Model based loss through incorporating random design and non-sub Gaussian data. Structured sparsity estimation means that penalized loss functions with a possible sparsity structure in a norm. These norms include norms generated from convex cones.",http://www.sciencedirect.com/science/article/pii/S030440762300194X
Journal of Econometrics,2023,"When will Arctic sea ice disappear? Projections of area, extent, thickness, and volume","Francis Diebold, Glenn Rudebusch, Maximilian Göbel, Philippe Goulet Coulombe and Boyuan Zhang","Rapidly diminishing Arctic summer sea ice is a strong signal of the pace of global climate change. We provide point, interval, and density forecasts for four measures of Arctic sea ice: area, extent, thickness, and volume. Importantly, we enforce the joint constraint that these measures must simultaneously arrive at an ice-free Arctic. We apply this constrained joint forecast procedure to models relating sea ice to atmospheric carbon dioxide concentration and models relating sea ice directly to time. The resulting “carbon-trend” and “time-trend” projections are mutually consistent and predict a nearly ice-free summer Arctic Ocean by the mid-2030s with an 80% probability. Moreover, the carbon-trend projections show that global adoption of a lower carbon path would likely delay the arrival of a seasonally ice-free Arctic by only a few years.",http://www.sciencedirect.com/science/article/pii/S0304407623001951
Journal of Econometrics,2023,Two-way fixed effects and differences-in-differences estimators with several treatments,"Clément de Chaisemartin and D’Haultfœuille, Xavier","We study two-way-fixed-effects regressions (TWFE) with several treatment variables. Under a parallel trends assumption, we show that the coefficient on each treatment identifies a weighted sum of that treatment’s effect, with possibly negative weights, plus a weighted sum of the effects of the other treatments. Thus, those estimators are not robust to heterogeneous effects and may be contaminated by other treatments’ effects. We further show that omitting a treatment from the regression can actually reduce the estimator’s bias, unlike what would happen under constant treatment effects. We propose an alternative difference-in-differences estimator, robust to heterogeneous effects and immune to the contamination problem. In the application we consider, the TWFE regression identifies a highly non-convex combination of effects, with large contamination weights, and one of its coefficients significantly differs from our heterogeneity-robust estimator.",http://www.sciencedirect.com/science/article/pii/S0304407623001963
Journal of Econometrics,2023,Out-of-sample tests for conditional quantile coverage an application to Growth-at-Risk,"Valentina Corradi, Jack Fosten and Daniel Gutknecht","This paper proposes tests for out-of-sample comparisons of interval forecasts based on parametric conditional quantile models. The tests rank the distance between actual and nominal conditional coverage with respect to the set of conditioning variables from all models, for a given loss function. We propose a pairwise test to compare two models for a single predictive interval. The set-up is then extended to a comparison across multiple models and/or intervals. The limiting distribution varies depending on whether models are strictly non-nested or overlapping. In the latter case, degeneracy may occur. We establish the asymptotic validity of wild bootstrap based critical values across all cases. An empirical application to Growth-at-Risk (GaR) uncovers situations in which a richer set of financial indicators are found to outperform a commonly-used benchmark model when predicting downside risk to economic activity.",http://www.sciencedirect.com/science/article/pii/S0304407623002063
Journal of Econometrics,2023,Bayesian Artificial Neural Networks for frontier efficiency analysis,"Mike Tsionas, Christopher F. Parmeter and Valentin Zelenyuk","Artificial neural networks have offered their share of econometric insights, given their power to model complex relationships. One area where they have not been readily deployed is the estimation of frontiers. The literature on frontier estimation has seen its share of research comparing and contrasting data envelopment analysis (DEA) and stochastic frontier analysis (SFA), the two workhorse estimators. These studies rely on both Monte Carlo experiments and actual data sets to examine a range of performance issues which can be used to elucidate insights on the benefits or weaknesses of one method over the other. As can be imagined, neither method is universally better than the other. The present paper proposes an alternative approach that is quite flexible in terms of functional form and distributional assumptions and it amalgamates the benefits of both DEA and SFA. Specifically, we bridge these two popular approaches via Bayesian artificial neural networks while accounting for possible endogeneity of inputs. We examine the performance of this new machine learning approach using Monte Carlo experiments which is found to be very good, comparable to, or often better than, the current standards in the literature. To illustrate the new techniques, we provide an application of this approach to a data set of large US banks.",http://www.sciencedirect.com/science/article/pii/S0304407623002075
Journal of Econometrics,2023,Treatment effect models with strategic interaction in treatment decisions,Tadao Hoshino and Takahide Yanagi,"This study considers treatment effect models in which others’ treatment decisions can affect both one’s own treatment and outcome. Focusing on the case of two-player interactions, we formulate treatment decision behavior as a complete information game with multiple equilibria. Using a latent index framework and assuming a stochastic equilibrium selection, we prove that the marginal treatment effect from one’s own treatment and that from the partner are identifiable on the conditional supports of certain threshold variables determined through the game model. Based on our constructive identification results, we propose a two-step semiparametric procedure for estimating the marginal treatment effects using series approximation. We show that the proposed estimator is uniformly consistent and asymptotically normally distributed. As an empirical illustration, we investigate the impacts of risky behaviors on adolescents’ academic performance.",http://www.sciencedirect.com/science/article/pii/S0304407623002117
Journal of Econometrics,2023,Policy evaluation during a pandemic,Brantly Callaway and Tong Li,"National and local governments have implemented a large number of policies in response to the Covid-19 pandemic. Evaluating the effects of these policies, both on the number of Covid-19 cases as well as on other economic outcomes is a key ingredient for policymakers to be able to determine which policies are most effective as well as the relative costs and benefits of particular policies. In this paper, we consider the relative merits of common identification strategies that exploit variation in the timing of policies across different locations by checking whether the identification strategies are compatible with leading epidemic models in the epidemiology literature. We argue that unconfoundedness type approaches, that condition on the pre-treatment “state” of the pandemic, are likely to be more useful for evaluating policies than difference-in-differences type approaches due to the highly nonlinear spread of cases during a pandemic. For difference-in-differences, we further show that a version of this problem continues to exist even when one is interested in understanding the effect of a policy on other economic outcomes when those outcomes also depend on the number of Covid-19 cases. We propose alternative approaches that are able to circumvent these issues. We apply our proposed approach to study the effect of state level shelter-in-place orders early in the pandemic.",http://www.sciencedirect.com/science/article/pii/S0304407623001483
Journal of Econometrics,2023,Identification of auction models using order statistics,Yao Luo and Ruli Xiao,"Auction data often contain information on only the most competitive bids as opposed to all bids. The usual measurement error approaches to unobserved heterogeneity are inapplicable due to dependence among order statistics. We bridge this gap by providing a set of positive identification results. First, we show that symmetric auctions with discrete unobserved heterogeneity are identifiable using two consecutive order statistics and an instrument. Second, we extend the results to ascending auctions with unknown competition and unobserved heterogeneity.",http://www.sciencedirect.com/science/article/pii/S0304407623001513
Journal of Econometrics,2023,Dynamic discrete choice models with incomplete data: Sharp identification,"Yuya Sasaki, Yuya Takahashi, Yi Xin and Yingyao Hu","In many empirical studies, those states that are relevant for forward-looking economic agents to make decisions may not be included in the data to which researchers have access. This problem often arises in the context of declining/booming industries. In this paper, we develop the sharp identified sets of structural parameters and counterfactuals for dynamic discrete choice models when empirical data do not cover realizations of relevant future states. Applying the proposed method to the annual Toyo Keizai database, we study the behaviors of Japanese firms on foreign direct investments in China without observing the future states after Chinese economy slows down.",http://www.sciencedirect.com/science/article/pii/S0304407623001550
Journal of Econometrics,2023,"Identification-robust beta pricing, spanning, mimicking portfolios, and the benchmark neutrality of catastrophe bonds","Marie-Claude Beaulieu, Jean-Marie Dufour, Lynda Khalaf and Olena Melin","This paper proposes inference strategies for conditional and unconditional asset pricing models, which cover beta pricing, the zero-beta rate, risk prices, Jensen’s alphas, mean–variance spanning and intersection. We derive analytical conditions that link the identification of the zero-beta rate to spanning. Following finite-sample statistical considerations, the proposed procedures correct for measurement error with mimicking portfolios, and are invariant to fund repacking as well as and robust to: (i) missing factors, (ii) identification of risk price, and (iii) the quality of conditioning information. Empirically, we study benchmark neutrality of Catastrophe bond mutual funds. Results show that potential for diversification is significant before the 2008 financial crisis, yet lack support thereafter.",http://www.sciencedirect.com/science/article/pii/S0304407623001586
Journal of Econometrics,2023,Semiparametric estimation of latent variable asset pricing models,Jeroen Dalderop,"This paper studies semiparametric identification and estimation of consumption-based asset pricing models with latent state variables. First, we measure how consumption, dividends, and prices depend on Markovian state variables describing aggregate output growth. Subsequently, we identify state-dependent components in the stochastic discount factor using the Euler equation. We develop tractable algorithms for filtering, smoothing, and sieve maximum likelihood estimation, and establish its consistency. Empirically, we find sizable nonlinearities in the impact of expected growth and volatility on the price–dividend ratio and discount factor.",http://www.sciencedirect.com/science/article/pii/S0304407623001598
Journal of Econometrics,2023,Structural VAR models in the Frequency Domain,Alain Guay and Florian Pelgrin,"This paper proposes a joint methodology for the identification and inference of structural vector autoregressive models in the frequency domain. We show that identifying restrictions can be written naturally as an asymptotic least squares problem (Gouriéroux et al., 1985) in which there is a continuum of nonlinear estimating equations. Following Carrasco and Florens (2000), we then propose a continuum asymptotic least squares estimator (C-ALS) that efficiently exploits the continuum of estimating equations, thereby allowing to obtain optimal consistent estimates of impulse responses and reliable confidence intervals. Moreover, the identifying restrictions can be formally tested using an appropriate J-stat, and the frequency band can be selected using a data-driven procedure. Finally, we provide some Monte Carlo simulations and an application regarding the hours–productivity debate.",http://www.sciencedirect.com/science/article/pii/S0304407623001604
Journal of Econometrics,2023,We modeled long memory with just one lag!,"Luc Bauwens, Guillaume Chevillon and Sébastien Laurent","Two recent contributions have found conditions for large dimensional networks or systems to generate long memory in their individual components. We build on these and provide a multivariate methodology for modeling and forecasting series displaying long range dependence. We model long memory properties within a vector autoregressive system of order 1 and consider Bayesian estimation or ridge regression. For these, we derive a theory-driven parametric setting that informs a prior distribution or a shrinkage target. Our proposal significantly outperforms univariate time series long-memory models when forecasting a daily volatility measure for 250 U.S. company stocks over twelve years. This provides an empirical validation of the theoretical results showing long memory can be sourced to marginalization within a large dimensional system.",http://www.sciencedirect.com/science/article/pii/S0304407623001616
Journal of Econometrics,2023,High-dimensional conditionally Gaussian state space models with missing data,"Joshua Chan, Aubrey Poon and Dan Zhu","We develop an efficient sampling approach for handling complex missing data patterns and a large number of missing observations in conditionally Gaussian state space models. Two important examples are dynamic factor models with unbalanced datasets and large Bayesian VARs with variables in multiple frequencies. A key observation underlying the proposed approach is that the joint distribution of the missing data conditional on the observed data is Gaussian. Furthermore, the inverse covariance or precision matrix of this conditional distribution is sparse, and this special structure can be exploited to substantially speed up computations. We illustrate the methodology using two empirical applications. The first application combines quarterly, monthly and weekly data using a large Bayesian VAR to produce weekly GDP estimates. In the second application, we extract latent factors from unbalanced datasets involving over a hundred monthly variables via a dynamic factor model with stochastic volatility.",http://www.sciencedirect.com/science/article/pii/S0304407623001628
Journal of Econometrics,2023,Large stochastic volatility in mean VARs,"Jamie Cross, Chenghan Hou, Gary Koop and Aubrey Poon","Bayesian vector autoregressions with stochastic volatility in both the conditional mean and variance (SVMVARs) are widely used for studying the macroeconomic effects of uncertainty. Despite their popularity, intensive computational demands when estimating such models has constrained researchers to specifying a small number of latent volatilities, and made out-of-sample forecasting exercises impractical. In this paper, we propose an efficient Markov chain Monte Carlo (MCMC) algorithm that facilitates timely posterior and predictive inference with large SVMVARs. In a simulation exercise, we show that the new algorithm is significantly faster than the state-of-the-art particle Gibbs with ancestor sampling algorithm, and exhibits superior mixing properties. In two applications, we show that large SVMVARs are generally useful for structural analysis and out-of-sample forecasting, and are especially useful in periods of high uncertainty such as the Great Recession and the COVID-19 pandemic.",http://www.sciencedirect.com/science/article/pii/S030440762300163X
Journal of Econometrics,2023,Maximum likelihood estimation for α-stable double autoregressive models,"Dong Li, Yuxin Tao, Yaxing Yang and Rongmao Zhang","The paper investigates the maximum likelihood estimation (MLE) for a first-order double autoregressive model with standardized non-Gaussian symmetric α-stable innovation (sDAR) within a unified framework of stationary and explosive cases. It is shown that the MLE of all parameters, including the stable exponent in the innovation, are strongly consistent and asymptotically normal (with the exception of the intercept for the explosive case). Particularly, the MLE of the parameter in the conditional location is always asymptotically normal, regardless of the stationary or explosive case. This point totally differs from that for linear α-stable AR models in Andrews et al. (2009). Furthermore, it is the first time to provide exact values of the quantities related to the innovation in the asymptotic covariance matrices when the true innovation is the standard Cauchy distribution. Additionally, a modified Kolmogorov-type test statistic is proposed for model diagnostic checking in the stationary case. Monte Carlo simulation studies are conducted to confirm our theoretical findings and assess the finite-sample performance of the MLE and the modified Kolmogorov-type test. An empirical example is analyzed to illustrate the usefulness of sDAR models.",http://www.sciencedirect.com/science/article/pii/S0304407623001653
Journal of Econometrics,2023,Testing many restrictions under heteroskedasticity,Stanislav Anatolyev and Mikkel Sølvsten,"We propose a hypothesis test that allows for many tested restrictions in a heteroskedastic linear regression model. The test compares the conventional F statistic to a critical value that corrects for many restrictions and conditional heteroskedasticity. This correction uses leave-one-out estimation to correctly center the critical value and leave-three-out estimation to appropriately scale it. The large sample properties of the test are established in an asymptotic framework where the number of tested restrictions may be fixed or may grow with the sample size, and can even be proportional to the number of observations. We show that the test is asymptotically valid and has non-trivial asymptotic power against the same local alternatives as the exact F test when the latter is valid. Simulations corroborate these theoretical findings and suggest excellent size control in moderately small samples, even under strong heteroskedasticity.",http://www.sciencedirect.com/science/article/pii/S0304407623001677
Journal of Econometrics,2023,Post-processed posteriors for sparse covariances,Kwangmin Lee and Jaeyong Lee,"We consider Bayesian inference of sparse covariance matrices and propose a post-processed posterior. This method consists of two steps. In the first step, posterior samples are obtained from the conjugate inverse-Wishart posterior without considering the sparse structural assumption. The posterior samples are transformed in the second step to satisfy the sparse structural assumption through a generalized thresholding function. This non-traditional Bayesian procedure is justified by showing that the post-processed posterior attains the optimal minimax rates under the spectral norm loss in high-dimensional settings. We also propose the post-processed posterior for contaminated data and apply it to the estimation of the sparse idiosyncratic covariance of the approximate factor model. The advantages of our method are demonstrated via a simulation study and a real data analysis with S&P 500 data.",http://www.sciencedirect.com/science/article/pii/S0304407623001914
Journal of Econometrics,2023,Sieve BLP: A semi-nonparametric model of demand for differentiated products,Ao Wang,"We develop a semi-nonparametric approach to identify and estimate the demand for differentiated products. The proposed method adopts a random coefficients discrete choice logit model (i.e., mixed logit model) in which the distribution of random coefficients is nonparametrically specified. Our method minimizes misspecification error in the distribution to which routinely used parametric approach is subject. In addition, it overcomes the practical challenge of dimensionality in the number of products that remains the main hurdle in the nonparametric estimation of demand functions. We propose a sieve estimation procedure (referred to as sieve BLP) that remains simple to implement. Extensive Monte Carlo simulations show its robust finite-sample performance under various data generating processes. We use the method to investigate the welfare implications of a sugar tax in the ready-to-eat cereal industry in the US. This application underscores the usefulness of sieve BLP due to its ability to allow for flexibly specified individual heterogeneity in demand, especially when the researcher aims to quantify the distributional effects of a policy change.",http://www.sciencedirect.com/science/article/pii/S0304407622000860
Journal of Econometrics,2023,Testing for time stochastic dominance,"Kyungho Lee, Oliver Linton and Yoon-Jae Whang","We propose nonparametric tests for the null hypothesis of time stochastic dominance. Time stochastic dominance makes a partial order of different prospects over time based on the net present value criteria for general utility and time discount function classes. For example, time stochastic dominance can be used for ranking investment strategies or environmental policies based on the expected net present value of the future benefits. We consider an Lp-type test statistic and derive its large sample distribution under standard panel data sampling scheme with fixed time dimension. We suggest a path-wise (or cluster) bootstrap procedure that allows individual time series dependence over the time horizon. We describe two approaches, the contact-set approach and the numerical delta method, that may lead to enhanced power compared to the conventional least-favorable-case based approach. We prove the asymptotic validity of our testing procedures. We investigate the finite sample performance of the tests in simulation studies. As an illustration, we apply the proposed tests to evaluate the Million Baht Village Fund Program in Thailand and carbon emission trading scheme in China.",http://www.sciencedirect.com/science/article/pii/S0304407622000963
Journal of Econometrics,2023,Theory of evolutionary spectra for heteroskedasticity and autocorrelation robust inference in possibly misspecified and nonstationary models,Alessandro Casini,"The literature on heteroskedasticity and autocorrelation robust (HAR) inference is extensive but its usefulness relies on stationarity of the relevant process, say Vt, usually a function of the data and estimated model residuals. Yet, a large body of work shows widespread evidence of various forms of nonstationarity in the latter. Also, many testing problems are such that Vt is stationary under the null hypothesis but nonstationary under the alternative. In either case, the consequences are possible size distortions and, especially, a reduction in power which can be substantial (e.g., non-monotonic power), since all such estimates are based on weighted sums of the sample autocovariances of Vt, which are inflated. We propose HAR inference methods valid under a broad class of nonstationary processes, labeled Segmented Local Stationary, which possess a spectrum that varies both over frequencies and time. It is allowed to change either slowly and continuously and/or abruptly at some time points, thereby encompassing most nonstationary models used in applied work. We introduce a double kernel estimator (DK-HAC) that applies a smoothing over both lagged autocovariances and time. The optimal kernels and bandwidth sequences are derived under a mean-squared error criterion. The data-dependent bandwidths rely on the “plug-in” approach using approximating parametric models having time-varying parameters estimated with standard methods applied to local data. Our method yields tests with good size and power in finite-samples. In particular, the power gains are achieved without notable size distortions.",http://www.sciencedirect.com/science/article/pii/S0304407622000999
Journal of Econometrics,2023,Sharpe Ratio analysis in high dimensions: Residual-based nodewise regression in factor models,"Mehmet Caner, Marcelo Medeiros and Gabriel F.R. Vasconcelos","We provide a new theory for nodewise regression when the residuals from a fitted factor model are used. We apply our results to the analysis of the consistency of Sharpe Ratio estimators when there are many assets in a portfolio. We allow for an increasing number of assets as well as time observations of the portfolio. Since the nodewise regression is not feasible due to the unknown nature of idiosyncratic errors, we provide a feasible-residual-based nodewise regression to estimate the precision matrix of errors which is consistent even when number of assets, p, exceeds the time span of the portfolio, n. In another new development, we also show that the precision matrix of returns can be estimated consistently, even with an increasing number of factors and p>n. We show that: (1) with p>n, the Sharpe Ratio estimators are consistent in global minimum-variance and mean–variance portfolios; and (2) with p>n, the maximum Sharpe Ratio estimator is consistent when the portfolio weights sum to one; and (3) with p<",http://www.sciencedirect.com/science/article/pii/S0304407622000926
Journal of Econometrics,2023,Partial identification and inference in moment models with incomplete data,"Yanqin Fan, Xuetao Shi and Jing Tao","In this paper, we develop asymptotically valid inference in moment equality models with incomplete data, where the sample information is insufficient to identify the joint distribution of all the variables in the model. Examples of such models include the selection-on-observables framework, the counterfactual distribution, and parametric regressions with incomplete data. In the first two examples, the parameter of interest includes the values of the distribution and quantile functions of the individual treatment effect and the correlation coefficient of the potential outcomes. We show that the parameter of interest satisfies a semiparametric moment equality model with both point identified nuisance parameters and a partially identified copula function. We construct an asymptotically valid confidence set for the parameter of interest taking account of shape restrictions on the copula function. The critical value is constructed via a multiplier bootstrap. A simulation study is conducted to illustrate the finite sample performance of our inference procedure.",http://www.sciencedirect.com/science/article/pii/S0304407622001002
Journal of Econometrics,2023,Distribution-invariant differential privacy,Xuan Bi and Xiaotong Shen,"Differential privacy is becoming one gold standard for protecting the privacy of publicly shared data. It has been widely used in social science, data science, public health, information technology, and the U.S. decennial census. Nevertheless, to guarantee differential privacy, existing methods may unavoidably alter the conclusion of original data analysis, as privatization often changes the sample distribution. This phenomenon is known as the trade-off between privacy protection and statistical accuracy. In this work, we mitigate this trade-off by developing a distribution-invariant privatization (DIP) method to reconcile both high statistical accuracy and strict differential privacy. As a result, any downstream statistical or machine learning task yields essentially the same conclusion as if one used the original data. Numerically, under the same strictness of privacy protection, DIP achieves superior statistical accuracy in in a wide range of simulation studies and real-world benchmarks.",http://www.sciencedirect.com/science/article/pii/S030440762200121X
Journal of Econometrics,2023,Maximum pairwise-rank-likelihood-based inference for the semiparametric transformation model,"Tao Yu, Pengfei Li, Baojiang Chen, Ao Yuan and Jing Qin","In this paper, we study the linear transformation model in a general setup. This model includes many important and popular models in statistics and econometrics as special cases. Although it has been studied for many years, the methods in the literature are based on kernel-smoothing techniques or make use of only the ranks of the responses in the estimation of the parametric components. The former approach needs a tuning parameter, which is not easily optimally specified in practice; and some of the latter may be computationally expensive. In this paper, we propose two methods: a pairwise rank likelihood method and an estimation-equation-based method motivated from the score function of this pairwise rank likelihood. We also explore the theoretical properties of the proposed estimators. Via extensive numerical studies, we demonstrate that our methods are appealing in that the estimators are not only robust to the distribution of the random errors but also lead to mean square errors that are in many cases comparable to or smaller than those of existing methods.",http://www.sciencedirect.com/science/article/pii/S0304407622001208
Journal of Econometrics,2023,GARCH density and functional forecasts,"Karim M. Abadir, Alessandra Luati and Paolo Paruolo","This paper derives the analytic form of the multi-step ahead prediction density of a Gaussian GARCH(1,1) process with a possibly asymmetric news impact curve in the GJR class. These results can be applied when single-period returns are modeled as a GJR Gaussian GARCH(1,1) and interest lies in single-period returns at some future forecast horizon. The Gaussian density has been used in applications as an approximation to this as yet unknown prediction density; the analytic form derived here shows that this prediction density, while symmetric, can be far from Gaussian. This explicit form can be used to compute exact tail probabilities and functionals, such as the Value at Risk and the Expected Shortfall, to quantify expected future required risk capital for single-period returns. Finally, the paper shows how estimation uncertainty can be mapped onto uncertainty regions for any functional of this prediction distribution.",http://www.sciencedirect.com/science/article/pii/S0304407622001154
Journal of Econometrics,2023,Robust inference in first-price auctions: Overbidding as an identifying restriction,Serafin Grundl and Yu Zhu,"Laboratory experiments find consistently that bidding in first-price auctions tends to be more aggressive than predicted by the risk-neutral Bayesian Nash Equilibrium (RNBNE) — a finding known as the overbidding puzzle. Several alternative models can explain the overbidding puzzle, but no canonical alternative to RNBNE has emerged. Instead of estimating a particular model of overbidding, we use the overbidding restriction itself for identification, which allows us to bound the valuation distribution and the seller’s revenue under counterfactual reserve prices in the spirit of Haile and Tamer (2003). These bounds are consistent with RNBNE and all models of overbidding, and the bounds remain valid even if there is unobserved heterogeneity in bidding strategies. We evaluate the validity of the bounds numerically and in experimental data.",http://www.sciencedirect.com/science/article/pii/S0304407622001221
Journal of Econometrics,2023,Testing stochastic dominance with many conditioning variables,"Oliver Linton, Myung Hwan Seo and Yoon-Jae Whang","We propose tests of the conditional first- and second-order stochastic dominance in the presence of growing numbers of covariates. Our approach builds on a semiparametric location-scale model, where the conditional distribution of the outcome given the covariates is characterized by nonparametric mean and skedastic functions with independent innovations from an unknown distribution. The nonparametric regression functions are estimated by utilizing the ℓ1-penalized nonparametric series estimation with thresholding. Deviation bounds for the regression functions and series coefficients estimates are obtained allowing for the time series dependence. We propose test statistics, which are the maximum (integrated) deviation of a composite of the estimated regression functions and the residual empirical distribution, and introduce a smooth stationary bootstrap to compute p-values. We investigate the finite sample performance of the bootstrap critical values by a set of Monte Carlo simulations. Finally, our method is illustrated by an application to stochastic dominance among portfolio returns given all the past information.",http://www.sciencedirect.com/science/article/pii/S0304407622001191
Journal of Econometrics,2023,Partial identification in nonseparable binary response models with endogenous regressors,Jiaying Gu and Thomas M. Russell,"This paper considers (partial) identification of a variety of counterfactual parameters in binary response models with possibly endogenous regressors. Our framework allows for nonseparable index functions with multi-dimensional latent variables, and does not require parametric distributional assumptions. We leverage results on hyperplane arrangements and cell enumeration from the literature on computational geometry in order to provide a tractable means of computing the identified set. We demonstrate how various functional form, independence, and monotonicity assumptions can be imposed as constraints in our optimization procedure to tighten the identified set. Finally, we apply our method to study the effects of health insurance on the decision to seek medical treatment.",http://www.sciencedirect.com/science/article/pii/S0304407622001166
Journal of Econometrics,2023,Robust inference with stochastic local unit root regressors in predictive regressions,Yanbo Liu and Peter Phillips,"This paper explores predictive regression models with stochastic unit root (STUR) components and robust inference procedures that encompass a wide class of persistent and time-varying stochastically nonstationary regressors. The paper extends the mechanism of endogenously generated instrumentation known as IVX, showing that these methods remain valid for short and long-horizon predictive regressions in which the predictors have STUR and local STUR (LSTUR) generating mechanisms. Both mean regression and quantile regression methods are considered. The asymptotic distributions of the IVX estimators are new and require some new methods in their derivation. The distributions are compared to previous results and, as in earlier work, lead to pivotal limit distributions for Wald testing procedures that remain robust for both single and multiple regressors with various degrees of persistence and stochastic and fixed local departures from unit roots. Numerical experiments corroborate the asymptotic theory, and IVX testing shows good power and size control. The IVX methods are illustrated in an empirical application to evaluate the predictive capability of economic fundamentals in forecasting S&P 500 excess returns.",http://www.sciencedirect.com/science/article/pii/S0304407622001233
Journal of Econometrics,2023,Model averaging for asymptotically optimal combined forecasts,Yi-Ting Chen and Chu-An Liu,"We propose a model-averaging (MA) method for constructing an asymptotically optimal combination of a set of point forecast sequences generated from a class of predictive regressions. The asymptotic optimality is defined in terms of approximating an unknown conditional-mean sequence based on the local(-to-zero) asymptotics. Our method has the following essential features. First, it is more general than combining a set of single point forecasts. Second, the asymptotic optimality is generally dependent on the estimation scheme and the asymptotic ratio of the length of forecast sequence relative to the in-sample size. Third, the asymptotically optimal weights may be consistently estimated under suitable conditions, while it needs the time series to be sufficiently long. We also assess the forecasting performance of our method using simulation data and real data.",http://www.sciencedirect.com/science/article/pii/S0304407622001245
Journal of Econometrics,2023,Global robust Bayesian analysis in large models,Paul Ho,"This paper develops a tool for global prior sensitivity analysis in large Bayesian models. Without imposing parametric restrictions, the methodology provides bounds for posterior means or quantiles given any prior close to the original in relative entropy and reveals features of the prior that are important for the posterior statistics of interest. We develop a sequential Monte Carlo algorithm and use approximations to the likelihood and statistic of interest to implement the calculations. The methodology finds that the prior tightness hyperparameters in the hierarchical vector autoregression model from Giannone et al. (2015) are relatively insensitive to their hyperpriors. However, in the New Keynesian model of Smets and Wouters (2007), the error bands for the impulse response of output to a monetary policy shock depend heavily on the prior. The upper bound is especially sensitive, and the prior on wage rigidity plays a particularly important role.",http://www.sciencedirect.com/science/article/pii/S0304407622001257
Journal of Econometrics,2023,Discrete mixtures of normals pseudo maximum likelihood estimators of structural vector autoregressions,Gabriele Fiorentini and Enrique Sentana,"Likelihood inference in structural vector autoregressions with independent non-Gaussian shocks leads to parametric identification and efficient estimation at the risk of inconsistencies under distributional misspecification. We prove that autoregressive coefficients and (scaled) impact multipliers remain consistent, but the drifts and shocks’ standard deviations are generally inconsistent. Nevertheless, we show consistency when the non-Gaussian log-likelihood uses a discrete scale mixture of normals in the symmetric case, or an unrestricted finite mixture more generally, and compare the efficiency of these estimators to other consistent two-step proposals, including our own. Finally, our empirical application looks at dynamic linkages between three popular volatility indices.",http://www.sciencedirect.com/science/article/pii/S0304407622001269
Journal of Econometrics,2023,"Prices, profits, proxies, and production","Victor Aguiar, Nail Kashaev and Roy Allen","This paper studies nonparametric identification and counterfactual bounds for heterogeneous firms that can be ranked in terms of productivity. Our approach works when quantities and prices are latent, rendering standard approaches inapplicable. Instead, we require observation of profits or other optimizing-values such as costs or revenues, and either prices or price proxies of flexibly chosen variables. We extend classical duality results for price-taking firms to a setup with discrete heterogeneity, endogeneity, and limited variation in possibly latent prices. Finally, we show that convergence results for nonparametric estimators may be directly converted to convergence results for production sets.",http://www.sciencedirect.com/science/article/pii/S0304407622001300
Journal of Econometrics,2023,Uniform inference in linear panel data models with two-dimensional heterogeneity,Xun Lu and Liangjun Su,"This paper studies uniform inference in a linear panel data model when the slope coefficients may exhibit heterogeneity over both the individual and time dimensions and they can be correlated with the regressors. We propose a generalized two-way fixed effects (GTWFE) estimation procedure to estimate the model. To establish the asymptotic properties of the GTWFE estimators, we invert a number of large dimensional square matrices by approximating them with quasi-Kronecker structured matrices. We establish the asymptotic normality of our GTWFE estimators and show that their convergence rates depend on the unknown degree of parameter heterogeneity. To make a uniform inference on the common slope component, we propose a novel triple-bootstrap procedure to estimate the asymptotic variance. Simulations show the superb performance of our estimators and inference procedure. We apply our method to study the relationship between savings and investments, and find significant parameter heterogeneity along both the individual and time dimensions.",http://www.sciencedirect.com/science/article/pii/S0304407622001312
Journal of Econometrics,2023,Specification tests for time-varying coefficient models,"Zhonghao Fu, Yongmiao Hong, Liangjun Su and Xia Wang","Time-varying coefficient models have been widely used to characterize changing relationships among economic and financial variables. The existing literature usually specifies the time-varying coefficient vector as a stationary stochastic process, a deterministic function of time, and a unit root process, respectively. In this paper, we propose two tests to distinguish these three specifications. Both test statistics follow asymptotic normal distributions under the respective null hypotheses and diverge to infinity in probability under the corresponding alternatives. To improve the finite sample performance of the tests, we propose a dependent wild bootstrap to obtain the bootstrap critical value (or P-value) and establish its asymptotic validity. Simulation studies show that our bootstrap-based tests perform reasonably well in finite samples. We apply the proposed tests to the time-varying specifications of the equity return’s predictive model, the U.S. Taylor rule, and inflation persistence, respectively. The results suggest that a unit root process is favored for the first application, whereas a deterministic function of time should be adopted for the latter two applications.",http://www.sciencedirect.com/science/article/pii/S0304407622001440
Journal of Econometrics,2023,A GMM approach to estimate the roughness of stochastic volatility,"Anine E. Bolko, Kim Christensen, Mikko S. Pakkanen and Bezirgen Veliyev","We develop a GMM approach for estimation of log-normal stochastic volatility models driven by a fractional Brownian motion with unrestricted Hurst exponent. We show that a parameter estimator based on the integrated variance is consistent and, under stronger conditions, asymptotically normally distributed. We inspect the behavior of our procedure when integrated variance is replaced with a noisy measure of volatility calculated from discrete high-frequency data. The realized estimator contains sampling error, which skews the fractal coefficient toward “illusive roughness.” We construct an analytical approach to control the impact of measurement error without introducing nuisance parameters. In a simulation study, we demonstrate convincing small sample properties of our approach based both on integrated and realized variance over the entire memory spectrum. We show the bias correction attenuates any systematic deviance in the parameter estimates. Our procedure is applied to empirical high-frequency data from numerous leading equity indexes. With our robust approach the Hurst index is estimated around 0.05, confirming roughness in stochastic volatility.",http://www.sciencedirect.com/science/article/pii/S0304407622001476
Journal of Econometrics,2023,News-implied linkages and local dependency in the equity market,"Shuyi Ge, Shaoran Li and Oliver Linton","This paper studies a heterogeneous coefficient spatial factor model that separately addresses both common factor risks (strong cross-sectional dependence) and local dependency (weak cross-sectional dependence) in equity returns. From the asset pricing perspective, we derive the theoretical implications of no asymptotic arbitrage for the heterogeneous spatial factor model, generalizing the work of Kou et al. (2018). We also provide the associated Wald tests for the APT restrictions in the general case when there are both traded and non-traded factors. On the empirical side, it is challenging to measure granular firm-to-firm connectivity for a high-dimensional panel of equity returns. We use extensive business news to construct firms’ links through which local shocks transmit, and we use those news-implied linkages as a proxy for the connectivity among firms. Empirically, we document a considerable degree of local dependency among S&P500 stocks, and the spatial component does a great job in capturing the remaining correlations in the de-factored returns. We find that adding spatial interaction terms to factor models reduces mispricing and boosts model fitting. By comparing the performance of the model estimated using different networks, we show that the news-implied linkages provide a comprehensive and integrated proxy for firm-to-firm connectivity.",http://www.sciencedirect.com/science/article/pii/S0304407622001488
Journal of Econometrics,2023,Threshold regression with nonparametric sample splitting,Yoonseok Lee and Yulong Wang,This paper develops a threshold regression model where an unknown relationship between two variables nonparametrically determines the threshold. We allow the observations to be cross-sectionally dependent so that the model can be applied to determine an unknown spatial border for sample splitting over a random field. We derive the uniform rate of convergence and the nonstandard limiting distribution of the nonparametric threshold estimator. We also obtain the root-n consistency and the asymptotic normality of the regression coefficient estimator. We illustrate empirical relevance of this new model by estimating the tipping point in social segregation problems as a function of demographic characteristics; and determining metropolitan area boundaries using nighttime light intensity collected from satellite imagery.,http://www.sciencedirect.com/science/article/pii/S030440762200149X
Journal of Econometrics,2023,"Variance–covariance from a metropolis chain on a curved, singular manifold",A. Ronald Gallant,"We consider estimation of variance and covariance from a point cloud that are draws from a posterior distribution that lie on a curved, singular manifold. The motivating application is Bayesian inference regarding a likelihood subject to overidentified moment equations using MCMC (Markov Chain Monte Carlo). The MCMC draws lie on a singular manifold that typically is curved. Variance and covariance are Euclidean concepts. A curved, singular manifold is not typically a Euclidean space. We explore some suggestions on how to adapt a Euclidean concept to a non-Euclidean space then build on them to propose and illustrate appropriate methods.",http://www.sciencedirect.com/science/article/pii/S0304407622001506
Journal of Econometrics,2023,Identification and inference of network formation games with misclassified links,Luis E. Candelaria and Takuya Ura,"This paper considers a network formation model when links are potentially misclassified. We focus on a game-theoretical model of strategic network formation with incomplete information, in which the linking decisions depend on agents’ exogenous attributes and endogenous network characteristics. In the presence of link misclassification, we derive moment conditions that characterize the identified set for the preference parameters associated with homophily and network externalities. Based on the moment equality conditions, we provide an inference method that is asymptotically valid when a single network of many agents is observed. Finally, we apply our misclassification-robust method to study the preference parameters of a lending network in rural villages in southern India.",http://www.sciencedirect.com/science/article/pii/S0304407622001531
Journal of Econometrics,2023,Using monotonicity restrictions to identify models with partially latent covariates,"Minji Bang, Wayne Gao, Andrew Postlewaite and Holger Sieg","This paper develops a new method for identifying econometric models with partially latent covariates. Such data structures arise in industrial organization and labor economics settings where data are collected using an input-based sampling strategy, e.g., if the sampling unit is one of multiple labor input factors. We show that the latent covariates can be nonparametrically identified, if they are functions of a common shock satisfying some plausible monotonicity assumptions. With the latent covariates identified, semiparametric estimation of the outcome equation proceeds within a standard IV framework that accounts for the endogeneity of the covariates. We illustrate the usefulness of our method using a new application that focuses on the production functions of pharmacies. We find that differences in technology between chains and independent pharmacies may partially explain the observed transformation of the industry structure.",http://www.sciencedirect.com/science/article/pii/S0304407622001555
Journal of Econometrics,2023,Using large samples in econometrics,James MacKinnon,"As I demonstrate using evidence from a journal data repository that I manage, the datasets used in empirical work are getting larger. When we use very large datasets, it can be dangerous to rely on standard methods for statistical inference. In addition, we need to worry about computational issues. We must be careful in our choice of statistical methods and the algorithms used to implement them.",http://www.sciencedirect.com/science/article/pii/S0304407622001580
Journal of Econometrics,2023,Profile GMM estimation of panel data models with interactive fixed effects,"Shengjie Hong, Liangjun Su and Tao Jiang","This paper studies panel data models with interactive fixed effects where the regressors are allowed to be correlated with the idiosyncratic error terms. We propose a two-step profile GMM estimation procedure to estimate the parameters of interest. In the first step we obtain a preliminary consistent estimate of the slope coefficient via a nuclear-norm-regularization (NNR) based profile GMM procedure. In the second step, via an iterative procedure, we conduct post-NNR profile GMM estimation of the slope coefficient, factors, and factor loadings, with an improved convergence rate for the estimate of the slope coefficient. We establish the asymptotic properties of the preliminary estimates and the iterative estimates, and propose an efficient profile GMM estimator. We also study the determination of the number of factors and propose Hausman tests for the exogeneity of the regressor. Monte Carlo simulations suggest that the proposed estimation and testing methods work well in the determination of the number of factors, the estimation of the model parameters and the test for exogeneity. As an empirical application, we apply our model and method to study the price elasticity of U.S. imports.",http://www.sciencedirect.com/science/article/pii/S0304407622001592
Journal of Econometrics,2023,Bootstrap specification tests for dynamic conditional distribution models,Indeewara Perera and Mervyn J. Silvapulle,"This paper proposes bootstrap based tests for the specification of a given parametric conditional distribution in autoregressive time series with GARCH-type disturbances. The tests are based on an estimated residual empirical process and are implemented by parametric bootstrap. We show that the proposed tests are asymptotically valid, consistent, and have nontrivial asymptotic power against a large proportion of local alternatives. Our approach relies on non-primitive regularity conditions and certain properties of exponential almost sure convergence. The regularity conditions are shown to be satisfied by GARCH(p,q); this technique of verification is applicable to other models as well. In our Monte Carlo study, the proposed tests performed well and better than several competing tests, including the information matrix test. A real data example illustrates the testing procedure.",http://www.sciencedirect.com/science/article/pii/S0304407622001634
Journal of Econometrics,2023,Testing the martingale difference hypothesis in high dimension,"Jinyuan Chang, Qing Jiang and Xiaofeng Shao","In this paper, we consider testing the martingale difference hypothesis for high-dimensional time series. Our test is built on the sum of squares of the element-wise max-norm of the proposed matrix-valued nonlinear dependence measure at different lags. To conduct the inference, we approximate the null distribution of our test statistic by Gaussian approximation and provide a simulation-based approach to generate critical values. The asymptotic behavior of the test statistic under the alternative is also studied. Our approach is nonparametric as the null hypothesis only assumes the time series concerned is martingale difference without specifying any parametric forms of its conditional moments. As an advantage of Gaussian approximation, our test is robust to the cross-series dependence of unknown magnitude. To the best of our knowledge, this is the first valid test for the martingale difference hypothesis that not only allows for large dimension but also captures nonlinear serial dependence. The practical usefulness of our test is illustrated via simulation and a real data analysis. The test is implemented in a user-friendly R-function.",http://www.sciencedirect.com/science/article/pii/S0304407622001658
Journal of Econometrics,2023,Semiparametric partially linear varying coefficient modal regression,"Aman Ullah, Tao Wang and Weixin Yao","We in this paper propose a semiparametric partially linear varying coefficient (SPLVC) modal regression, in which the conditional mode function of the response variable given covariates admits a partially linear varying coefficient structure. In comparison to existing regressions, the newly developed SPLVC modal regression captures the “most likely” effect and provides superior prediction performance when the data distribution is skewed. The consistency and asymptotic properties of the resultant estimators for both parametric and nonparametric parts are rigorously established. We employ a kernel-based objective function to simplify the computation and a modified modal-expectation–maximization (MEM) algorithm to estimate the model numerically. Furthermore, taking the residual sums of modes as the loss function, we construct a goodness-of-fit testing statistic for hypotheses on the coefficient functions, whose limiting null distribution is shown to follow an asymptotically χ2-distribution with a scale dependent on density functions. To achieve sparsity in the high-dimensional SPLVC modal regression, we develop a regularized estimation procedure by imposing a penalty on the coefficients in the parametric part to eliminate the irrelevant variables. Monte Carlo simulations and two real-data applications are conducted to examine the performance of the suggested estimation methods and hypothesis test. We also briefly explore the extension of the SPLVC modal regression to the case where some varying coefficient functions admit higher-order smoothness.",http://www.sciencedirect.com/science/article/pii/S030440762200166X
Journal of Econometrics,2023,Indirect inference estimation of dynamic panel data models,Yong Bao and Xuewen Yu,"This paper proposes an estimator for higher-order dynamic panel models based on the idea of indirect inference by matching the simple within-group estimator with its analytical approximate expectation. The resulting estimator is shown to be consistent and asymptotically normal. For the special case of first-order dynamic panel, the estimator yields numerically the same result from an existing procedure in the literature, but the inference to follow differs and this paper examines the differences and implications for hypothesis testing. Monte Carlo simulations show that the proposed estimator is virtually unbiased, achieves usually lower root mean squared error than competing estimators, and delivers very reliable empirical size across various parameter configurations and error distributions. This new estimator is used to estimate the convergence parameter in an inequality measure among 63 countries during 1985–2015. It shows strong evidence of convergence over long test horizons but much weaker evidence over a 5-year horizon for developing countries.",http://www.sciencedirect.com/science/article/pii/S0304407622001683
Journal of Econometrics,2023,Macroeconomic forecasting and variable ordering in multivariate stochastic volatility models,"Jonas E. Arias, Juan F Rubio-Ramirez and Minchul Shin","We document five novel empirical findings on the well-known potential ordering drawback associated with the time-varying parameter vector autoregression with stochastic volatility developed by Cogley and Sargent (2005) and Primiceri (2005). First, the ordering does not affect point prediction. Second, the standard deviation of the predictive densities implied by different orderings can differ substantially. Third, the average length of the prediction intervals is also sensitive to the ordering. Fourth, the best ordering for one variable in terms of log-predictive scores does not necessarily imply the best ordering for another variable under the same metric. Fifth, the ordering problem becomes exacerbated in conditional forecasting exercises. Then, we consider three alternative ordering invariant models: a canonical discounted Wishart stochastic volatility model and two dynamic stochastic correlation models. When the forecasting performance of these ordering invariant models is compared to Cogley, Primiceri, and Sargent’s ordering variant model, the former underperforms relative to all orderings and the latter two have an out-of-sample forecasting performance comparable with the median outcomes across orderings.",http://www.sciencedirect.com/science/article/pii/S0304407622001695
Journal of Econometrics,2023,Nonparametric identification and estimation of the extended Roy model,Ji Hyung Lee and Byoung Park,"We propose a new identification method for the extended Roy model, in which the agents maximize their utility rather than just their outcome. We nonparametrically identify the joint distribution of potential outcomes, which is of great importance in causal inference. We exploit the extended Roy model structure and the monotonicity assumption but do not require any functional form assumption nor any support assumption. The identification is achieved by matching the indifferent agents across choices, who are identified by the local instrumental variable method. Based on the identification result, we propose an easy-to-implement nonparametric simulation-based estimator and derive its convergence rate. An empirical illustration on Malawian farmers’ hybrid maize adoption is provided.",http://www.sciencedirect.com/science/article/pii/S0304407622001774
Journal of Econometrics,2023,Lasso inference for high-dimensional time series,"Robert Adamek, Stephan Smeekes and Ines Wilms","In this paper we develop valid inference for high-dimensional time series. We extend the desparsified lasso to a time series setting under Near-Epoch Dependence (NED) assumptions allowing for non-Gaussian, serially correlated and heteroskedastic processes, where the number of regressors can possibly grow faster than the time dimension. We first derive an error bound under weak sparsity, which, coupled with the NED assumption, means this inequality can also be applied to the (inherently misspecified) nodewise regressions performed in the desparsified lasso. This allows us to establish the uniform asymptotic normality of the desparsified lasso under general conditions, including for inference on parameters of increasing dimensions. Additionally, we show consistency of a long-run variance estimator, thus providing a complete set of tools for performing inference in high-dimensional linear time series models. Finally, we perform a simulation exercise to demonstrate the small sample properties of the desparsified lasso in common time series settings.",http://www.sciencedirect.com/science/article/pii/S0304407622001804
Journal of Econometrics,2023,ETF Basket-Adjusted Covariance estimation,"Kris Boudt, Kirill Dragun, Orimar Sauri and Steven Vanduffel","The increase in trading frequency of Exchanged Traded Funds (ETFs) presents a positive externality for financial risk management when the price of the ETF is available at a higher frequency than the price of the component stocks. The positive spillover consists in improving the accuracy of pre-estimators of the integrated covariance of the stocks included in the ETF basket. The proposed ETF Basket-Adjusted Covariance (BAC) equals the pre-estimator plus a minimal adjustment matrix such that the covariance-implied stock-ETF covariation equals a target value. We focus on a truncated pre-averaged version of the (Hayashi and Yoshida, 2005) pre-estimator and derive the asymptotic properties of its implied stock-ETF covariation. The simulation study confirms that the accuracy gains are substantial in all cases considered. In the empirical part of the paper, we show the gains in tracking error efficiency when using the BAC adjustment to construct portfolios that replicate a broad index using a subset of stocks.",http://www.sciencedirect.com/science/article/pii/S0304407622001816
Journal of Econometrics,2023,Distinguishing incentive from selection effects in auction-determined contracts,"Laurent Lamy, Manasa Patnam and Michael Visser","This paper develops a novel approach to estimate how contract and principal–agent characteristics influence a post-auction outcome when the matching between agents and principals derives from an auction process. We propose a control-function approach to account jointly for the endogeneity of contracts and matching. This consists of, first, estimating the primitives of an interdependent values auction model – which is shown to be non-parametrically identified from the bidding data – second, constructing control functions based on the distribution of the unobserved private signals conditional on the auction outcome. A Monte Carlo study shows that our augmented outcome equation corrects the endogeneity biases well, even in small samples. We apply our methodology to a labor market application and estimate the effect of sports players’ auction-determined wages on their individual performances. We also use our structural estimates to evaluate the strength of matching inefficiencies and assess counterfactual reservation wage policies.",http://www.sciencedirect.com/science/article/pii/S0304407622001828
Journal of Econometrics,2023,Peer effects and endogenous social interactions,Koen Jochmans,"This paper proposes a solution to the problem of endogenous selection of peers in the linear-in-means model. We do not require to specify a model for how the selection of peers comes about. Rather, we exploit two restrictions that are inherent in many such specifications to construct conditional moment conditions. The restrictions in question are that link decisions that involve a given individual are not all independent of one another, but that they are independent of the link decisions made between other pairs of individuals that are located sufficiently far away in the network. These conditions imply that instrumental variables can be constructed from leave-own-out networks.",http://www.sciencedirect.com/science/article/pii/S0304407622001853
Journal of Econometrics,2023,The role of score and information bias in panel data likelihoods,"Martin Schumann, Thomas A. Severini and Gautam Tripathi","We show why reducing information bias can improve the performance of likelihood based estimators and confidence regions in small samples, and why it seems to matter more for inference than for estimation. The insights in this paper are helpful in explaining several simulation findings in the panel data literature. E.g., we can explain the well documented phenomenon that reducing the score bias alone often reduces the finite sample variance of estimators and improves the coverage of confidence regions in small samples, and why confidence regions based on conditional (on sufficient statistics) likelihoods can have excellent coverage even in very short panels. We can also explain the simulation results in Schumann, Severini, and Tripathi (2021), who find that, in short panels, estimators and confidence regions based on pseudolikelihoods that are simultaneously first-order score and information unbiased perform much better than those based on pseudolikelihoods that are only first-order score unbiased.",http://www.sciencedirect.com/science/article/pii/S0304407622001889
Journal of Econometrics,2023,Community network auto-regression for high-dimensional time series,"Elynn Y. Chen, Jianqing Fan and Xuening Zhu","Modeling responses on the nodes of a large-scale network is an important task that arises commonly in practice. This paper proposes a community network vector autoregressive (CNAR) model, which utilizes the network structure to characterize the dependence and intra-community homogeneity of the high-dimensional time series. The CNAR model greatly increases the flexibility and generality of the network vector autoregressive (NAR) model proposed by Zhu et al. (2017) by allowing heterogeneous network effects across different network communities. In addition, the non-community-related latent factors are included to account for unknown cross-sectional dependence. The number of network communities can diverge as the network expands, which leads to estimating a diverging number of model parameters. We obtain a set of stationary conditions and develop an efficient two-step weighted least-squares estimator. The consistency and asymptotic normality properties of the estimators are established. Theoretical results show that the two-step estimator can further improve the efficiency of one-step estimator when the error admits a factor structure. The advantages of the CNAR model are illustrated on a variety of synthetic and real datasets.",http://www.sciencedirect.com/science/article/pii/S0304407622001890
Journal of Econometrics,2023,Nonparametric identification and estimation with discrete instruments and regressors,Isaac Loh,"In a nonparametric instrumental regression model in which the regressor and instrument are discretely distributed, we strengthen the conventional moment independence assumption between instrument and residual to include higher order moments. We give conditions under which a function of interest is partially identified when the regressor has more mass points than the instrument. We also show that the function is point identified as long as certain latent parameters corresponding to conditional moments of the residual lie outside of a set of measure zero. We give an asymptotically normal estimator for the structural function when it is point identified. We also provide a straightforward method for inference under partial identification. These perform well in Monte-Carlo simulations and in two empirical applications.",http://www.sciencedirect.com/science/article/pii/S0304407622001907
Journal of Econometrics,2023,Asymptotic F test in regressions with observations collected at high frequency over long span,Daniel F. Pellatt and Yixiao Sun,"This paper proposes tests of linear hypotheses when the variables may be continuous-time processes with observations collected at a high sampling frequency over a long span. Utilizing series long run variance (LRV) estimation in place of the traditional kernel LRV estimation, we develop easy-to-implement and more accurate F tests in both stationary and nonstationary environments. The nonstationary environment accommodates exogenous regressors that are general semimartingales. Endogenous regressors are allowed in a nonstationary environment similar to cointegration models in the usual discrete-time setting. The F tests can be implemented in exactly the same way as in the discrete-time setting. The F tests are, therefore, robust to the continuous-time or discrete-time nature of the data. Simulations demonstrate the improved size accuracy and competitive power of the F tests relative to existing continuous-time testing procedures and their improved versions. The F tests are of practical interest as recent work by Chang et al. (2021) demonstrates that traditional inference methods can become invalid and produce spurious results when continuous-time processes are observed on finer grids over a long span.",http://www.sciencedirect.com/science/article/pii/S0304407622001919
Journal of Econometrics,2023,Two-step estimation of censored quantile regression for duration models with time-varying regressors,Songnian Chen,"Common duration models are characterized by strong homogeneity and thus are highly restrictive in allowing for how regressors affect the conditional duration distribution. In particular, the implied sign and relative marginal quantile effects remain the same over the entire range of the conditional duration distribution, which rules out general heterogeneous effects in duration data. Quantile regression, which offers a flexible and unified framework that allows for general heterogeneous effects, is particularly well suited to duration analysis. Based on the insights behind the accelerated failure time model (AFT) with time-varying regressors (Cox and Oakes, 1984) and the standard quantile regression model (Koenker and Bassett, 1978), Chen (2019) recently developed a quantile regression framework with time-varying regressors. However, Chen’s (2019) estimator is very difficult to compute because the estimation procedure involves a non-convex and nonlinear high dimensional optimization problem due to censoring and the nonlinearity of the quantile function. In this paper I propose an easy-to-implement two-step quantile regression estimator, which significantly reduces the computational burden. The estimator is shown to be consistent and asymptotically normal. Monte Carlo experiments indicate that our estimator perform well in finite samples.",http://www.sciencedirect.com/science/article/pii/S0304407622001920
Journal of Econometrics,2023,Testing and signal identification for two-sample high-dimensional covariances via multi-level thresholding,"Song Chen, Bin Guo and Yumou Qiu","The paper considers testing and signal identification for covariance matrices from two populations of marginally sub-Gaussian distributed. A multi-level thresholding procedure is proposed for testing the equality of two high-dimensional covariance matrices, which is designed to detect sparse and faint differences between the covariances. A novel U-statistic composition is developed to establish the asymptotic distribution of the thresholding statistics in conjunction with the matrix blocking and the coupling techniques. It is shown that the proposed test is more powerful than the existing tests in detecting sparse and weak signals in covariances. Multiple testing procedures are constructed to discover different covariances and the sub-groups of variables with different covariance structures between the two populations. The proposed procedures are based on the multi-level thresholding test, which are able to control the false discovery proportion (FDP) with high power. Simulation experiments and a case study on the returns of the S&P 500 stocks before and after the COVID-19 pandemic are conducted to demonstrate and compare the utilities of the proposed methods.",http://www.sciencedirect.com/science/article/pii/S0304407622001944
Journal of Econometrics,2023,Penalized time-varying model averaging,"Yuying Sun, Yongmiao Hong, Shouyang Wang and Xinyu Zhang","This paper proposes a new penalized time-varying model averaging method to determine optimal time-varying combination weights for candidate models, which avoids over-fitting and yields sparseness from various potential predictive variables. The asymptotic optimality and convergence rate of the selected weights are derived even when all candidate models are misspecified, and the consistency and normality of the proposed time-varying model averaging estimator are obtained when the true model is included in the candidate models. Simulation studies and empirical applications to inflation forecasting highlight the merits of the proposed method.",http://www.sciencedirect.com/science/article/pii/S0304407622002019
Journal of Econometrics,2023,Time-varying unobserved heterogeneity in earnings shocks,Irene Botosaru,"This paper considers the transitory-permanent model for the earnings process, and allows for time-varying individual-specific unobserved heterogeneity in each shock. The cross-sectional heterogeneity in each shock is drawn from an unknown distribution at each time period. Sufficient conditions for the nonparametric identification of the cross-sectional density functions of the heterogeneity are provided, under different assumptions on the time series behavior of the transitory shock. The method proposed is then applied to earnings data to document a high degree of cross-sectional heterogeneity in each shock.",http://www.sciencedirect.com/science/article/pii/S0304407622002020
Journal of Econometrics,2023,Intraday cross-sectional distributions of systematic risk,"Torben Andersen, Raul Riva, Martin Thyrsgaard and Viktor Todorov","We develop a test for the detection of intraday changes in the cross-sectional distribution of assets’ exposure to observable factors. The test is constructed for a panel of high-frequency asset returns, with the size of the cross-section and the sampling frequency increasing simultaneously. It is based on a comparison of the empirical characteristic functions of estimates of the assets’ factor loadings at different parts of the trading day, formed from local blocks of asset returns and the corresponding factor realizations. The limiting behavior of the test statistic is governed by unobservable latent factors in the asset prices. The critical values of the test are constructed on the basis of a novel simulation-based procedure. Empirical implementation of the test to stocks in the S&P 500 index and the five Fama–French factors, as well as the momentum factor, reveals different intraday behavior of the factor loadings: assets’ exposure to size, market and value risks vary systematically over the trading day while the three remaining factors do not exhibit statistically significant intraday variation. Moreover, we find diverse, and for some factors large, reactions in the assets’ factor loadings to major economic or firm specific news releases. Finally, we document that time-varying correlations between the observable risk factors drive a wedge between the time-of-day pattern of market betas, estimated with and without control for the other observable risk factors.",http://www.sciencedirect.com/science/article/pii/S0304407622002032
Journal of Econometrics,2023,Comparing stochastic volatility specifications for large Bayesian VARs,Joshua Chan,"Large Bayesian vector autoregressions with various forms of stochastic volatility have become increasingly popular in empirical macroeconomics. One main difficulty for practitioners is to choose the most suitable stochastic volatility specification for their particular application. We develop Bayesian model comparison methods–based on marginal likelihood estimators that combine conditional Monte Carlo and adaptive importance sampling–to choose among a variety of stochastic volatility specifications. The proposed methods can also be used to select an appropriate shrinkage prior on the VAR coefficients, which is a critical component for avoiding over-fitting in high-dimensional settings. Using US quarterly data of different dimensions, we find that both the Cholesky stochastic volatility and factor stochastic volatility outperform the common stochastic volatility specification. Their superior performance, however, can mostly be attributed to the more flexible priors that accommodate cross-variable shrinkage.",http://www.sciencedirect.com/science/article/pii/S0304407622002056
Journal of Econometrics,2023,The distribution of rolling regression estimators,Zongwu Cai and Ted Juhl,"We establish the asymptotic distribution for rolling linear regression models using various window widths. The limiting distribution depends on the width of the rolling window and on a “bias process” that is typically ignored in practice. Based on the asymptotic distribution, we tabulate critical values used to find uniform confidence intervals for the average values of regression parameters over the windows. We propose a corrected rolling regression technique that removes the bias process by rolling over smoothed parameter estimates. The procedure is illustrated using a series of Monte Carlo experiments. The paper includes an empirical example to show how the confidence bands suggest alternative conclusions about the persistence of inflation.",http://www.sciencedirect.com/science/article/pii/S0304407622002068
Journal of Econometrics,2023,Estimation and identification of latent group structures in panel data,Ali Mehrabani,"This paper provides a framework for joint estimation and identification of latent group structures in panel data models using a pairwise fusion penalized approach. The latent structure of the model allows individuals to be classified into different groups where the number of groups and the group membership are unknown. The individuals within a group have common slope parameters, while parameter heterogeneity is allowed across the groups. A penalized least squares (PLS) approach is introduced for models with exogenous regressors. When the model contains endogenous regressors, a penalized generalized method of moment (PGMM) is introduced. To implement the proposed approach, an alternating direction method of multipliers algorithm has been developed. The proposed method is further illustrated by simulation studies which demonstrate the finite sample performance of the method, and is applied in an empirical analysis.",http://www.sciencedirect.com/science/article/pii/S030440762200207X
Journal of Econometrics,2023,Parametric estimation of long memory in factor models,Yunus Emre Ergemen,"A dynamic factor model is proposed in that factor dynamics are driven by stochastic time trends describing arbitrary persistence levels. The proposed model is essentially a long memory factor model, which nests standard I(0) and I(1) behavior smoothly in common factors. In the estimation, principal components analysis (PCA) and conditional sum of squares (CSS) estimations are employed. For the dynamic model parameters, centered normal asymptotics are established at the usual parametric rates, and their small-sample properties are explored via Monte-Carlo experiments. The method is then applied to a panel of U.S. industry realized volatilities.",http://www.sciencedirect.com/science/article/pii/S030440762200210X
Journal of Econometrics,2023,Estimation and inference in factor copula models with exogenous covariates,Alexander Mayer and Dominik Wied,"A factor copula model is proposed in which factors are either simulable or estimable from exogenous information. Point estimation and inference are based on a simulated methods of moments (SMM) approach with non-overlapping simulation draws. Consistency and limiting normality of the estimator is established and the validity of bootstrap standard errors is shown. Doing so, previous results from the literature are verified under low-level conditions imposed on the individual components of the factor structure. Monte Carlo evidence confirms the accuracy of the asymptotic theory in finite samples and an empirical application illustrates the usefulness of the model to explain the cross-sectional dependence between stock returns.",http://www.sciencedirect.com/science/article/pii/S0304407623000039
Journal of Econometrics,2023,Dividend suspensions and cash flows during the Covid-19 pandemic: A dynamic econometric model,"Davide Pettenuzzo, Riccardo Sabbatucci and Allan Timmermann","Firms suspended dividend payments in unprecedented numbers in response to the outbreak of the Covid-19 pandemic. We develop a multivariate dynamic econometric model that allows dividend suspensions to affect the conditional mean, volatility, and jump probability of growth in daily industry-level dividends and demonstrate how the parameters of this model can be estimated using Bayesian Gibbs sampling methods. We find considerable heterogeneity across industries in the dynamics of daily dividend growth and the impact of dividend suspensions.",http://www.sciencedirect.com/science/article/pii/S030440762300012X
Journal of Econometrics,2023,Joint inference based on Stein-type averaging estimators in the linear regression model,Tom Boot,"While averaging unrestricted with restricted estimators is known to reduce estimation risk, it is an open question whether this reduction in turn can improve inference. To analyze this question, we construct joint confidence regions centered at James–Stein averaging estimators in both homoskedastic and heteroskedastic linear regression models. These regions are asymptotically valid when the number of restrictions increases possibly proportionally with the sample size. When used for hypothesis testing, we show that suitable restrictions enhance power over the standard F-test. We study the practical implementation through simulations and an application to consumption-based asset pricing.",http://www.sciencedirect.com/science/article/pii/S0304407623000155
Journal of Econometrics,2023,A functional estimation approach to the first-price auction models,"Andreea Enache, Jean-Pierre Florens and Erwann Sbai",This paper introduces new methods of identification and estimation of the first-price sealed bid auction model and compares them with the previous existing ones.,http://www.sciencedirect.com/science/article/pii/S0304407623000192
Journal of Econometrics,2023,Identifying causal effects in experiments with spillovers and non-compliance,"Francis J. DiTraglia, Camilo García-Jimeno, O’Keeffe-O’Donovan, Rossa and Alejandro Sánchez-Becerra","This paper shows how to use a randomized saturation experimental design to identify and estimate causal effects in the presence of spillovers–one person’s treatment may affect another’s outcome–and one-sided non-compliance—subjects can only be offered treatment, not compelled to take it up. Two distinct causal effects are of interest in this setting: direct effects quantify how a person’s own treatment changes her outcome, while indirect effects quantify how her peers’ treatments change her outcome. We consider the case in which spillovers occur within known groups, and take-up decisions are invariant to peers’ realized offers. In this setting we point identify the effects of treatment-on-the-treated, both direct and indirect, in a flexible random coefficients model that allows for heterogeneous treatment effects and endogenous selection into treatment. We go on to propose a feasible estimator that is consistent and asymptotically normal as the number and size of groups increases. We apply our estimator to data from a large-scale job placement services experiment, and find negative indirect treatment effects on the likelihood of employment for those willing to take up the program. These negative spillovers are offset by positive direct treatment effects from own take-up.",http://www.sciencedirect.com/science/article/pii/S0304407623000210
Journal of Econometrics,2023,Instrument strength in IV estimation and inference: A guide to theory and practice,Michael Keane and Timothy Neal,"Two stage least squares (2SLS) has poor properties if instruments are exogenous but weak. But how strong do instruments need to be for 2SLS estimates and test statistics to exhibit acceptable properties? A common standard is that first-stage F≥10. This is adequate to ensure two-tailed t-tests have modest size distortions. But other problems persist: In particular, we show 2SLS standard errors are artificially small in samples where the estimate is most contaminated by the OLS bias. Hence, if the bias is positive, the t-test has little power to detect true negative effects, and inflated power to find positive effects. This phenomenon, which we call a “power asymmetry,” persists even if first-stage F is in the thousands. Robust tests like Anderson–Rubin perform better, and should be used in lieu of the t-test even with strong instruments. We also show how 2SLS test statistics typically suffer from very low power if first-stage F is only 10, leading us to suggest a higher standard of instrument strength in empirical practice.",http://www.sciencedirect.com/science/article/pii/S0304407623000222
Journal of Econometrics,2023,Binary response models for heterogeneous panel data with interactive fixed effects,"Jiti Gao, Fei Liu, Bin Peng and Yayi Yan","In this paper, we investigate binary response models for heterogeneous panel data with interactive fixed effects by allowing both the cross-sectional dimension and the temporal dimension to diverge. From a practical point of view, the proposed framework can be applied to predict the probability of corporate failure, conduct credit rating analysis, etc. Theoretically and methodologically, we build a link between a maximum likelihood estimation and a least squares approach, provide a simple information criterion to detect the number of factors, and establish the corresponding asymptotic theory. In addition, we conduct intensive simulations to examine the theoretical findings. In an empirical study, we focus on the sign prediction of stock returns, and then use the results of sign forecast to conduct portfolio analysis.",http://www.sciencedirect.com/science/article/pii/S0304407623000234
Journal of Econometrics,2023,Uniform inference for value functions,"Sergio Firpo, Antonio Galvao and Thomas Parker","We propose a method to conduct uniform inference for the (optimal) value function, that is, the function that results from optimizing an objective function marginally over one of its arguments. Marginal optimization is not Hadamard differentiable (that is, compactly differentiable) as a map between the spaces of objective and value functions, which is problematic because standard inference methods for nonlinear maps usually rely on Hadamard differentiability. However, we show that the map from objective function to an Lp functional of a value function, for 1≤p≤∞, are Hadamard directionally differentiable. As a result, we establish consistency and weak convergence of nonparametric plug-in estimates of Cramér–von Mises and Kolmogorov–Smirnov test statistics applied to value functions. For practical inference, we develop detailed resampling techniques that combine a bootstrap procedure with estimates of the directional derivatives. In addition, we establish local and uniform size control of one-sided tests which use the resampling procedure. Monte Carlo simulations assess the finite-sample properties of the proposed methods and show accurate empirical size and nontrivial power of the procedures. Finally, we apply our methods to the evaluation of a job training program using bounds for the distribution function of treatment effects.",http://www.sciencedirect.com/science/article/pii/S0304407623000246
Journal of Econometrics,2023,IV methods for Tobit models,"Andrew Chesher, Dongwoo Kim and Adam Rosen","This paper studies models of processes generating censored outcomes with endogenous explanatory variables and instrumental variable restrictions. Tobit-type left censoring at zero is the primary focus in the exposition. Extension to stochastic censoring is sketched. The models do not specify the process determining endogenous explanatory variables and they do not embody restrictions justifying control function approaches. Consequently, they can be partially or point identifying. Identified sets are characterized and it is shown how inference can be performed on scalar functions of partially identified parameters when exogenous variables have rich support. In an application using data on UK household tobacco expenditures inference is conducted on the coefficient of an endogenous total expenditure variable with and without a Gaussian distributional restriction on the unobservable and compared with the results obtained using a point identifying complete triangular model.",http://www.sciencedirect.com/science/article/pii/S0304407623000258
Journal of Econometrics,2023,Debiased machine learning of set-identified linear models,Vira Semenova,"This paper provides estimation and inference methods for an identified set’s boundary (i.e., support function) where the selection among a very large number of covariates is based on modern regularized tools. I characterize the boundary using a semiparametric moment equation. Combining Neyman-orthogonality and sample splitting ideas, I construct a root-N consistent, uniformly asymptotically Gaussian estimator of the boundary and propose a multiplier bootstrap procedure to conduct inference. I apply this result to the Partially Linear Model, the Partially Linear IV Model and the Average Partial Derivative with an interval-valued outcome.",http://www.sciencedirect.com/science/article/pii/S030440762300026X
Journal of Econometrics,2023,Jackknife estimation of a cluster-sample IV regression model with many weak instruments,"John C. Chao, Norman Swanson and Tiemen Woutersen","This paper proposes new jackknife IV estimators that are robust to the effects of many weak instruments and error heteroskedasticity in a cluster sample setting with cluster-specific effects and possibly many included exogenous regressors. The estimators that we propose are designed to properly partial out the cluster-specific effects and included exogenous regressors while preserving the re-centering property of the jackknife methodology. To the best of our knowledge, our proposed procedures provide the first consistent estimators under many weak instrument asymptotics in the setting considered. We also present results on the asymptotic normality of our estimators and show that t-statistics based on said estimators are asymptotically normal under the null and consistent under fixed alternatives. Monte Carlo results show that our t-statistics perform better in controlling size in finite samples than those based on alternative jackknife IV procedures previously introduced in the literature.",http://www.sciencedirect.com/science/article/pii/S0304407623000271
Journal of Econometrics,2023,Spatial autoregressions with an extended parameter space and similarity-based weights,Francesca Rossi and Offer Lieberman,"We provide in this paper asymptotic theory for a spatial autoregressive model (SAR, henceforth) in which the spatial coefficient, λ, is allowed to be less than or equal to unity, as well as consistent with a local to unit root (LUR) model and of the moderate integration (MI) from unity type, and the spatial weights are allowed to be similarity-based and data driven. Other special cases of our setting include the random walk, a model in which all the weights are equal, the standard SAR model in which λ<1 and the similarity based autoregression in which λ=1 and data do not display a natural order. As the norming rates for the asymptotic theory are very different in the λ<1 - compared with the λ=1 and LUR cases, we resort to random norming that treats all cases in a uniform manner. It turns out that standard CLT results prevail in a large class of models in which the infinity norm of the inverse of the weighting structure that characterizes the reduced-form process is Onγ , γ∈[0,1), and is non-standard in the case γ=1. We use a shifted profile likelihood to obtain results which are valid for all cases. A small simulation experiment supports our findings and the usefulness of our model is illustrated with an empirical application of the Boston housing data set in which the estimate of λ appeared to be very close to unity.",http://www.sciencedirect.com/science/article/pii/S0304407623000283
Journal of Econometrics,2023,Wild bootstrap inference for penalized quantile regression for longitudinal data,Carlos Lamarche and Thomas Parker,"The existing theory of penalized quantile regression for longitudinal data has focused primarily on point estimation. In this work, we investigate statistical inference. We propose a wild residual bootstrap procedure and show that it is asymptotically valid for approximating the distribution of the penalized estimator. The model puts no restrictions on individual effects, and the estimator achieves consistency by letting the shrinkage decay in importance asymptotically. The new method is easy to implement and simulation studies show that it has accurate small sample behavior in comparison with existing procedures. Finally, we illustrate the new approach using U.S. Census data to estimate a model that includes more than eighty thousand parameters.",http://www.sciencedirect.com/science/article/pii/S0304407623000313
Journal of Econometrics,2023,Refining set-identification in VARs through independence,Thorsten Drautzburg and Jonathan Wright,"Identification in VARs has traditionally mainly relied on second moments. Some researchers have considered using higher moments as well, but there are concerns about the strength of the identification obtained in this way. In this paper, we propose refining existing identification schemes by augmenting sign restrictions with a requirement that rules out shocks whose higher moments significantly depart from independence. This approach does not assume that higher moments help with identification; it is robust to weak identification. In simulations we show that it controls coverage well, in contrast to approaches that assume that the higher moments deliver point-identification. However, it requires large sample sizes and/or considerable non-normality to reduce the width of confidence intervals by much. We consider some empirical applications. We find that it can reject many possible rotations. The resulting confidence sets for impulse responses may be non-convex, corresponding to disjoint parts of the space of rotation matrices. We show that in this case, augmenting sign and magnitude restrictions with an independence requirement can yield bigger gains.",http://www.sciencedirect.com/science/article/pii/S0304407623000325
Journal of Econometrics,2023,Efficient estimation of average derivatives in NPIV models: Simulation comparisons of neural network estimators,"Jiafeng Chen, Xiaohong Chen and Elie Tamer","Artificial Neural Networks (ANNs) can be viewed as nonlinear sieves that can approximate complex functions of high dimensional variables more effectively than linear sieves. We investigate the performance of various ANNs in nonparametric instrumental variables (NPIV) models of moderately high dimensional covariates that are relevant to empirical economics. We present two efficient procedures for estimation and inference on a weighted average derivative (WAD): an orthogonalized plug-in with optimally-weighted sieve minimum distance (OP-OSMD) procedure and a sieve efficient score (ES) procedure. Both estimators for WAD use ANN sieves to approximate the unknown NPIV function and are n-asymptotically normal and first-order equivalent. We provide a detailed practitioner’s recipe for implementing both efficient procedures. We compare their finite-sample performances in various simulation designs that involve smooth NPIV function of up to 13 continuous covariates, different nonlinearities and covariate correlations. Some Monte Carlo findings include: (1) tuning and optimization are more delicate in ANN estimation; (2) given proper tuning, both ANN estimators with various architectures can perform well; (3) easier to tune ANN OP-OSMD estimators than ANN ES estimators; (4) stable inferences are more difficult to achieve with ANN (than spline) estimators; (5) there are gaps between current implementations and approximation theories. Finally, we apply ANN NPIV to estimate average partial derivatives in two empirical demand examples with multivariate covariates.",http://www.sciencedirect.com/science/article/pii/S0304407623000349
Journal of Econometrics,2023,Shrinkage estimation of multiple threshold factor models,Chenchen Ma and Yundong Tu,"This paper proposes a multiple threshold factor model to enhance the flexibility in modeling the underlying regime switching mechanism for high dimensional time series. The factor loadings are assumed to switch between different regimes according to the value of a threshold variable. A novel estimation procedure is proposed to consistently estimate the multiple thresholds with the aid of sorting operation, principal component analysis and shrinkage estimation, which is practically easy-to-implement and computationally efficient. Furthermore, asymptotic properties for the multiple threshold estimators are established, together with other theoretical results. Monte Carlo simulations demonstrate that the procedure works well in finite samples. The U.S. data sets are analyzed with the proposed model to illustrate the threshold effect of economy policy uncertainty on the financial market.",http://www.sciencedirect.com/science/article/pii/S0304407623000489
Journal of Econometrics,2023,Approximate factor models with weaker loadings,Jushan Bai and Serena Ng,"Pervasive cross-section dependence is increasingly recognized as a characteristic of economic data and the approximate factor model provides a useful framework for analysis. Assuming a strong factor structure where Λ0′Λ0/Nα is positive definite in the limit when α=1, early work established convergence of the principal component estimates of the factors and loadings up to a rotation matrix. This paper shows that the estimates are still consistent and asymptotically normal when α∈(0,1] albeit at slower rates and under additional assumptions on the sample size. The results hold whether α is constant or varies across factor loadings. The framework developed for heterogeneous loadings and the simplified proofs that can be also used in strong factor analysis are of independent interest.",http://www.sciencedirect.com/science/article/pii/S030440762300060X
Journal of Econometrics,2023,Large volatility matrix analysis using global and national factor models,Sung Hoon Choi and Donggyu Kim,"Several large volatility matrix inference procedures have been developed, based on the latent factor model. They often assumed that there are a few of common factors, which can account for volatility dynamics. However, several studies have demonstrated the presence of local factors. In particular, when analyzing the global stock market, we often observe that nation-specific factors explain their own country’s volatility dynamics. To account for this, we propose the Double Principal Orthogonal complEment Thresholding (Double-POET) method, based on multi-level factor models, and also establish its asymptotic properties. Furthermore, we demonstrate the drawback of using the regular principal orthogonal component thresholding (POET) when the local factor structure exists. We also describe the blessing of dimensionality using Double-POET for local covariance matrix estimation. Finally, we investigate the performance of the Double-POET estimator in an out-of-sample portfolio allocation study using international stocks from 20 financial markets.",http://www.sciencedirect.com/science/article/pii/S0304407623000635
Journal of Econometrics,2023,Uniform and Lp convergences for nonparametric continuous time regressions with semiparametric applications,"Ruijun Bu, Jihyun Kim and Bin Wang","We obtain uniform and Lp convergence rates of kernel type nonparametric estimators for the instantaneous conditional mean and variance functions of continuous time regressions, where the regressor is assumed to be a general recurrent diffusion. Our asymptotics are developed under a general set-up, with a shrinking sampling interval and an increasing time span, and without the stationarity assumption. Based on our convergence results, we develop a semiparametric inferential procedure for continuous time predictive regressions. In particular, a robust semiparametric likelihood ratio test for linear predictability is proposed, with its limit distribution established. We also apply our convergence results to obtain the asymptotics of a semiparametric maximum likelihood estimator of the drift of recurrent diffusions. In our simulation study, we examine the finite sample performance of our robust test against several existing tests in the literature. An empirical illustration is presented to test the predictability of the excess returns of two major stock indices using the commonly used dividend–price ratio and earnings–price ratio as the predictor.",http://www.sciencedirect.com/science/article/pii/S0304407623000726
Journal of Econometrics,2023,Identifying latent group structures in spatial dynamic panels,"Liangjun Su, Wuyi Wang and Xingbai Xu","This paper considers the identification of latent group structures in spatial dynamic panels. We follow Lee and Yu (2010) and consider a rich spatial dynamic panel data (SDPD) model with two-way fixed effects. In addition, we also allow for latent panel structures where individuals can be classified into a few groups such that individuals within the same group share the common slope parameters and do not otherwise. Both the number of groups and the individuals’ group membership are unknown. To identify the latent group structures, we first adopt the GMM to obtain the preliminary unconstrained estimates of the slope coefficients. Then we apply the sequential binary segmentation algorithm (SBSA) of Wang and Su (2021) to these estimates and obtain the clusters. A BIC-type information criterion is proposed to choose the number of latent groups consistently. The asymptotic analysis shows that this method can identify the true group structure consistently, and the post-classification estimators enjoy the oracle property. Monte Carlo simulations demonstrate that our method has good finite sample performance. Finally, we apply our approach to the US housing market and identify two latent Metropolitan Statistical Areas groups.",http://www.sciencedirect.com/science/article/pii/S0304407623000738
Journal of Econometrics,2023,One-way or two-way factor model for matrix sequences?,"Yong He, Xinbing Kong, Lorenzo Trapani and Long Yu","This paper investigates the issue of determining the dimensions of row and column factor spaces in matrix-valued data. Exploiting the eigen-gap in the spectrum of sample second moment matrices of the data, we propose a family of randomised tests to check whether a one-way or two-way factor structure exists or not. Our tests do not require any arbitrary thresholding on the eigenvalues, and can be applied with (virtually) no restrictions on the relative rate of divergence of the cross-sections to the sample sizes as they pass to infinity. Although tests are based on a randomisation which does not vanish asymptotically, we propose a de-randomised, “strong” (based on the Law of the Iterated Logarithm) decision rule to choose in favour or against the presence of common factors. We use the proposed tests and decision rule in two ways. We further cast our individual tests in a sequential procedure whose output is an estimate of the number of common factors. Our tests are built on two variants of the sample second moment matrix of the data: one based on a row (or column) “flattened” version of the matrix-valued sequence, and one based on a projection-based method. Our simulations show that both procedures work well in large samples and, in small samples, the one based on the projection method delivers a superior performance compared to existing methods in virtually all cases considered.",http://www.sciencedirect.com/science/article/pii/S030440762300074X
Journal of Econometrics,2023,"Wald, QLR, and score tests when parameters are subject to linear inequality constraints",Yanqin Fan and Xuetao Shi,"This paper develops Wald-type, QLR, and score-type tests for linear equality constraints in a general class of extremum estimation problems where the parameter space is characterized by a finite number of linear equality and inequality constraints. We show that the asymptotic null distributions of the Wald and QLR statistics are discontinuous in an implicit nuisance parameter and propose an algorithm to identify it. In contrast, the asymptotic null distribution of the score statistic is not discontinuous in any model parameter but depends on a polytope projection. We present an algorithm based on the Fourier–Motzkin elimination to compute such a projection. We study the consistency and local power properties of the three tests. Finally, we present numerical results of our tests’ finite sample performance from a Monte Carlo study and conduct an empirical illustration of a Mincer earnings regression.",http://www.sciencedirect.com/science/article/pii/S0304407623000787
Journal of Econometrics,2023,Testing for the appropriate level of clustering in linear regression models,"James MacKinnon, Morten Nielsen and Matthew Webb","The overwhelming majority of empirical research that uses cluster-robust inference assumes that the clustering structure is known, even though there are often several possible ways in which a dataset could be clustered. We propose two tests for the correct level of clustering in regression models. One test focuses on inference about a single coefficient, and the other on inference about two or more coefficients. We provide both asymptotic and wild bootstrap implementations. The proposed tests work for a null hypothesis of either no clustering or “fine” clustering against alternatives of “coarser” clustering. We also propose a sequential testing procedure to determine the appropriate level of clustering. Simulations suggest that the bootstrap tests perform very well under the null hypothesis and can have excellent power. An empirical example suggests that using the tests leads to sensible inferences.",http://www.sciencedirect.com/science/article/pii/S0304407623001021
Journal of Econometrics,2023,Social threshold regression,"Antri Konstantinidi, Andros Kourtellos and Yiguo Sun","This paper develops a threshold social interaction model which introduces group-specific endogenous effects as well as contextual effects into a conventional spatial Durbin model. We propose a two-step GMM estimator for the threshold and regression parameters, derive asymptotic theory, and provide bootstrap inference. Finally, we assess the performance of our methods using a Monte Carlo simulation and provide an empirical application on the role of peer effects on student academic achievement using Add Health data.",http://www.sciencedirect.com/science/article/pii/S0304407623001045
Journal of Econometrics,2023,Stochastic properties of nonlinear locally-nonstationary filters,Francisco Blasques and Marc Nientker,"This article delivers conditions for the existence of a unique stationary, ergodic and φ-mixing invertible solution for nonlinear time-varying parameter models that are locally non-stationary or explosive. The assumptions are different from existing conditions as they do not impose Lipschitz, bounded growth or drift restrictions. Instead we require that the time series contains resetting dynamics. These dynamics are present in time series with sudden changes, such as stock prices with financial bubbles. We explain how our results can be applied to multiple models with time-varying conditional mean and time-varying conditional variance. Additionally, we show the empirical relevance of locally non-stationary models by developing a robust score-driven locally-nonstationary location model that allows us to identify price bubbles. In particular, we revisit the early warning system for Alert of Price Spikes (ALPS) developed by the World Food Programme (WFP, 2014). Our model is able to correctly identify price bubbles both in-sample and out-of-sample. We show that this information can be crucially relevant for the Global Information Early Warning System of the Food and Agriculture Organization (GIEWS) of the United Nations which identifies market bubbles in prices of grains and cereals in order to manage and prevent impending food crises in developing countries.",http://www.sciencedirect.com/science/article/pii/S0304407623001264
Journal of Econometrics,2023,Inference on individual treatment effects in nonseparable triangular models,"Jun Ma, Vadim Marmer and Zhengfei Yu","In nonseparable triangular models with a binary endogenous treatment and a binary instrumental variable, Vuong and Xu (2017) established identification results for individual treatment effects (ITEs) under the rank invariance assumption. Using their approach, Feng et al. (2019) proposed a uniformly consistent kernel estimator for the density of the ITE that utilizes estimated ITEs. In this paper, we establish the asymptotic normality of the density estimator of Feng et al. (2019) and show that the ITE estimation errors have a non-negligible effect on the asymptotic distribution of the estimator. We propose asymptotically valid standard errors that account for ITEs estimation, as well as a bias correction. Furthermore, we develop uniform confidence bands for the density of the ITE using the jackknife multiplier or nonparametric bootstrap critical values.",http://www.sciencedirect.com/science/article/pii/S0304407623001276
Journal of Econometrics,2023,The spread of COVID-19 in London: Network effects and optimal lockdowns,"Christian Julliard, Ran Shi and Kathy Yuan","We generalise a stochastic version of the workhorse SIR (Susceptible-Infectious-Removed) epidemiological model to account for spatial dynamics generated by network interactions. Using the London metropolitan area as a salient case study, we show that commuter network externalities account for about 42% of the propagation of COVID-19. We find that the UK lockdown measure reduced total propagation by 44%, with more than one third of the effect coming from the reduction in network externalities. Counterfactual analyses suggest that: (i) the lockdown was somehow late, but further delay would have had more extreme consequences; (ii) a targeted lockdown of a small number of highly connected geographic regions would have been equally effective, arguably with significantly lower economic costs; (iii) targeted lockdowns based on threshold number of cases are not effective, since they fail to account for network externalities.",http://www.sciencedirect.com/science/article/pii/S0304407623001288
Journal of Econometrics,2023,Efficient peer effects estimators with group effects,"Guido Kuersteiner, Ingmar Prucha and Ying Zeng","We study linear peer effect models where peers interact in groups and individual’s outcomes are linear in the group mean outcome and characteristics. We allow for unobserved random group effects as well as observed fixed group effects. The specification is in part motivated by the moment conditions imposed in Graham (2008). We show that these moment conditions can be cast in terms of a linear random group effects model and that they lead to a class of GMM estimators with parameters generally identified as long as there is sufficient variation in group size or group types. We also show that our class of GMM estimators contains a Quasi Maximum Likelihood estimator (QMLE) for the random group effects model, as well as the Wald estimator of Graham (2008) and the within estimator of Lee (2007) as special cases. Our identification results extend insights in Graham (2008) that show how assumptions about random group effects, variation in group size and certain forms of heteroscedasticity can be used to overcome the reflection problem in identifying peer effects. Our QMLE and GMM estimators accommodate additional covariates and are valid in situations with a large but finite number of different group sizes or types. Because our estimators are general moment based procedures, using instruments other than binary group indicators in estimation is straight forward. Our QMLE estimator accommodates group level covariates in the spirit of Mundlak and Chamberlain and offers an alternative to fixed effects specifications. This model feature significantly extends the applicability of Graham’s identification strategy to situations where group assignment may not be random but correlation of group level effects with peer effects can be controlled for with observable group level characteristics. Monte-Carlo simulations show that the bias of the QMLE estimator decreases with the number of groups and the variation in group size, and increases with group size. We also prove the consistency and asymptotic normality of the estimator under reasonable assumptions.",http://www.sciencedirect.com/science/article/pii/S030440762300129X
Journal of Econometrics,2023,Sparse quantile regression,Le-Yu Chen and Sokbae (Simon) Lee,"We consider both ℓ0-penalized and ℓ0-constrained quantile regression estimators. For the ℓ0-penalized estimator, we derive an exponential inequality on the tail probability of excess quantile prediction risk and apply it to obtain non-asymptotic upper bounds on the mean-square parameter and regression function estimation errors. We also derive analogous results for the ℓ0-constrained estimator. The resulting rates of convergence are nearly minimax-optimal and the same as those for ℓ1-penalized and non-convex penalized estimators. Further, we characterize expected Hamming loss for the ℓ0-penalized estimator. We implement the proposed procedure via mixed integer linear programming and also a more scalable first-order approximation algorithm. We illustrate the finite-sample performance of our approach in Monte Carlo experiments and its usefulness in a real data application concerning conformal prediction of infant birth weights (with n≈103 and up to p>103). In sum, our ℓ0-based method produces a much sparser estimator than the ℓ1-penalized and non-convex penalized approaches without compromising precision.",http://www.sciencedirect.com/science/article/pii/S0304407623001306
Journal of Econometrics,2023,What’s trending in difference-in-differences? A synthesis of the recent econometrics literature,"Jonathan Roth, Sant’Anna, Pedro H.C., Alyssa Bilinski and John Poe","This paper synthesizes recent advances in the econometrics of difference-in-differences (DiD) and provides concrete recommendations for practitioners. We begin by articulating a simple set of “canonical” assumptions under which the econometrics of DiD are well-understood. We then argue that recent advances in DiD methods can be broadly classified as relaxing some components of the canonical DiD setup, with a focus on (i) multiple periods and variation in treatment timing, (ii) potential violations of parallel trends, or (iii) alternative frameworks for inference. Our discussion highlights the different ways that the DiD literature has advanced beyond the canonical model, and helps to clarify when each of the papers will be relevant for empirical work. We conclude by discussing some promising areas for future research.",http://www.sciencedirect.com/science/article/pii/S0304407623001318
Journal of Econometrics,2023,Semi-nonparametric estimation of random coefficients logit model for aggregate demand,"Zhentong Lu, Xiaoxia Shi and Jing Tao","In this paper, we propose a two-step semi-nonparametric estimator for the widely used random coefficients logit demand model. The approach applies to the same setup as Berry et al. (1995, BLP)-type of models with many products, but has the advantage of not requiring computing demand inversion. In particular, the first step of our approach estimates the fixed coefficients via a computationally very easy linear sieve generalized method of moments (GMM). The second step uncovers the distribution of the random coefficient via a sieve minimum distance or GMM procedure. We show identification and derive the asymptotic properties of the estimator in a large market environment. Monte Carlo simulations and empirical illustrations support the theoretical results and demonstrate the usefulness of our estimator in practice.",http://www.sciencedirect.com/science/article/pii/S0304407623001458
Journal of Econometrics,2023,Tail index estimation in the presence of covariates: Stock returns’ tail risk dynamics,"João Nicolau, Paulo Rodrigues and Marian Z. Stoykov","This paper provides novel theoretical results for the estimation of the conditional tail index of Pareto and Pareto-type distributions in a time series context. We show that both the estimators and relevant test statistics are normally distributed in the limit, when independent and identically distributed or dependent data are considered. Simulation results provide support for the theoretical findings and highlight the good finite sample properties of the approach in a time series context. The proposed methodology is then used to analyse stock returns’ tail risk dynamics. Two empirical applications are provided. The first consists in testing whether the time-varying tail exponents across firms follow Kelly and Jiang’s (2014) assumption of common firm level tail dynamics. The results obtained from our sample seem not to favour this hypothesis. The second application, consists of the evaluation of the impact of two market risk indicators, VIX and Expected Shortfall (ES) and two firm specific covariates, capitalization and market-to-book on stocks tail risk dynamics. Although all variables seem important drivers of firms’ tail risk dynamics, it is found that ES and firms’ capitalization seem to have overall wider impact.",http://www.sciencedirect.com/science/article/pii/S030440762300146X
Journal of Econometrics,2023,Reproducibility and transparency versus privacy and confidentiality: Reflections from a data editor,Lars Vilhuber,"Transparency and reproducibility are often seen in opposition to privacy and confidentiality. Data that need to be kept confidential are seen as an impediment to reproducibility, and privacy would seem to inhibit transparency. I bring a more nuanced view to the discussion, and show, using examples from over 1,000 reproducibility assessments, that confidential data can very well be used in reproducible and transparent research. The key insight is that access to most confidential data, while tedious, is open to hundreds if not thousands of researchers. In cases where few researchers can consider accessing such data in the future, reproducibility services, such as those provided by some journals, can provide some evidence for effective reproducibility even when the same data may not be available for future research.",http://www.sciencedirect.com/science/article/pii/S0304407623001471
Journal of Econometrics,2023,Identification-robust nonparametric inference in a linear IV model,Bertille Antoine and Pascal Lavergne,"For a linear IV regression, we propose two new inference procedures on parameters of endogenous variables that are robust to any identification pattern, do not rely on a linear first-stage equation, and account for heteroskedasticity of unknown form. Building on Bierens (1982), we first propose an Integrated Conditional Moment (ICM) type statistic constructed by setting the parameters to the value under the null hypothesis. The ICM procedure tests at the same time the value of the coefficient and the specification of the model. We then adopt a conditionality principle to condition on a set of ICM statistics that informs on identification strength. Our two procedures uniformly control size irrespective of identification strength. They are powerful irrespective of the nonlinear form of the link between instruments and endogenous variables and are competitive with existing procedures in simulations and application.",http://www.sciencedirect.com/science/article/pii/S030440762200046X
Journal of Econometrics,2023,Over-identified Doubly Robust identification and estimation,"Arthur Lewbel, Jin Young Choi and Zhuzhu Zhou","Consider two parametric models. At least one is correctly specified, but we do not know which. Both models include a common vector of parameters. An estimator for this common parameter vector is called Doubly Robust (DR) if it is consistent no matter which model is correct. We provide a general technique for constructing DR estimators (assuming the models are over identified). Our Over-identified Doubly Robust (ODR) technique is a simple extension of the Generalized Method of Moments. We illustrate our ODR with a variety of models. Our empirical application is instrumental variables estimation, where either one of two instrument vectors might be invalid.",http://www.sciencedirect.com/science/article/pii/S0304407622000434
Journal of Econometrics,2023,Modeling realized covariance measures with heterogeneous liquidity: A generalized matrix-variate Wishart state-space model,Bastian Gribisch and Jan Patrick Hartkopf,"We propose to generalize the Wishart state-space model for realized covariance matrices of asset returns in order to capture complex measurement error structures induced by modern robust and data efficient realized covariance estimators and heterogeneous liquidity across assets. Our model assumes that the latent covariance matrix of the assets is observed through their realized covariance matrix with a Riesz measurement density, which generalizes the Wishart to monotone missing data. The Riesz alleviates the Wishart-implied attenuation of measurement errors and translates into a convenient likelihood factorization which facilitates inference using simple Bayesian MCMC procedures. The state-space approach allows for a flexible description of the covariance dynamics implied by the data and an empirical application shows that the model performs very well in- and out-of-sample.",http://www.sciencedirect.com/science/article/pii/S0304407622000392
Journal of Econometrics,2023,A higher-order correct fast moving-average bootstrap for dependent data,"Davide La Vecchia, Alban Moor and Olivier Scaillet","We develop the theory of a novel fast bootstrap for dependent data. Our scheme deploys i.i.d. resampling of smoothed moment indicators. We characterize the class of parametric and semiparametric estimation problems for which the method is valid. We show the asymptotic refinements of the new procedure, proving that it is higher-order correct under mild assumptions on the time series, the estimating functions, and the smoothing kernel. We illustrate the applicability and the advantages of our procedure for M-estimation, generalized method of moments, and generalized empirical likelihood estimation. In a Monte Carlo study, we consider an autoregressive conditional duration model and we compare our method with other extant, routinely-applied first- and higher-order correct methods. The results provide numerical evidence that the novel bootstrap yields higher-order accurate confidence intervals, while remaining computationally lighter than its higher-order correct competitors. A real-data example on dynamics of trading volume of US stocks illustrates the empirical relevance of our method.",http://www.sciencedirect.com/science/article/pii/S0304407622000422
Journal of Econometrics,2023,On the power of the conditional likelihood ratio and related tests for weak-instrument robust inference,Nicolas Van de Sijpe and Frank Windmeijer,"Power curves of the Conditional Likelihood Ratio (CLR) and related tests for testing H0:β=β0 in linear models with a single endogenous variable, y=xβ+u, estimated using potentially weak instrumental variables have been presented for two different designs. One design keeps the variance matrix of the structural and first-stage errors, Σ, constant, the other instead keeps the variance matrix of the reduced-form and first-stage errors, Ω, constant. The values of Σ govern the endogeneity features of the model. The fixed-Ω design changes these endogeneity features with changing values of β in a way that makes it less suitable for an analysis of the behaviour of the tests in low to moderate endogeneity settings, or when β and the correlation of the structural and first-stage errors, ρuv, have the same sign. At larger values of |β|, the fixed-Ω design implicitly selects values for Σ where the power of the CLR test is high. We further show that the Likelihood Ratio statistic is identical to the t0(βˆL)2 statistic as proposed by Mills et al. (2014), where βˆL is the Liml estimator. In fixed-Σ design Monte Carlo simulations, we find that Liml- and Fuller-based conditional Wald tests and the Fuller-based conditional t02 test are more powerful than the CLR test when the degree of endogeneity is low to moderate. The conditional Wald tests are further the most powerful of these tests when β and ρuv have the same sign. We show that in the fixed-Ω design, setting β0=0 and the diagonal elements of Ω equal to 1 is not without loss of generality, unlike in the fixed-Σ design.",http://www.sciencedirect.com/science/article/pii/S0304407622000367
Journal of Econometrics,2023,A corrected Clarke test for model selection and beyond,"Florian Brück, Jean-David Fermanian and Aleksey Min","We introduce a large family of model selection tests based on the expectation of an arbitrary, possibly non-smooth, parametric criterion function of the data. The considered methodology is illustrated for several econometric problems, including linear and quantile regression. It covers the case of strictly locally non-nested models and some overlapping models. The asymptotic theory of the proposed test statistic is stated. A general exchangeable bootstrap scheme allows the evaluation of its limiting law as well as its asymptotic variance. Our framework includes the tests for non-nested model selection of Vuong (1989) and Clarke (2007) as particular cases. We show that the statistic of the latter test is not Binomial distributed as originally stated and we provide its corrected limiting law. In a simulation study, we empirically verify the distributional approximation of our test statistic in a finite sample and examine the empirical level and power of the corresponding model selection tests in various settings. Finally, an analysis of a financial dataset illustrates the proposed model selection procedure at work.",http://www.sciencedirect.com/science/article/pii/S0304407622000446
Journal of Econometrics,2023,Bootstrap inference for Hawkes and general point processes,"Giuseppe Cavaliere, Ye Lu, Anders Rahbek and Jacob Stærk-Østergaard","Inference and testing in general point process models such as the Hawkes model is predominantly based on asymptotic approximations for likelihood-based estimators and tests. As an alternative, and to improve finite sample performance, this paper considers bootstrap-based inference for interval estimation and testing. Specifically, for a wide class of point process models we consider a novel bootstrap scheme labeled ‘fixed intensity bootstrap’ (FIB), where the conditional intensity is kept fixed across bootstrap repetitions. The FIB, which is very simple to implement and fast in practice, extends previous ideas from the bootstrap literature on time series in discrete time, where the so-called ‘fixed design’ and ‘fixed volatility’ bootstrap schemes have shown to be particularly useful and effective. We compare the FIB with the classic recursive bootstrap, which is here labeled ‘recursive intensity bootstrap’ (RIB). In RIB algorithms, the intensity is stochastic in the bootstrap world and implementation of the bootstrap is more involved, due to its sequential structure. For both bootstrap schemes, we provide new bootstrap (asymptotic) theory which allows to assess bootstrap validity, and propose a ‘non-parametric’ approach based on resampling time-changed transformations of the original waiting times. We also establish the link between the proposed bootstraps for point process models and the related autoregressive conditional duration (ACD) models. Lastly, we show effectiveness of the different bootstrap schemes in finite samples through a set of detailed Monte Carlo experiments, and provide applications to both financial data and social media data to illustrate the proposed methodology.",http://www.sciencedirect.com/science/article/pii/S0304407622000574
Journal of Econometrics,2023,Statistical inference for linear mediation models with high-dimensional mediators and application to studying stock reaction to COVID-19 pandemic,"Xu Guo, Runze Li, Jingyuan Liu and Mudong Zeng","Mediation analysis draws increasing attention in many research areas such as economics, finance and social sciences. In this paper, we propose new statistical inference procedures for high dimensional mediation models, in which both the outcome model and the mediator model are linear with high dimensional mediators. Traditional procedures for mediation analysis cannot be used to make statistical inference for high dimensional linear mediation models due to high-dimensionality of the mediators. We propose an estimation procedure for the indirect effects of the models via a partially penalized least squares method, and further establish its theoretical properties. We further develop a partially penalized Wald test on the indirect effects, and prove that the proposed test has a χ2 limiting null distribution. We also propose an F-type test for direct effects and show that the proposed test asymptotically follows a χ2-distribution under null hypothesis and a noncentral χ2-distribution under local alternatives. Monte Carlo simulations are conducted to examine the finite sample performance of the proposed tests and compare their performance with existing ones. We further apply the newly proposed statistical inference procedures to study stock reaction to COVID-19 pandemic via an empirical analysis of studying the mediation effects of financial metrics that bridge company’s sector and stock return.",http://www.sciencedirect.com/science/article/pii/S0304407622000598
Journal of Econometrics,2023,Time series estimation of the dynamic effects of disaster-type shocks,Richard Davis and Serena Ng,"This paper provides three results for SVARs under the assumption that the primitive shocks are mutually independent. First, a framework is proposed to accommodate a disaster-type variable with infinite variance into a SVAR. We show that the least squares estimates of the SVAR are consistent but have non-standard asymptotics. Second, the disaster shock is identified as the component with the largest kurtosis. An estimator that is robust to infinite variance is used to recover the mutually independent components. Third, an independence test on the residuals pre-whitened by the Choleski decomposition is proposed to test the restrictions imposed on a SVAR. The test can be applied whether the data have fat or thin tails, and to over as well as exactly identified models. Three applications are considered. In the first, the independence test is used to shed light on the conflicting evidence regarding the role of uncertainty in economic fluctuations. In the second, disaster shocks are shown to have short term economic impact arising mostly from feedback dynamics. The third uses the framework to study the dynamic effects of economic shocks post-covid.",http://www.sciencedirect.com/science/article/pii/S0304407622000665
Journal of Econometrics,2023,Asymptotic properties of Bayesian inference in linear regression with a structural break,Kenichi Shimizu,"This paper studies large sample properties of a Bayesian approach to inference about slope parameters γ in linear regression models with a structural break. In contrast to the conventional approach to inference about γ that does not take into account the uncertainty of the unknown break date, the Bayesian approach that we consider incorporates such uncertainty. Our main theoretical contribution is a Bernstein–von Mises type theorem (Bayesian asymptotic normality) for γ under a wide class of priors, which essentially indicates an asymptotic equivalence between the conventional frequentist and Bayesian inference. Consequently, a frequentist researcher could look at credible intervals of γ to check robustness with respect to the uncertainty of the break date. Simulation studies show that the conventional confidence intervals of γ tend to undercover in finite samples whereas the credible intervals offer more reasonable coverages in general. As the sample size increases, the two methods coincide, as predicted from our theoretical conclusion. Using data from Paye and Timmermann (2006) on stock return prediction, we illustrate that the traditional confidence intervals on γ might underrepresent the true sampling uncertainty.",http://www.sciencedirect.com/science/article/pii/S030440762200077X
Journal of Econometrics,2023,A condition for the identification of multivariate models with binary instruments,Florian F. Gunsilius,"This article introduces an empirical condition for the nonparametric point-identification of multivariate instrumental variable models with continuous endogenous variables using binary instruments. Verifying this condition can confirm point-identification in settings in which traditional approaches are not applicable. In particular, it shows that nonlinear instrumental variable models with general heterogeneity can be point-identified with only a binary instrument. This generalizes existing identification results which either restrict the unobserved heterogeneity substantially or require the instrument to have a large support. The main assumption on the instrumental variable model is cyclic monotonicity of its first stage, a multivariate generalization of the classical rank-invariance assumption for univariate models. Asymptotic convergence results for the empirical observable distributions are derived that allow to check the condition in practice. The identification rests on a fixed-set convergence result of cyclically monotone maps between quasi-concave functions.",http://www.sciencedirect.com/science/article/pii/S0304407622000872
Journal of Econometrics,2023,Bootstrap analysis of mutual fund performance,"Haitao Huang, Lei Jiang, Xuan Leng and Liang Peng",We study bootstrap methods for fund performance evaluation. We first show that two prominent bootstrap tests have biased test sizes in a large cross-section with short time series and lack test power to detect skilled funds when a substantial number of unskilled funds are present. We then develop the theory for a valid bootstrap Hotelling’s T-squared test for zero alpha. We apply the proposed bootstrap test in a practical two-step procedure to identify skilled funds. Our empirical analysis finds that skilled funds are more engaged in active management and hold stocks with higher expected anomalous returns.,http://www.sciencedirect.com/science/article/pii/S0304407622000951
Journal of Econometrics,2023,The effects of training incidence and planned training duration on labor market transitions,"Bernd Fitzenberger, Aderonke Osikominu and Marie Paul","This paper uses a dynamic model for employment and training transitions in discrete time to estimate average employment effects of training incidence and planned training duration for the treated. We account for the dynamics of program start and continuation while treatment and outcome transitions are linked through correlated individual-specific effects. Identification relies on a consistency condition, conditional sequential randomization, time-varying covariates, and exclusion restrictions. We implement our framework applying MCMC methods to rich administrative data for a large training program in Germany. In addition to finding a positive training effect on employment between 6 and 12 percentage points 2.5 years after program start, we demonstrate that a longer planned training duration typically results in higher medium- to long-term employment gains.",http://www.sciencedirect.com/science/article/pii/S0304407622000690
Journal of Econometrics,2023,Model averaging prediction by K-fold cross-validation,Xinyu Zhang and Chu-An Liu,"This paper considers the model averaging prediction in a quasi-likelihood framework that allows for parameter uncertainty and model misspecification. We propose an averaging prediction that selects the data-driven weights by minimizing a K-fold cross-validation. We provide two theoretical justifications for the proposed method. First, when all candidate models are misspecified, we show that the proposed averaging prediction using K-fold cross-validation weights is asymptotically optimal in the sense of achieving the lowest possible prediction risk. Second, when the model set includes correctly specified models, we demonstrate that the proposed K-fold cross-validation asymptotically assigns all weights to the correctly specified models. Monte Carlo simulations show that the proposed averaging prediction achieves lower empirical risk than other existing model averaging methods. As an empirical illustration, the proposed method is applied to credit card default prediction.",http://www.sciencedirect.com/science/article/pii/S0304407622000975
Journal of Econometrics,2023,Logical differencing in dyadic network formation models with nontransferable utilities,"Wayne Gao, Ming Li and Sheng Xu","This paper considers a semiparametric model of dyadic network formation under nontransferable utilities (NTU). Such dyadic links arise frequently in real-world social interactions that require bilateral consent but by their nature induce additive non-separability. In our model we show how unobserved individual heterogeneity in the network formation model can be canceled out without requiring additive separability. The approach uses a new method we call logical differencing. The key idea is to construct an observable event involving the intersection of two mutually exclusive restrictions—derived based on weak multivariate monotonicity—on the fixed effects. Based on this identification strategy we provide consistent estimators of the network formation model under NTU. Finite-sample performance of our method is analyzed in a simulation study, and an empirical illustration using the risk-sharing network data from Nyakatoke demonstrates that our proposed method is able to obtain economically intuitive estimates.",http://www.sciencedirect.com/science/article/pii/S0304407622000884
Journal of Econometrics,2023,Reprint of: Formulation and estimation of stochastic frontier production function models,"Dennis Aigner, C. Lovell and Peter Schmidt","Previous studies of the so-called frontier production function have not utilized an adequate characterization of the disturbance term for such a model. In this paper we provide an appropriate specification, by defining the disturbance term as the sum of symmetric normal and (negative) half-normal random variables. Various aspects of maximum-likelihood estimation for the coefficients of a production function with an additive disturbance term of this sort are then considered.",http://www.sciencedirect.com/science/article/pii/S0304407623000465
Journal of Econometrics,2023,Reprint of: Generalized Autoregressive Conditional Heteroskedasticity,Tim Bollerslev,A natural generalization of the ARCH (Autoregressive Conditional Heteroskedastic) process introduced in Engle (1982) to allow for past conditional variances in the current conditional variance equation is proposed. Stationarity conditions and autocorrelation structure for this new class of parametric models are derived. Maximum likelihood estimation and testing are also considered. Finally an empirical example relating to the uncertainty of the inflation rate is presented.,http://www.sciencedirect.com/science/article/pii/S0304407623000477
Journal of Econometrics,2023,Reprint of: Initial conditions and moment restrictions in dynamic panel data models,Richard Blundell and Stephen Bond,"Estimation of the dynamic error components model is considered using two alternative linear estimators that are designed to improve the properties of the standard first-differenced GMM estimator. Both estimators require restrictions on the initial conditions process. Asymptotic efficiency comparisons and Monte Carlo simulations for the simple AR(1) model demonstrate the dramatic improvement in performance of the proposed estimators compared to the usual first-differenced GMM estimator, and compared to non-linear GMM. The importance of these results is illustrated in an application to the estimation of a labour demand model using company panel data.",http://www.sciencedirect.com/science/article/pii/S0304407623000751
Journal of Econometrics,2023,Reprint of: Testing for unit roots in heterogeneous panels,"Kyung So Im, Mohammad Pesaran and Yongcheol Shin","This paper proposes unit root tests for dynamic heterogeneous panels based on the mean of individual unit root statistics. In particular it proposes a standardized t-bar test statistic based on the (augmented) Dickey–Fuller statistics averaged across the groups. Under a general setting this statistic is shown to converge in probability to a standard normal variate sequentially with T (the time series dimension) →∞, followed by N (the cross sectional dimension) →∞. A diagonal convergence result with T and N→∞ while N/T→k,k being a finite non-negative constant, is also conjectured. In the special case where errors in individual Dickey–Fuller (DF) regressions are serially uncorrelated a modified version of the standardized t-bar statistic is shown to be distributed as standard normal as N→∞ for a fixed T, so long as T ¿ 5 in the case of DF regressions with intercepts and T ¿ 6 in the case of DF regressions with intercepts and linear time trends. An exact fixed N and T test is also developed using the simple average of the DF statistics. Monte Carlo results show that if a large enough lag order is selected for the underlying ADF regressions, then the small sample performances of the t-bar test is reasonably satisfactory and generally better than the test proposed by Levin and Lin (1993).",http://www.sciencedirect.com/science/article/pii/S0304407623000763
Journal of Econometrics,2023,Reprint of: On the network topology of variance decompositions: Measuring the connectedness of financial firms,Francis Diebold and Kamil Yilmaz,"We propose several connectedness measures built from pieces of variance decompositions, and we argue that they provide natural and insightful measures of connectedness. We also show that variance decompositions define weighted, directed networks, so that our connectedness measures are intimately related to key measures of connectedness used in the network literature. Building on these insights, we track daily time-varying connectedness of major U.S. financial institutions’ stock return volatilities in recent years, with emphasis on the financial crisis of 2007–2008.",http://www.sciencedirect.com/science/article/pii/S0304407623000775
Journal of Econometrics,2023,Isotonic regression discontinuity designs,Andrii Babii and Rohit Kumar,"This paper studies the estimation and inference for the isotonic regression at the boundary point, an object that is particularly interesting and required in the analysis of monotone regression discontinuity designs. We show that the isotonic regression is inconsistent in this setting and derive the asymptotic distributions of boundary corrected estimators. Interestingly, the boundary corrected estimators can be bootstrapped without subsampling or additional nonparametric smoothing which is not the case for the interior point. The Monte Carlo experiments indicate that shape restrictions can improve dramatically the finite-sample performance of unrestricted estimators. Lastly, we estimate the causal effect of incumbency in U.S. House elections via the isotonic regression discontinuity design.",http://www.sciencedirect.com/science/article/pii/S0304407621000506
Journal of Econometrics,2023,Estimation and inference for policy relevant treatment effects,Yuya Sasaki and Takuya Ura,"The policy relevant treatment effect (PRTE) measures the average effect of switching from a status-quo policy to a counterfactual policy under consideration. Estimation of the PRTE involves estimation of multiple preliminary parameters, including propensity scores, conditional expectation functions of the outcome and covariates given the propensity score, and marginal treatment effects. These preliminary estimators can affect the asymptotic distribution of the PRTE estimator in complicated and intractable manners. In this light, we propose an orthogonal score for double debiased estimation of the PRTE, whereby the asymptotic distribution of the PRTE estimator is obtained without any influence of preliminary parameter estimators as far as they satisfy mild requirements of convergence rates. To our knowledge, this paper is the first to develop limit distribution theories for inference about the PRTE.",http://www.sciencedirect.com/science/article/pii/S0304407621001494
Journal of Econometrics,2023,Estimation of treatment effects under endogenous heteroskedasticity,Jason Abrevaya and Haiqing Xu,"This paper considers a treatment effect model in which individual treatment effect may be heterogeneous, even among observationally identical individuals. Specifically, by extending the classical instrumental-variables (IV) model with an endogenous binary treatment, the heteroskedasticity of the error disturbance is allowed to vary with the treatment variable so that the treatment generates both mean and variance effect on the outcome. In this endogenous heteroskedasticity IV (EHIV) model, the standard IV estimator can be inconsistent for the average treatment effect (ATE) and lead to incorrect inference. After nonparametric identification is established, closed-form estimators are provided under the linear EHIV specification for the mean and variance treatment effect, as well as the average treatment effect on the treated (ATT). Asymptotic properties of the estimators are derived. We use Monte Carlo experiments to investigate the performance of the proposed approach and then consider an empirical application regarding the effect of fertility on female labor supply. Our findings demonstrate the importance of accounting for endogenous heteroskedasticity.",http://www.sciencedirect.com/science/article/pii/S0304407621001500
Journal of Econometrics,2023,Identifying treatment effects in the presence of confounded types,Desire Kedagni,"In this paper, I consider identification of treatment effects when the treatment is endogenous. The use of instrumental variables is a popular solution to deal with endogeneity, but this may give misleading answers when the instrument is invalid. I show that when an (unobserved) instrument is invalid due to correlation with the first stage unobserved heterogeneity, a proxy for the instrument helps partially identify not only the local average treatment effect, but also the entire potential outcomes distributions for compliers. I exploit the fact that the distribution of the observed outcome in each group defined by the treatment and the instrument is a mixture of the distributions of interest. I write the identified set in the form of conditional moment inequalities, and provide an easily implementable inference procedure. Under some tail restrictions, the potential outcomes distributions are point-identified for compliers. Finally, I illustrate my methodology on data from the National Longitudinal Survey of Young Men to estimate returns to college using college proximity as a proxy for the instrument low college cost. I find that a college degree increases the average hourly wage of the compliers by 15%–30%.",http://www.sciencedirect.com/science/article/pii/S0304407621001512
Journal of Econometrics,2023,Forward-selected panel data approach for program evaluation,Zhentao Shi and Jingyi Huang,"Policy evaluation is central to economic data analysis, but economists mostly work with observational data in view of limited opportunities to carry out controlled experiments. In the potential outcome framework, the panel data approach (Hsiao et al., 2012) constructs the counterfactual by exploiting the correlation between cross-sectional units in panel data. The choice of cross-sectional control units, a key step in its implementation, is nevertheless unresolved in data-rich environments when many possible controls are at the researcher’s disposal. We propose the forward selection method to choose control units, and establish validity of the post-selection inference. Our asymptotic framework allows the number of possible controls to grow much faster than the time dimension. The easy-to-implement algorithms and their theoretical guarantee extend the panel data approach to big data settings.",http://www.sciencedirect.com/science/article/pii/S0304407621001536
Journal of Econometrics,2023,Partially identifying competing risks models: An application to the war on cancer,Dongwoo Kim,Competing risks models for discretely measured durations are partially identifying due to the unknown dependence structure between risks and the discrete nature of the outcome. This article develops a highly tractable bounds approach for underlying distributions of latent durations by exploiting the discreteness. Bounds are obtained from a system of nonlinear (in)equalities. I devise a sequential solution method that requires much less computational burden than existing methods. Asymptotic properties of bound estimators and a simple bootstrap procedure are provided. I apply the proposed approach to re-evaluate trends in cancer mortality extending the data studied in Honoré and Lleras-Muney (2006). Estimated patterns differ from the original findings.,http://www.sciencedirect.com/science/article/pii/S0304407621001913
Journal of Econometrics,2023,Identifying marginal treatment effects in the presence of sample selection,"Otavio Bartalotti, Desire Kedagni and Vitor Possebom","This article presents identification results for the marginal treatment effect (MTE) when there is sample selection. We show that the MTE is partially identified for individuals who are always observed regardless of treatment, and derive uniformly sharp bounds on this parameter under three increasingly restrictive sets of assumptions. The first result imposes standard MTE assumptions with an unrestricted sample selection mechanism. The second set of conditions imposes monotonicity of the sample selection variable with respect to treatment, considerably shrinking the identified set. Finally, we incorporate a stochastic dominance assumption which tightens the lower bound for the MTE. Our analysis extends to discrete instruments. The results rely on a mixture reformulation of the problem where the mixture weights are identified, extending Lee’s (2009) trimming procedure to the MTE context. We propose estimators for the bounds derived and use data made available by Deb et al. (2006) to empirically illustrate the usefulness of our approach.",http://www.sciencedirect.com/science/article/pii/S0304407621002797
Journal of Econometrics,2023,Identification and estimation of triangular models with a binary treatment,Santiago Pereda-Fernández,"I study the identification and estimation of a nonseparable triangular model with an endogenous binary treatment. I impose neither rank invariance nor rank similarity on the unobservable term of the outcome equation. Identification is achieved by using continuous variation of the instrument and a shape restriction on the distribution of the unobservables, which is modeled with a copula. The latter captures the endogeneity of the model and is one of the components of the marginal treatment effect, making it informative about the effects of extending the treatment to untreated individuals. The estimation is a multi-step procedure based on rotated quantile regression. Finally, I use the estimator to revisit the effects of Work First Job Placements on future earnings.",http://www.sciencedirect.com/science/article/pii/S0304407622000410
Journal of Econometrics,2023,Treatment recommendation with distributional targets,"Anders Kock, David Preinerstorfer and Bezirgen Veliyev","We study the problem of a decision maker who must provide the best possible treatment recommendation based on an experiment. The desirability of the outcome distribution resulting from the policy recommendation is measured through a functional capturing the distributional characteristic that the decision maker is interested in optimizing. This could be, e.g., its inherent inequality, welfare, level of poverty or its distance to a desired outcome distribution. If the functional of interest is not quasi-convex or if there are constraints, the optimal recommendation may be a mixture of treatments. This vastly expands the set of recommendations that must be considered. We characterize the difficulty of the problem by obtaining maximal expected regret lower bounds. Furthermore, we propose two (near) regret-optimal policies. The first policy is static and thus applicable irrespectively of the subjects arriving sequentially or not in the course of the experimentation phase. The second policy can utilize that subjects arrive sequentially by successively eliminating inferior treatments and thus spends the sampling effort where it is most needed.",http://www.sciencedirect.com/science/article/pii/S0304407622001518
Journal of Econometrics,2023,Probabilistic prediction for binary treatment choice: With focus on personalized medicine,Charles Manski,"This paper extends my research applying statistical decision theory to treatment choice with sample data, using maximum regret to evaluate the performance of statistical treatment rules. The specific new contribution is to study as-if optimization using estimates of illness probabilities in a class of medical decisions. Beyond its specifics, the paper sends a broad message. Statisticians and computer scientists have addressed conditional prediction for decision making in indirect ways, the former applying classical statistical theory and the latter measuring prediction accuracy in test samples. Neither approach is satisfactory. Statistical decision theory provides a coherent, generally applicable methodology.",http://www.sciencedirect.com/science/article/pii/S0304407622001579
Journal of Econometrics,2023,Nonparametric difference-in-differences in repeated cross-sections with continuous treatments,"D’Haultfœuille, Xavier, Stefan Hoderlein and Yuya Sasaki","This paper studies the identification of causal effects of a continuous treatment using a new difference-in-difference strategy. Our approach allows for endogeneity of the treatment, and employs repeated cross-sections. It requires an exogenous change over time which affects the treatment in a heterogeneous way, stationarity of the distribution of unobservables and a rank invariance condition on the time trend. On the other hand, we do not impose any functional form restrictions or an additive time trend, and we are invariant to the scaling of the dependent variable. Under our conditions, the time trend can be identified using a control group, as in the binary difference-in-differences literature. In our scenario, however, this control group is defined by the data. We then identify average and quantile treatment effect parameters. We develop corresponding nonparametric estimators and study their asymptotic properties. Finally, we apply our results to the effect of disposable income on consumption.",http://www.sciencedirect.com/science/article/pii/S0304407622001452
Journal of Econometrics,2023,Synthetic Learner: Model-free inference on treatments over time,Davide Viviano and Jelena Bradic,"Understanding the effect of a particular treatment or a policy pertains to many areas of interest, ranging from political economics, marketing to healthcare. In this paper, we develop a non-parametric algorithm for detecting the effects of treatment over time in the context of Synthetic Controls. The method builds on counterfactual predictions from many algorithms without necessarily assuming that the algorithms correctly capture the model. We introduce an inferential procedure to detect treatment effects and show that the testing procedure controls size asymptotically for stationary, beta mixing processes without imposing any restriction on the set of base algorithms under consideration. We discuss consistency guarantees for average treatment effect estimates and derive regret bounds for the proposed methodology. The class of algorithms may include Random Forest, Lasso, or any other machine-learning estimator. Numerical studies and an application illustrate the advantages of the method.",http://www.sciencedirect.com/science/article/pii/S030440762200152X
Journal of Econometrics,2023,Estimation and inference of treatment effects with L2-boosting in high-dimensional settings,"Jannis Kueck, Ye Luo, Martin Spindler and Zigan Wang","Empirical researchers are increasingly faced with rich data sets containing many controls or instrumental variables, making it essential to choose an appropriate approach to variable selection. In this paper, we provide results for valid inference after post- or orthogonal L2-boosting is used for variable selection. We consider treatment effects after selecting among many control variables and instrumental variable models with potentially many instruments. To achieve this, we establish new results for the rate of convergence of iterated post-L2-boosting and orthogonal L2-boosting in a high-dimensional setting similar to Lasso, i.e., under approximate sparsity without assuming the beta-min condition. These results are extended to the 2SLS framework and valid inference is provided for treatment effect analysis. We give extensive simulation results for the proposed methods and compare them with Lasso. In an empirical application, we construct efficient IVs with our proposed methods to estimate the effect of pre-merger overlap of bank branch networks in the US on the post-merger stock returns of the acquirer bank.",http://www.sciencedirect.com/science/article/pii/S0304407622000471
Journal of Econometrics,2023,Multiple treatments with strategic substitutes,Jorge F. Balat and Sukjin Han,"We develop an empirical framework to identify and estimate the effects of treatments on outcomes of interest when the treatments are the result of strategic interaction (e.g., bargaining, oligopolistic entry, peer effects). We consider a model where agents play a discrete game of complete information and strategic substitutability, whose equilibrium actions (i.e., binary treatments) determine a post-game outcome in a nonseparable model with endogeneity. Due to the simultaneity in the first stage, the model as a whole is incomplete and the selection process fails to exhibit the conventional monotonicity. Without imposing parametric restrictions or large support assumptions, this poses challenges in recovering treatment parameters. To address these challenges, we establish a monotonic pattern of the equilibria in the first-stage game in terms of the number of treatments selected. Based on this finding, we derive bounds on the average treatment effects (ATE’s) under nonparametric shape restrictions and the existence of excluded exogenous variables. We show that the instrument variation that compensates strategic substitution helps solve the multiple equilibria problem. We apply our method to data on airlines and air pollution in cities in the U.S. We find that (i) the causal effect of each airline on pollution is positive, and (ii) the effect is increasing in the number of firms but at a decreasing rate.",http://www.sciencedirect.com/science/article/pii/S0304407622001671
Journal of Econometrics,2023,Regression-adjusted estimation of quantile treatment effects under covariate-adaptive randomizations,"Liang Jiang, Peter Phillips, Yubo Tao and Yichong Zhang","Datasets from field experiments with covariate-adaptive randomizations (CARs) usually contain extra covariates in addition to the strata indicators. We propose to incorporate these additional covariates via auxiliary regressions in the estimation and inference of unconditional quantile treatment effects (QTEs) under CARs. We establish the consistency and limit distribution of the regression-adjusted QTE estimator and prove that the use of multiplier bootstrap inference is non-conservative under CARs. The auxiliary regression may be estimated parametrically, nonparametrically, or via regularization when the data are high-dimensional. Even when the auxiliary regression is misspecified, the proposed bootstrap inferential procedure still achieves the nominal rejection probability in the limit under the null. When the auxiliary regression is correctly specified, the regression-adjusted estimator achieves the minimum asymptotic variance. We also discuss forms of adjustments that can improve the efficiency of the QTE estimators. The finite sample performance of the new estimation and inferential methods is studied in simulations, and an empirical application to a well-known dataset concerned with expanding access to basic bank accounts on savings is reported.",http://www.sciencedirect.com/science/article/pii/S0304407622001865
Journal of Econometrics,2023,State-domain change point detection for nonlinear time series regression,"Yan Cui, Jun Yang and Zhou Zhou","Change point detection in time series has attracted substantial interest, but most of the existing results have been focused on detecting change points in the time domain. This paper considers the situation where nonlinear time series have potential change points in the state domain. We apply a density-weighted anti-symmetric kernel function to the state domain and therefore propose a nonparametric procedure to test the existence of change points. When the existence of change points is affirmative, we further introduce an algorithm to estimate the number of change points together with their locations. Theoretical results of the proposed detection and estimation procedures are given and a real dataset is used to illustrate our methods.",http://www.sciencedirect.com/science/article/pii/S0304407621002645
Journal of Econometrics,2023,Improved marginal likelihood estimation via power posteriors and importance sampling,"Yong Li, Nianling Wang and Jun Yu","Power posteriors have become popular in estimating the marginal likelihood of a Bayesian model. A power posterior is referred to as the posterior distribution that is proportional to the likelihood raised to a power b∈[0,1]. Important power-posterior-based algorithms include thermodynamic integration (TI) of Friel and Pettitt (2008) and steppingstone sampling (SS) of Xie et al. (2011). In this paper, it is shown that the Bernstein–von Mises (BvM) theorem holds for power posteriors under regularity conditions. Due to the BvM theorem, power posteriors, when adjusted by the square root of the auxiliary constant, have the same limit distribution as the original posterior distribution, facilitating the implementation of the modified TI and SS methods via importance sampling. Unlike the TI and SS methods that require repeated sampling from the power posteriors, the modified methods only need the original posterior output and hence, are computationally more efficient. Moreover, they completely avoid the coding efforts associated with sampling from the power posteriors. Primitive conditions, under which the TI and modified TI algorithms can produce consistent estimators of the marginal likelihood, are provided. The numerical efficiency of the proposed methods is illustrated using two models.",http://www.sciencedirect.com/science/article/pii/S0304407621002736
Journal of Econometrics,2023,Bias reduction in spot volatility estimation from options,Viktor Todorov and Yang Zhang,"We consider the problem of nonparametric spot volatility estimation from options that is robust to time-variation in volatility and presence of jumps in the underlying asset price. Using a higher-order expansion of the characteristic function of the underlying price increment over shrinking time intervals and option-based estimates of the latter over two distinct horizons, we achieve asymptotic bias-reduction in spot volatility estimation, relative to existing methods, that is due to time-variation in volatility and presence of jumps. Further asymptotic improvement is achieved by de-biasing the volatility estimator using an estimate for the bias in it due to the small jumps in the price process. The gains from the newly-developed volatility estimation approach are illustrated on simulated data and in an empirical application.",http://www.sciencedirect.com/science/article/pii/S0304407621002785
Journal of Econometrics,2023,Maximum likelihood estimation of stochastic frontier models with endogeneity,Samuele Centorrino and María Pérez-Urdiales,"We propose and study a maximum likelihood estimator of stochastic frontier models with endogeneity in cross-section data when the composite error term may be correlated with inputs and environmental variables. Our framework is a generalization of the normal half-normal stochastic frontier model with endogeneity. We derive the likelihood function in closed form using three fundamental assumptions: the existence of control functions that fully capture the dependence between regressors and unobservables; the conditional independence of the two error components given the control functions; and the conditional distribution of the stochastic inefficiency term given the control functions being a folded normal distribution. We also provide a Battese–Coelli estimator of technical efficiency. Our estimator is computationally fast and easy to implement. We present some of its asymptotic properties, and we showcase its finite sample behavior in Monte-Carlo simulations and an empirical application to farmers in Nepal.",http://www.sciencedirect.com/science/article/pii/S0304407621002761
Journal of Econometrics,2023,Irregular identification of structural models with nonparametric unobserved heterogeneity,Juan Carlos Escanciano,"One of the most important empirical findings in microeconometrics is the pervasiveness of heterogeneity in economic behavior (cf. Heckman, 2001). This paper shows that cumulative distribution functions and quantiles of the nonparametric unobserved heterogeneity have an infinite efficiency bound in many structural economic models of interest. The paper presents general and precise conditions to prove such results. The usefulness of the theory is demonstrated with several relevant examples in economics, including, among others, the proportion of individuals with severe long term unemployment duration, Average Marginal Effects (AME) in a correlated random coefficient model without monotonicity, and the distribution and quantiles of random coefficients in linear, binary and the popular semiparametric Mixed Logit model.",http://www.sciencedirect.com/science/article/pii/S0304407621003055
Journal of Econometrics,2023,Vector copulas,Yanqin Fan and Marc Henry,"This paper introduces vector copulas associated with multivariate distributions with given multivariate marginals, based on the theory of measure transportation, and establishes a vector version of Sklar’s theorem. The latter provides a theoretical justification for the use of vector copulas to characterize nonlinear or rank dependence between a finite number of random vectors (robust to within vector dependence), and to construct multivariate distributions with any given non-overlapping multivariate marginals. We construct Elliptical and Kendall families of vector copulas, derive their densities, and present algorithms to generate data from them. The use of vector copulas is illustrated with a stylized analysis of international financial contagion.",http://www.sciencedirect.com/science/article/pii/S0304407621002803
Journal of Econometrics,2023,Most powerful test against a sequence of high dimensional local alternatives,"Yi He, Sombut Jaidee and Jiti Gao","We develop a powerful quadratic test for the overall significance of many covariates in a dense regression model in the presence of nuisance parameters. By equally weighting the sample moments, the test is asymptotically correct in high dimensions even when the number of coefficients is larger than the sample size. Our theory allows a non-parametric error distribution and weakly exogenous nuisance variables, in particular autoregressors in many applications. Using random matrix theory, we show that the test has the optimal asymptotic testing power among a large class of competitors against local alternatives whose coordinates are dense in the eigenbasis of the high dimensional sample covariance matrix among regressors. The asymptotic results are adaptive to the covariates’ cross-sectional and temporal dependence structure and do not require a limiting spectral law of their sample covariance matrix. In the most general case, the nuisance estimation may play a role in the asymptotic limit and we give a robust modification for these irregular scenarios. Monte Carlo studies suggest a good power performance of our proposed test against high dimensional dense alternative for various data generating processes. We apply the test to detect the significance of over one hundred exogenous variables in the FRED-MD database for predicting the monthly growth in the US industrial production index.",http://www.sciencedirect.com/science/article/pii/S0304407621003079
Journal of Econometrics,2023,Conditional asymmetry in Power ARCH(∞) models,Julien Royer,"We consider an extension of ARCH(∞) models to account for conditional asymmetry in the presence of high persistence. After stating existence and stationarity conditions, this paper develops the statistical inference of such models and proves the consistency and asymptotic distribution of a Quasi Maximum Likelihood estimator. Some particular specifications are studied and we introduce a Portmanteau goodness-of-fit test. Additionally, test procedures for asymmetry and GARCH validity are derived. Finally, we present an application on a set of equity indices to reexamine the preeminence of GARCH(1,1) specifications. We find strong evidence that the short memory feature of such models is not suitable for peripheral assets.",http://www.sciencedirect.com/science/article/pii/S0304407621003031
Journal of Econometrics,2023,Quantile regression with censoring and sample selection,Songnian Chen and Qian Wang,"Arellano and Bonhomme (2017) considered nonparametric identification and semiparametric estimation of a quantile selection model, and Arellano and Bonhomme (2017s) extended the estimation approach to the case with censoring. However, there are some major drawbacks associated with the approach in Arellano and Bonhomme (2017s). In this paper we consider nonparametric and semiparametric identification of the quantile selection model with censoring, and we further propose a semiparametric estimation procedure by making some major adjustments to Arellano and Bonhomme’s (2017, 2017s) approaches to overcome the above mentioned drawbacks. Our estimator is shown to be consistent and asymptotically normal. A Monte Carlo study indicates that our estimator performs well in finite samples. Our method is illustrated with a CPS data to study wage inequality.",http://www.sciencedirect.com/science/article/pii/S0304407621003092
Journal of Econometrics,2023,A new robust inference for predictive quantile regression,"Zongwu Cai, Haiqiang Chen and Xiaosai Liao","This paper proposes a novel approach to offer a robust inferential theory across all types of persistent regressors in a predictive quantile regression model. We first estimate a quantile regression with an auxiliary regressor, which is generated as a weighted combination of an exogenous random walk process and a bounded transformation of the original regressor. With a similar spirit of rotation in factor analysis, one can then construct a weighted estimator using the estimated coefficients of the original predictor and the auxiliary regressor. Under some mild conditions, it shows that the self-normalized test statistic based on the weighted estimator converges to a standard normal distribution. Our new approach enjoys a good property that it can reach the local power under the optimal rate T with nonstationary predictor and T for stationary predictor, respectively. More importantly, our approach can be easily used to characterize mixed persistency degrees in multiple regressions. Simulations and empirical studies are provided to demonstrate the effectiveness of the newly proposed approach. The heterogeneous predictability of US stock returns at different quantile levels is reexamined.",http://www.sciencedirect.com/science/article/pii/S030440762100302X
Journal of Econometrics,2023,Quasi score-driven models,"F. Blasques, Christian Francq and Sébastien Laurent","This paper introduces the class of quasi score-driven (QSD) models. This new class inherits and extends the basic ideas behind the development of score-driven (SD) models and addresses a number of unsolved issues in the score literature. In particular, the new class of models (i) generalizes many existing models, including SD models, (ii) disconnects the updating equation from the log-likelihood implied by the conditional density of the observations, (iii) allows testing of the assumptions behind SD models that link the updating equation of the conditional moment to the conditional density, (iv) allows QML estimation of SD models, (v) and allows explanatory variables to enter the updating equation. We establish the asymptotic properties of the QLE, QMLE and MLE of the proposed QSD model as well as the likelihood ratio and Lagrange multiplier test statistics. The finite sample properties are studied by means of an extensive Monte Carlo study. Finally, we show the empirical relevance of QSD models to estimate the conditional variance of 400 US stocks.",http://www.sciencedirect.com/science/article/pii/S030440762200001X
Journal of Econometrics,2023,Structural inference in sparse high-dimensional vector autoregressions,"J. Krampe, E. Paparoditis and Carsten Trenkler","We consider statistical inference for impulse responses and forecast error variance decompositions in sparse, structural high-dimensional vector autoregressive (SVAR) systems. We introduce consistent estimators of impulse responses in the high-dimensional setting and suggest valid inference procedures for the same parameters. Statistical inference in our setting is much more involved since standard procedures, like the delta-method, do not apply. By using local projection equations, we first construct a de-sparsified version of regularized estimators of the moving average parameters associated with the VAR system. We then obtain estimators of the structural impulse responses by combining the aforementioned de-sparsified estimators with a non-regularized estimator of the contemporaneous impact matrix, also taking into account the high-dimensionality of the system. We show that the distribution of the derived estimators of structural impulse responses has a Gaussian limit. We also present a valid bootstrap procedure to estimate this distribution. Applications of the inference procedure in the construction of confidence intervals for impulse responses as well as in tests for forecast error variance decomposition are presented. Our procedure is illustrated by means of simulations and an empirical application.",http://www.sciencedirect.com/science/article/pii/S0304407622000057
Journal of Econometrics,2023,Identification of unobserved distribution factors and preferences in the collective household model,Stefan Hubner,"This paper develops a non-parametric collective random utility model with a continuous choice of consumption and sharing. We allow for a heterogeneous population by introducing unobserved distribution factors and preferences that are non-separable in the stochastic household choice system. We provide necessary and sufficient conditions for invertibility with respect to the vector of unobserved heterogeneity. Further, we show non-parametric identification of each individual’s idiosyncratic preferences and Pareto weights from the distribution of observed choices. Based on this result we develop a non-parametric two-step estimation procedure and estimate individual counterfactuals in a collective labour supply model for households with children using the LISS panel.",http://www.sciencedirect.com/science/article/pii/S0304407622000173
Journal of Econometrics,2023,Finite-sample corrected inference for two-step GMM in time series,Jungbin Hwang and Gonzalo Valdés,"This paper develops a finite-sample corrected inference for the efficient generalized method of moments (GMM) in time series. To capture a higher-order uncertainty embodied in estimating the time series GMM weight matrix, we extend the finite-sample corrected variance formula of Windmeijer (2005) to heteroskedasticity autocorrelated robust (HAR) inference. Using fixed-smoothing asymptotics, we show that our finite-sample corrected test statistics lead to standard asymptotic t or F critical values and suffer from less over-rejection of the null hypothesis than existing GMM procedures on finite-samples, including continuously updating GMM. Not only does our finite-sample corrected variance formula correct for the bias arising from the plugged-in long-run variance estimation, but it is also not exposed to a potential side effect of Windmeijer’s formula, which can introduce an additional source of over-rejection after the correction.",http://www.sciencedirect.com/science/article/pii/S0304407622000069
Journal of Econometrics,2023,PELVE: Probability Equivalent Level of VaR and ES,Hengxin Li and Ruodu Wang,"In the recent Fundamental Review of the Trading Book (FRTB), the Basel Committee on Banking Supervision proposed the shift from the 99% Value-at-Risk (VaR) to the 97.5% Expected Shortfall (ES) for internal models in market risk assessment. Inspired by the above transition, we introduce a new distributional index, the probability equivalence level of VaR and ES (PELVE), which identifies the balancing point for the equivalence between VaR and ES. PELVE enjoys many desirable theoretical properties and it distinguishes empirically heavy-tailed distributions from light-tailed ones via a threshold of 2.72. Convergence properties and asymptotic normality of the empirical PELVE estimators are established. Applying PELVE to financial asset and portfolio data leads to interesting observations that are not captured by VaR or ES alone. We find that, in general, the transition from VaR to ES in the FRTB yields an increase in risk capital for single-asset portfolios, but for well-diversified portfolios, the capital requirement remains almost unchanged. This leads to both a theoretical justification and an empirical evidence for the conclusion that the use of ES rewards portfolio diversification more than the use of VaR.",http://www.sciencedirect.com/science/article/pii/S0304407622000380
Journal of Econometrics,2023,"It ain’t where you’re from, it’s where you’re at: Hiring origins, firm heterogeneity, and wages","Sabrina Di Addario, Patrick Kline, Raffaele Saggio and Mikkel Sølvsten","Sequential auction models of labor market competition predict that the wages required to successfully poach a worker from a rival employer will depend on the productivities of both the poached and poaching firms. We develop a theoretically grounded extension of the two-way fixed effects model of Abowd et al. (1999) in which log hiring wages are comprised of a worker fixed effect, a fixed effect for the “destination” firm hiring the worker, and a fixed effect for the “origin” firm, or labor market state, from which the worker was hired. This specification is shown to nest the reduced form for hiring wages delivered by semi-parametric formulations of the canonical sequential auction model of Postel-Vinay and Robin (2002b) and its generalization in Bagger et al. (2014). Fitting the model to Italian social security records, origin effects are found to explain only 0.7% of the variance of hiring wages among job movers, while destination effects explain more than 23% of the variance. Across firms, destination effects are more than 13 times as variable as origin effects. Interpreted through the lens of Bagger et al. (2014)’s model, this finding requires that workers possess implausibly strong bargaining strength. Studying a cohort of workers entering the Italian labor market in 2005, we find that differences in origin effects yield essentially no contribution to the evolution of the gender gap in hiring wages, while differences in destination effects explain the majority of the gap at the time of labor market entry. These results suggest that where a worker is hired from tends to be relatively inconsequential for their wages in comparison to where they are currently employed.",http://www.sciencedirect.com/science/article/pii/S0304407622000641
Journal of Econometrics,2023,Do firm effects drift? Evidence from Washington administrative data,"Marta Lachowska, Alexandre Mas, Raffaele Saggio and Stephen Woodbury","We study the time-series properties of firm effects in the two-way fixed effects model popularized by Abowd et al. (1999) (AKM) using two approaches. The first—the rolling AKM approach (R-AKM)—estimates AKM models separately for successive two-year intervals. The second—the time-varying AKM approach (TV-AKM)—is an extension of the original AKM model that allows for unrestricted interactions of year and firm indicators. We apply to both approaches the leave-out methodology of Kline et al. (2020) to correct for biases in the estimated variance components. Using administrative wage records from Washington State, we find, first, that firm effects for hourly wage rates are highly persistent with an autocorrelation coefficient between firm effects in 2002 and 2014 of 0.74. Second, the R-AKM approach reveals cyclicality in firm effects and worker–firm sorting. During the Great Recession the variability in firm effects increased, while the degree of worker–firm sorting decreased. Third, misspecification of standard AKM models resulting from restricting firm effects to be fixed over time appears to be minimal.",http://www.sciencedirect.com/science/article/pii/S0304407622000604
Journal of Econometrics,2023,Firm pay dynamics,"Niklas Engbom, Christian Moser and Jan Sauermann","We study the nature of firm pay dynamics. To this end, we propose a statistical model that extends the seminal framework by Abowd et al. (1999) to allow for idiosyncratically time-varying firm pay policies. We estimate the model using linked employer–employee data for Sweden from 1985 to 2016. By drawing on detailed firm financials data, we show that firms that become more productive and accumulate capital raise pay, whereas firms lower pay as they add workers. A secular increase in firm-year pay dispersion in Sweden since 1985 is accounted for by greater persistence of firm pay among incumbent firms as well as greater dispersion in firm pay among entrant firms, as opposed to more volatile firm pay.",http://www.sciencedirect.com/science/article/pii/S0304407622000653
Journal of Econometrics,2023,Establishment age and wages,Johannes Schmieder,"This paper investigates the relationship between establishment age and wages. A key challenge is that new establishments differ from mature establishments along many dimensions due to non-random survival of establishments. Furthermore, worker composition changes as establishments age. To get around this, the paper uses linked employer–employee data from Germany and estimates the establishment age–wage relationship controlling for establishment and worker fixed effects and focuses on starting wages of workers. I show a striking pattern: starting wages in new establishments are 9 percent higher than in mature establishments (older than 20 years). I also report evidence that this difference is not driven by differences in hours, compensation for unemployment risk or slower future wage growth. Instead, I show that the difference is driven by, on average, high growth rates in new establishments as would be predicted by monopsonistic search models.",http://www.sciencedirect.com/science/article/pii/S0304407623000350
Journal of Econometrics,2023,Twisting the demand curve: Digitalization and the older workforce,"Erling Barth, James Davis, Richard Freeman and Kristina McElheran","Software represents a major and fast-growing share of firms’ capital investment, impacting demand for labor and what workers do on their jobs. Using U.S. Census Bureau panel data that link firms and workers, this paper estimates the effect of firm software capital on the earnings of workers by age group. We extend the AKM framework to include job-spell fixed effects that account for potential correlation between the worker–firm match and employee age, as well as including time-varying firm effects that allow for a correlation between wage-enhancing productivity shocks and software investments. Within job-spell, capitalized software investment raises worker earnings. However, it does so at a rate that declines after the age of 50, to about zero beyond 65. Our data further show that software capital increases the earnings of high-wage workers relative to low-wage workers and earnings in high-wage firms relative to low-wage firms, thereby widening earnings inequality within and across firms.",http://www.sciencedirect.com/science/article/pii/S0304407621003018
Journal of Econometrics,2023,Social connections and the sorting of workers to firms,"Marcus Eliason, Lena Hensvik, Francis Kramarz and Oskar Skans","We assess the presumption that social networks reinforce inequality by providing high-wage workers’ with preferential access to high-wage establishments. Our results based on very detailed Swedish register data contradict this view. We do show that high-wage job seekers tend to be connected to high-wage workers employed in high-wage establishments. Furthermore, social connections appear to directly cause the allocation of workers to jobs. But the sorting resulting from hires within social networks is less unequal than the sorting resulting from market hires, essentially because low-wage firms rely on social connections to hire high-wage workers.",http://www.sciencedirect.com/science/article/pii/S0304407622000896
Journal of Econometrics,2023,Gender differences in sorting on wages and risk,Kurt Lavetti and Ian Schmutte,"We use Brazilian matched employer–employee data to provide new evidence on gender disparities in labor market sorting on wages and workplace safety. We show that women and men sort in highly disparate, but systematic, ways on the basis of physical risk, despite sorting almost identically on financial risk. To understand what factors might explain these gender differences in sorting, we begin by ruling out the possibility that men and women receive different compensating wage differentials for risk. We find that women do earn smaller establishment wage premia than men on average, but this difference is also unlikely to explain the sorting patterns. Despite having little direct effect on wages, sorting on safety may have large indirect effects on the wage gap through its outsized influence on the segregation of women and men across establishments. This segregation results in men earning substantially higher establishment wage premia, which explains 28% of the entire gender wage gap in Brazil.",http://www.sciencedirect.com/science/article/pii/S030440762200183X
Journal of Econometrics,2023,Cyclical labor market sorting,"Leland D. Crane, Henry R. Hyatt and Seth M. Murray","We consider sorting in the labor market, that is, whether high- or low-productivity workers and firms tend to match with each other, and how this varies over time using U.S. linked employer–employee data. Composition changes of workers and firms move in opposite directions over the business cycle. During and after recessions, low-rank workers are less likely to work, while the employment share of low-rank firms increases. The agreement between worker and firm ranks increases in the early stages of labor market downturns.",http://www.sciencedirect.com/science/article/pii/S0304407622000616
Journal of Econometrics,2023,Employer policies and the immigrant–native earnings gap,"Benoit Dostie, Jiang Li, David Card and Daniel Parent","We use longitudinal data from the income tax system to study the impacts of firms’ employment and wage-setting policies on the level and change in immigrant–native wage differences in Canada. We focus on immigrants who arrived in the early 2000s, distinguishing between those with and without a college degree from two broad groups of countries – the U.S., the U.K. and Northern Europe, and the rest of the world. Consistent with a growing literature based on the two-way fixed effects model of Abowd, Kramarz, and Margolis (1999), we find that firm-specific wage premiums explain a significant share of earnings inequality in Canada and contribute to the average earnings gap between immigrants and natives. In the decade after receiving permanent status, earnings of immigrants rise relative to those of natives. Compositional effects due to selective outmigration and changing participation play no role in this gain. About one-sixth is attributable to movements up the job ladder to employers that offer higher pay premiums for all groups, with particularly large gains for immigrants from the “rest of the world” countries.",http://www.sciencedirect.com/science/article/pii/S0304407621002293
Journal of Econometrics,2023,"The determinants of displaced workers’ wages: Sorting, matching, selection, and the Hartz reforms",Simon Woodcock,"We present a simple new method to decompose the wage effects of displacement into components due to differences in the way that displaced and non-displaced workers are sorted across higher- and lower-paying employers (a sorting effect), differences in the quality of worker–employer matches that they enter into (a matching effect), and differences in their unobservable characteristics (a selection effect). In an extended application, we apply our decomposition to understand how the determinants of displaced workers’ wages in Germany changed following the 2003–2005 Hartz reforms. We find that the wages of displaced workers fell substantially after the reforms, and that over 80 percent of the decline was because they found re-employment at lower-paying employers. Sorting into worse matches explains a smaller 5–9 percent of the wage decline experienced by men, and 12–23.5 percent of the female wage decline. Collectively, the sorting and matching channels explain almost all of the post-reform decline in displaced workers’ wages, and selection played little role.",http://www.sciencedirect.com/science/article/pii/S030440762200118X
Journal of Econometrics,2023,The persistence of wages,"Anabela Carneiro, Pedro Portugal, Pedro Raposo and Paulo Rodrigues","This paper documents the extent to which wage persistence can be explained by permanent worker, employer, and match heterogeneity. Standard methods used to perform such decompositions for industry or racial wage gaps are inappropriate for decomposing wage persistence in dynamic panel data models because of the incidental parameter problem. When we apply these methods without bias correction, we find that the majority, 59.3 percent, of wage persistence is explained by worker heterogeneity, with employer and match heterogeneity explaining 29.7 and 11.0 percent, respectively. We evaluate three methods for addressing incidental parameter bias using a Monte Carlo study. An empirical application to Portuguese linked employer–employee data shows that the uncorrected estimates tend to understate wage persistence by around 24 to 42 percent, depending on the choice of the bias correction estimator used, and overstate the extent to which wage persistence arises from permanent unobserved heterogeneity. Furthermore, results indicate that the uncorrected estimates overstate the role of permanent worker heterogeneity, and understate the role of firm heterogeneity.",http://www.sciencedirect.com/science/article/pii/S0304407621002839
Journal of Econometrics,2023,"Union membership density and wages: The role of worker, firm, and job-title heterogeneity","John T. Addison, Pedro Portugal and Hugo de Almeida Vilares","We examine the association between union density and wages in Portugal where just 10 percent of all workers are union members but nine-tenths of them are covered by collective agreements. Using a unique dataset on workers, firms, and collective bargaining agreements, we examine the union density wage gap in total monthly wages and its sources – namely, worker, firm, and job-title or ‘occupational’ heterogeneity – using the Gelbach decomposition. The most important source of the mark-up associated with union density is the firm fixed effect, reflecting the differing wage policies of more and less unionized workplaces, which explains two-thirds of the wage gap. Next in importance is the job-title fixed effect, capturing occupational heterogeneity across industries. It makes up one-third of the gap, the inference being that the unobserved skills of workers contribute at most only trivially to the union density wage gap. In a separate analysis based on disaggregations of the total wage, it is also found that employers can in part offset the impact of the bargaining power of unions on wages through firm-specific wage arrangements in the form of the wage cushion. Finally, union density is shown to be associated with a modest reduction in wage inequality as the union density wage gap is highest among low-wage workers. This result is driven by the job-title fixed effect, low-wage workers benefiting more from being placed in higher paying ‘occupations.’",http://www.sciencedirect.com/science/article/pii/S0304407621003006
Journal of Econometrics,2023,Unequal use of social insurance benefits: The role of employers,"Sarah Bana, Kelly Bedard, Maya Rossin-Slater and Jenna Stearns","Disability Insurance (DI) and Paid Family Leave (PFL) programs are important sources of social insurance, but there is considerable inequality in benefit take-up, and little is known about the role of firms in determining benefit use. Using administrative data from California, we find that firms that pay higher earnings premiums also have substantially higher public DI and PFL take-up rates, and that this relationship is particularly strong among the lowest-earning workers within the firm. Our results suggest that changes in firm behavior may impact social insurance use, thus reducing an important dimension of inequality in America.",http://www.sciencedirect.com/science/article/pii/S0304407622000628
Journal of Econometrics,2023,Internal labor markets: A worker flow approach,"Ingrid Huitfeldt, Andreas R. Kostøl, Jan Nimczik and Andrea Weber",This paper develops a new method to study how workers’ career and wage profiles are shaped by internal labor markets (ILM) and job hierarchies in firms. We tackle the conceptual challenge of organizing jobs within firms into hierarchy levels by proposing a data-driven ranking method based on observed worker flows between occupations within firms. We apply our method to linked employer–employee data from Norway that record fine-grained occupational codes and track contract changes within firms. Our findings confirm existing evidence that is primarily based on case studies for single firms. We expand on this by documenting substantial heterogeneity in the structure and hierarchy of ILMs across a broad range of large firms. Our findings on wage and promotion dynamics in ILMs are consistent with models of careers in organizations.,http://www.sciencedirect.com/science/article/pii/S030440762200063X
Journal of Econometrics,2023,Estimation of spillover effects with matched data or longitudinal network data,Martin Braun and Valentin Verdier,"Social interactions often play a key role in determining the impact of policies, but measuring the magnitude of spillover effects empirically is notoriously challenging because, in most applications, a person’s relationships are likely to reflect her own characteristics (homophily), and people who are connected are likely to be affected by the same shocks (common factors). In addition, a significant share of social interactions is likely to occur through variables that are not observed by the researcher. When matched data are used, observations corresponding to the same cross-sectional units (e.g., workers or students) can be linked over time, and a cross-sectional unit’s relationships (e.g., co-workers or classmates) are indexed in each time period. We show that comparisons over time in the outcomes of individuals whose relationships changed can be used to measure the importance of social interactions in the presence of flexible patterns of selection on unobservables and common factors, even if social interactions only occur through unobservables. We apply our results to estimate the importance of peer effects in student learning in elementary school.",http://www.sciencedirect.com/science/article/pii/S0304407621002827
Journal of Econometrics,2023,Multi-dimensional latent group structures with heterogeneous distributions,"Xuan Leng, Heng Chen and Wendun Wang","This paper aims to identify the multi-dimensional latent grouped heterogeneity of distributional effects. We consider a panel quantile regression model with additive cross-section and time fixed effects. The cross-section effects and quantile slope coefficients are both characterized by grouped patterns of heterogeneity, but each unit can belong to different groups for cross-section effects and slopes. We propose a composite-quantile approach to jointly estimate multi-dimensional group memberships, slope coefficients, and fixed effects. We show that using multiple quantiles improves clustering accuracy if memberships are quantile-invariant. We apply the methods to examine the relationship between managerial incentives and risk-taking behavior.",http://www.sciencedirect.com/science/article/pii/S0304407621002177
Journal of Econometrics,2023,Canonical correlation-based model selection for the multilevel factors,"In Choi, Rui Lin and Yongcheol Shin","We develop a novel approach based on the canonical correlation analysis to identify the number of the global factors in the multilevel factor model. We propose the two consistent selection criteria, the canonical correlations difference (CCD) and the modified canonical correlations (MCC). Via Monte Carlo simulations, we show that CCD and MCC select the number of global factors correctly even in small samples, and they are robust to the presence of serially correlated and weakly cross-sectionally correlated idiosyncratic errors as well as the correlated local factors. Finally, we demonstrate the utility of our approach with an application to the multilevel asset pricing model for the stock return data in 12 industries in the U.S.",http://www.sciencedirect.com/science/article/pii/S0304407621002207
Journal of Econometrics,2023,Estimation of panel group structure models with structural breaks in group memberships and coefficients,"Robin L. Lumsdaine, Ryo Okui and Wendun Wang","This paper considers linear panel data models with a grouped pattern of heterogeneity when the latent group membership structure and/or the values of slope coefficients change at a break point. We propose a least squares approach to jointly estimate the break point, group membership structure, and coefficients. The proposed estimators are consistent, and the asymptotic distribution of the coefficient estimators is identical to that under known break point and group structure even when the cross-sectional sample size is much larger than the length of time series. Monte Carlo simulations and an empirical example illustrate the use of the approach and associated inference.",http://www.sciencedirect.com/science/article/pii/S0304407622000033
Journal of Econometrics,2023,Shrinkage estimation of network spillovers with factor structured errors,Ayden Higgins and Federico Martellosio,"This paper explores the estimation of a panel data model with cross-sectional interaction that is flexible both in its approach to specifying the network of connections between cross-sectional units, and in controlling for unobserved heterogeneity. It is assumed that there are different sources of information available on a network, which can be represented in the form of multiple weights matrices. These matrices may reflect observed links, different measures of connectivity, groupings or other network structures, and the number of matrices may be increasing with sample size. A penalised quasi-maximum likelihood estimator is proposed which aims to alleviate the risk of network misspecification by shrinking the coefficients of irrelevant weights matrices to exactly zero. Moreover, controlling for unobserved factors in estimation provides a safeguard against the misspecification that might arise from unobserved heterogeneity. The asymptotic properties of the estimator are derived in a framework where the true value of each parameter remains fixed as the total number of parameters increases. A Monte Carlo simulation is used to assess finite sample performance, and in an empirical application the method is applied to study the prevalence of network spillovers in determining growth rates across countries.",http://www.sciencedirect.com/science/article/pii/S0304407621003080
Journal of Econometrics,2023,A test for Kronecker Product Structure covariance matrix,"Patrik Guggenberger, Frank Kleibergen and Sophocles Mavroeidis","We propose a test for a covariance matrix to have Kronecker Product Structure (KPS). KPS implies a reduced rank restriction on a certain transformation of the covariance matrix and the new procedure is an adaptation of the Kleibergen and Paap (2006) reduced rank test. To derive the limiting distribution of the Wald type test statistic proves challenging partly because of the singularity of the covariance matrix estimator that appears in the weighting matrix. We show that the test statistic has a χ2 limiting null distribution with degrees of freedom equal to the number of restrictions tested. Local asymptotic power results are derived. Monte Carlo simulations reveal good size and power properties of the test. Re-examining fifteen highly cited papers conducting instrumental variable regressions, we find that KPS is not rejected in 56 out of 118 specifications at the 5% nominal size.",http://www.sciencedirect.com/science/article/pii/S0304407622000203
Journal of Econometrics,2023,Factor-based imputation of missing values and covariances in panel data of large dimensions,"Ercument Cahan, Jushan Bai and Serena Ng","Economists are blessed with a wealth of data for analysis, but more often than not, values in some entries of the data matrix are missing. Various methods have been proposed to handle missing observations in a few variables. We exploit the factor structure in panel data of large dimensions. Our tall-project algorithm first estimates the factors from a tall block in which data for all rows are observed, and projections of unit specific sample size are then used to estimate the factor loadings. A missing value is imputed by its estimated common component which we show is consistent and asymptotically normal without further iteration. Implications for using imputed data in factor augmented regressions are then discussed. To compensate for the downward bias in sample covariance matrices created by an omitted noise in each imputed value, we overlay the imputed data with re-sampled idiosyncratic residuals many times and use the average of the covariances to estimate the parameters of interest. Simulations show that the procedures have desirable finite sample properties.",http://www.sciencedirect.com/science/article/pii/S0304407622000215
Journal of Econometrics,2023,Group fused Lasso for large factor models with multiple structural breaks,Chenchen Ma and Yundong Tu,"This paper reformulates the identification of multiple structural breaks in factor loadings as a problem of detecting structural breaks in a factor regression, where the estimated pseudo factor corresponding to the largest eigenvalue is regressed on the remaining estimated factors. As a result, a group fused Lasso based estimation procedure is proposed to identify the break dates. Our procedure is practically easy-to-implement with standard statistical packages, overcoming the drawbacks of the existing methods that they often involve multiple tuning parameters and are computationally demanding in dealing with multiple unknown breaks. Theoretical properties of the proposed estimators are established, with a data driven choice of tuning parameter in the procedure. The Monte Carlo simulations and a real data application demonstrate that our procedure is fast implementable with desirable accuracy performance, and thus enjoys practical merits.",http://www.sciencedirect.com/science/article/pii/S0304407622000331
Journal of Econometrics,2023,High-dimensional VARs with common factors,"Ke Miao, Peter Phillips and Liangjun Su","This paper studies high-dimensional vector autoregressions (VARs) augmented with common factors that allow for strong cross-sectional dependence. Models of this type provide a convenient mechanism for accommodating the interconnectedness and temporal co-variability that are often present in large dimensional systems. We propose an ℓ1-nuclear-norm regularized estimator and derive the non-asymptotic upper bounds for the estimation errors as well as large sample asymptotics for the estimates. A singular value thresholding procedure is used to determine the correct number of factors with probability approaching one. Both the LASSO estimator and the conservative LASSO estimator are employed to improve estimation precision. The conservative LASSO estimates of the non-zero coefficients are shown to be asymptotically equivalent to the oracle least squares estimates. Simulations demonstrate that our estimators perform reasonably well in finite samples given the complex high-dimensional nature of the model. In an empirical illustration we apply the methodology to explore dynamic connectedness in the volatilities of financial asset prices and the transmission of ‘investor fear’. The findings reveal that a large proportion of connectedness is due to the common factors. Conditional on the presence of these common factors, the results still document remarkable connectedness due to the interactions between the individual variables, thereby supporting a common factor augmented VAR specification.",http://www.sciencedirect.com/science/article/pii/S030440762200032X
Journal of Econometrics,2023,Treatment effects in interactive fixed effects models with a small number of time periods,Brantly Callaway and Sonia Karami,"This paper considers identifying and estimating the Average Treatment Effect on the Treated (ATT) when untreated potential outcomes are generated by an interactive fixed effects model. That is, in addition to time-period and individual fixed effects, we consider the case where there is an unobserved time invariant variable whose effect on untreated potential outcomes may change over time and which can therefore cause outcomes (in the absence of participating in the treatment) to follow different paths for the treated group relative to the untreated group. The models that we consider in this paper generalize many commonly used models in the treatment effects literature including difference in differences and individual-specific linear trend models. Unlike the majority of the literature on interactive fixed effects models, we do not require the number of time periods to go to infinity to consistently estimate the ATT. Our main identification result relies on having the effect of some time invariant covariate (e.g., race or sex) not vary over time. Using our approach, we show that the ATT can be identified with as few as three time periods and with panel or repeated cross sections data.",http://www.sciencedirect.com/science/article/pii/S030440762200029X
Journal of Econometrics,2023,Quasi-maximum likelihood estimation of break point in high-dimensional factor models,"Jiangtao Duan, Jushan Bai and Xu Han","This paper proposes a quasi-maximum likelihood (QML) estimator of the break point for large-dimensional factor models with a single structural break in the factor loading matrix. We show that the QML estimator is consistent for the true break point when the covariance matrix of the pre- or post-break factor loading (or both) is singular. Consistency here means that the deviation of the estimated break date from the true break date k0 converges to zero as the sample size grows. This is a much stronger result than the break fraction kˆ/T being T-consistent (super-consistent) for k0/T. Also, singularity occurs for most types of structural changes, except for a rotational change. Even for a rotational change, the QML estimator is still T-consistent in terms of the break fraction. Simulation results confirm the theoretical properties of our estimator, and it significantly outperforms existing estimators for change points in factor models.",http://www.sciencedirect.com/science/article/pii/S0304407622000379
Journal of Econometrics,2023,Information criteria for latent factor models: A study on factor pervasiveness and adaptivity,"Xiao Guo, Yu Chen and Cheng Yong Tang","We study the information criteria extensively under general conditions for high-dimensional latent factor models. Upon carefully analyzing the estimation errors of the principal component analysis method, we establish theoretical results on the estimation accuracy of the latent factor scores, incorporating the impact from possibly weak factor pervasiveness; our analysis does not require the same factor strength of all the leading factors. To estimate the number of the latent factors, we propose a new penalty specification with a two-fold consideration: i) being adaptive to the strength of the factor pervasiveness, and ii) favoring more parsimonious models. Our theory establishes the validity of the proposed approach under general conditions. Additionally, we construct examples to demonstrate that when the factor strength is too weak, scenarios exist such that no information criterion can consistently identify the latent factors. We illustrate the performance of the proposed adaptive information criteria with extensive numerical examples, including simulations and a real data analysis.",http://www.sciencedirect.com/science/article/pii/S0304407622000707
Journal of Econometrics,2023,Identifying latent factors based on high-frequency data,"Yucheng Sun, Wen Xu and Chuanhai Zhang","This paper tests whether the continuous component of an observable candidate factor is in the space spanned by the counterparts of latent common factors with high-frequency financial data. We introduce two identification strategies corresponding to two types of regressions: the regressions of intraday asset returns on the estimated factors and the candidate, and the regression of the candidate factor on the estimated ones. We construct the test statistics by adding randomness to the statistics obtained from residuals of the regressions, and demonstrate the consistency of the novel randomized tests. Simulations are conducted to evaluate the performance of the tests in finite samples. We also perform empirical applications to identify the relationships between some candidate factors and the latent ones, and further use the factors selected by the tests for portfolio allocation.",http://www.sciencedirect.com/science/article/pii/S0304407622000938
Journal of Econometrics,2023,Large dimensional latent factor modeling with missing observations and applications to causal inference,Ruoxuan Xiong and Markus Pelger,"This paper develops the inferential theory for latent factor models estimated from large dimensional panel data with missing observations. We propose an easy-to-use all-purpose estimator for a latent factor model by applying principal component analysis to an adjusted covariance matrix estimated from partially observed panel data. We derive the asymptotic distribution for the estimated factors, loadings and the imputed values under an approximate factor model and general missing patterns. The key application is to estimate counterfactual outcomes in causal inference from panel data. The unobserved control group is modeled as missing values, which are inferred from the latent factor model. The inferential theory for the imputed values allows us to test for individual treatment effects at any time under general adoption patterns where the units can be affected by unobserved factors.",http://www.sciencedirect.com/science/article/pii/S0304407622000914
Journal of Econometrics,2023,Testing for structural changes in large dimensional factor models via discrete Fourier transform,"Zhonghao Fu, Yongmiao Hong and Xia Wang","We propose a new test for structural changes in large dimensional factor models using a discrete Fourier transform (DFT) approach. When structural changes occur, the conventional principal component analysis may fail to estimate the common factors and factor loadings consistently, and the estimated residuals contain information about the structural changes. This allows us to compare the DFT of the estimated residuals weighted by the estimated common factors with the null (zero) spectrum implied by no structural change. The proposed test is powerful against both smooth structural changes and abrupt structural breaks with an unknown number of breaks and unknown break dates in factor loadings. It can detect a class of local alternatives at the rate N−1/2T−1/2, where N and T are the numbers of cross-sectional units and time periods, respectively. Monte Carlo studies demonstrate that the proposed test has reasonable size and excellent power in detecting various structural changes in factor loadings. When applied to the U.S. macroeconomic data, the test reveals significant and robust evidence of time-varying factor loadings for the post-Great Moderation sample and the pre-Great Recession subsample, which the existing literature may fail to address.",http://www.sciencedirect.com/science/article/pii/S0304407622001270
Journal of Econometrics,2023,Cluster-robust inference: A guide to empirical practice,"James MacKinnon, Morten Nielsen and Matthew Webb","Methods for cluster-robust inference are routinely used in economics and many other disciplines. However, it is only recently that theoretical foundations for the use of these methods in many empirically relevant situations have been developed. In this paper, we use these theoretical results to provide a guide to empirical practice. We do not attempt to present a comprehensive survey of the (very large) literature. Instead, we bridge theory and practice by providing a thorough guide on what to do and why, based on recently available econometric theory and simulation evidence. To practice what we preach, we include an empirical analysis of the effects of the minimum wage on labor supply of teenagers using individual data.",http://www.sciencedirect.com/science/article/pii/S0304407622000781
Journal of Econometrics,2023,Fully modified least squares cointegrating parameter estimation in multicointegrated systems,Igor L. Kheifets and Peter Phillips,"Multicointegration is traditionally defined as a particular long run relationship among variables in a parametric vector autoregressive model that introduces additional cointegrating links between these variables and partial sums of the equilibrium errors. This paper departs from the parametric model, using a semiparametric formulation that reveals the explicit role that singularity of the long run conditional covariance matrix plays in determining multicointegration. The semiparametric framework has the advantage that short run dynamics do not need to be modeled and estimation by standard techniques such as fully modified least squares (FM-OLS) on the original I1 system is straightforward. The paper derives FM-OLS limit theory in the multicointegrated setting, showing how faster rates of convergence are achieved in the direction of singularity and that the limit distribution depends on the distribution of the conditional one-sided long run covariance estimator used in FM-OLS estimation. Wald tests of restrictions on the regression coefficients have nonstandard limit theory which depends on nuisance parameters in general. The usual tests are shown to be conservative when the restrictions are isolated to the directions of singularity and, under certain conditions, are invariant to singularity otherwise. Simulations show that approximations derived in the paper work well in finite samples. The findings are illustrated empirically in an analysis of fiscal sustainability of the US government over the post-war period.",http://www.sciencedirect.com/science/article/pii/S030440762100186X
Journal of Econometrics,2023,High dimensional semiparametric moment restriction models,"Chaohua Dong, Jiti Gao and Oliver Linton","We consider nonlinear moment restriction semiparametric models where both the dimension of the parameter vector and the number of restrictions are divergent with sample size and an unknown smooth function is involved. We propose an estimation method based on the sieve generalized method of moments (sieve-GMM). We establish consistency and asymptotic normality for the estimated quantities when the number of parameters increases modestly with sample size. We also consider the case where the number of potential parameters/covariates is very large, i.e., increases rapidly with sample size, but the true model exhibits sparsity. We use a penalized sieve GMM approach to select the relevant variables, and establish the oracle property of our method in this case. We also provide new results for inference. We propose several new test statistics for the over-identification and establish their large sample properties. We provide a simulation study and an application to data from the NLSY79 used by Carneiro et al. (2011).",http://www.sciencedirect.com/science/article/pii/S0304407621001883
Journal of Econometrics,2023,Second-order refinements for t-ratios with many instruments,Yukitoshi Matsushita and Taisuke Otsu,"This paper studies second-order properties of the many instruments robust t-ratios based on the limited information maximum likelihood and Fuller estimators for instrumental variable regression models with homoskedastic errors under the many instruments asymptotics, where the number of instruments may increase proportionally with the sample size n, and proposes second-order refinements to the t-ratios to improve the size and power properties. Based on asymptotic expansions of the null and non-null distributions of the t-ratios derived under the many instruments asymptotics, we show that the second-order terms of those expansions may have non-trivial impacts on the size as well as the power properties. Furthermore, we propose adjusted t-ratios whose approximation errors for the null rejection probabilities are of order O(n−1) in contrast to the ones for the unadjusted t-ratios of order O(n−1/2), and show that these adjustments induce some desirable power properties in terms of the local maximinity. Although these results are derived under homoskedastic errors, we also establish a stochastic expansion for a heteroskedasticity robust t-ratio, and propose an analogous adjustment under slight deviations from homoskedasticity.",http://www.sciencedirect.com/science/article/pii/S0304407621001901
Journal of Econometrics,2023,Smoothed quantile regression with large-scale inference,"Xuming He, Xiaoou Pan, Kean Ming Tan and Wen-Xin Zhou","Quantile regression is a powerful tool for learning the relationship between a response variable and a multivariate predictor while exploring heterogeneous effects. This paper focuses on statistical inference for quantile regression in the “increasing dimension” regime. We provide a comprehensive analysis of a convolution smoothed approach that achieves adequate approximation to computation and inference for quantile regression. This method, which we refer to as conquer, turns the non-differentiable check function into a twice-differentiable, convex and locally strongly convex surrogate, which admits fast and scalable gradient-based algorithms to perform optimization, and multiplier bootstrap for statistical inference. Theoretically, we establish explicit non-asymptotic bounds on estimation and Bahadur–Kiefer linearization errors, from which we show that the asymptotic normality of the conquer estimator holds under a weaker requirement on dimensionality than needed for conventional quantile regression. The validity of multiplier bootstrap is also provided. Numerical studies confirm conquer as a practical and reliable approach to large-scale inference for quantile regression. Software implementing the methodology is available in the R package conquer.",http://www.sciencedirect.com/science/article/pii/S0304407621001950
Journal of Econometrics,2023,Modeling and forecasting realized volatility with the fractional Ornstein–Uhlenbeck process,"Xiaohu Wang, Weilin Xiao and Jun Yu","This paper proposes to model and forecast realized volatility (RV) using the fractional Ornstein–Uhlenbeck (fO–U) process with a general Hurst parameter, H. A two-stage method is introduced for estimating parameters in the fO–U process based on discrete-sampled observations. In the first stage, H is estimated based on the ratio of two second-order differences of observations from different frequencies. In the second stage, with the estimated H, the other parameters of the model are estimated by the method of moments. All estimators have closed-form expressions and are easy to implement. A large sample theory of the proposed estimators is derived. Extensive simulations show that the proposed estimators and the large-sample theory perform well in finite samples. We apply the model and the method to the logarithmic daily RV series of various financial assets. Our empirical findings suggest that H is much smaller than 1/2, indicating that the RV series have rough sample paths, and that the mean reversion parameter takes a small positive number, indicating that the RV series are stationary but have slow mean reversion. The proposed model is compared with many alternative models, including the fractional Brownian motion, ARFIMA, and HAR, in forecasting RV and logarithmic RV.",http://www.sciencedirect.com/science/article/pii/S0304407621002037
Journal of Econometrics,2023,A discrete-time hedging framework with multiple factors and fat tails: On what matters,"Maciej Augustyniak, Alexandru Badescu and Jean-François Bégin","This article presents a quadratic hedging framework for a general class of discrete-time affine multi-factor models and investigates the extent to which multi-component volatility factors, fat tails, and a non-monotonic pricing kernel can improve the hedging performance. A semi-explicit hedging formula is derived for our general framework which applies to a myriad of the option pricing models proposed in the discrete-time literature. We conduct an extensive empirical study of the impact of modelling features on the hedging effectiveness of S&P 500 options. Overall, we find that fat tails can be credited for half of the hedging improvement observed, while a second volatility factor and a non-monotonic pricing kernel each contribute to a quarter of this improvement. Moreover, our study indicates that the added value of these features for hedging is different than for pricing. A robustness analysis shows that a similar conclusion can be reached when considering the Dow Jones Industrial Average. Finally, the use of a hedging-based loss function in the estimation process is investigated in an additional robustness test, and this choice has a rather marginal impact on hedging performance.",http://www.sciencedirect.com/science/article/pii/S0304407621002049
Journal of Econometrics,2023,Estimating the variance of a combined forecast: Bootstrap-based approach,Ulrich Hounyo and Kajal Lahiri,"This paper considers bootstrap inference in model averaging for predictive regressions. We first show that the standard pairwise bootstrap is not valid in the context of model averaging. This common bootstrap approach induces a bias-related term in the bootstrap variance of averaging estimators. We then propose and justify a fixed-design residual-based bootstrap resampling approach for model averaging. In a local asymptotic framework, we show the validity of the bootstrap in estimating the variance of a combined forecast and the asymptotic covariance matrix of a combined parameter vector with fixed weights. Our proposed method preserves non-parametrically the cross-sectional dependence between different models and the time series dependence in the errors simultaneously. The finite sample performance of these methods is assessed via Monte Carlo simulations. We illustrate our approach using an empirical study of the Taylor rule equation with 24 alternative specifications.",http://www.sciencedirect.com/science/article/pii/S0304407621002244
Journal of Econometrics,2023,When bias contributes to variance: True limit theory in functional coefficient cointegrating regression,Peter Phillips and Ying Wang,"Limit distribution theory in the econometric literature for functional coefficient cointegrating regression is incorrect in important ways, influencing rates of convergence, distributional properties, and practical work. The correct limit theory reveals that components from both bias and variance terms contribute to variability in the asymptotics. The errors in the literature arise because random variability in the bias term has been neglected in earlier research. In stationary regression this random variability is of smaller order and can be ignored in asymptotic analysis but not without consequences for finite sample performance. Implications of the findings for rate efficient estimation are discussed. Simulations in the Online Supplement provide further evidence supporting the new limit theory in nonstationary functional coefficient regressions.",http://www.sciencedirect.com/science/article/pii/S0304407621002190
Journal of Econometrics,2023,Relaxing conditional independence in an endogenous binary response model,Alyssa Carlson,"For binary response models, the literature primarily addresses endogeneity by a control function approach assuming conditional independence (CF-CI). However, as the literature also notes, CF-CI implies conditions like homoskedasticity (of the latent error with respect to the instruments) that fail in many empirical settings. I propose an alternative approach that allows for heteroskedasticity, achieving identification with a conditional mean restriction. These identification results apply to a latent Gaussian error term with flexibly parametrized heteroskedasticity. I propose a two step conditional maximum likelihood estimator and derive its asymptotic distribution. In simulations, the new estimator outperforms others when CF-CI fails and is fairly robust to distributional misspecification.",http://www.sciencedirect.com/science/article/pii/S030440762100230X
Journal of Econometrics,2023,Scalable inference for a full multivariate stochastic volatility model,"Petros Dellaportas, Michalis K. Titsias, Katerina Petrova and Anastasios Plataniotis","We introduce a multivariate stochastic volatility model that imposes no restrictions on the structure of the volatility matrix and treats all its elements as functions of latent stochastic processes. Inference is achieved via a carefully designed feasible and scalable MCMC that has quadratic, rather than cubic, computational complexity for evaluating the multivariate normal densities required. We illustrate how our model can be applied on macroeconomic applications through a stochastic volatility VAR model, comparing it to competing approaches in the literature. We also demonstrate how our approach can be applied to a large dataset containing 571 stock daily returns of Euro STOXX index.",http://www.sciencedirect.com/science/article/pii/S030440762100227X
Journal of Econometrics,2023,"A simple joint model for returns, volatility and volatility of volatility","Ding, Yashuang (Dexter)","We propose a model that allows for conditional heteroskedasticity in the volatility of asset returns and incorporates current return information into the volatility nowcast and forecast. Our model can capture all stylised facts of asset returns even with Gaussian innovations and is simple to implement. Moreover, we show that our model converges weakly to the GARCH-type diffusion as the length of the discrete time intervals between observations goes to zero. Empirical evidence shows that our model has a better fit, a more efficient parameter estimator as well as more accurate volatility and VaR forecasts than other common GARCH-type models.",http://www.sciencedirect.com/science/article/pii/S0304407621002268
Journal of Econometrics,2023,Testing and support recovery of correlation structures for matrix-valued observations with an application to stock market data,"Xin Chen, Dan Yang, Yan Xu, Yin Xia, Dong Wang and Haipeng Shen","Estimation of the covariance matrix of asset returns is crucial to portfolio construction. As suggested by economic theories, the correlation structure among assets differs between emerging markets and developed countries. It is therefore imperative to make rigorous statistical inference on correlation matrix equality between the two groups of countries. However, if the traditional vector-valued approach is undertaken, such inference is either infeasible due to limited number of countries comparing to the relatively abundant assets, or invalid due to the violations of temporal independence assumption. This highlights the necessity of treating the observations as matrix-valued rather than vector-valued. With matrix-valued observations, our problem of interest can be formulated as statistical inference on covariance structures under sub-Gaussian distributions, i.e., testing non-correlation and correlation equality, as well as the corresponding support estimations. We develop procedures that are asymptotically optimal under some regularity conditions. Simulation results demonstrate the computational and statistical advantages of our procedures over certain existing state-of-the-art methods for both normal and non-normal distributions. Application of our procedures to stock market data reveals interesting patterns and validates several economic propositions via rigorous statistical testing.",http://www.sciencedirect.com/science/article/pii/S0304407621002281
Journal of Econometrics,2023,Why randomize? Minimax optimality under permutation invariance,Yuehao Bai,"This paper studies finite sample minimax optimal randomization schemes and estimation schemes in estimating parameters including the average treatment effect, when treatment effects are heterogeneous. A randomization scheme is a distribution over a group of permutations of a given treatment assignment vector. An estimation scheme is a joint distribution over assignment vectors, linear estimators, and permutations of assignment vectors. The key element in the minimax problem is that the worst case is over a class of distributions of the data which is invariant to a group of permutations. First, I show that given any assignment vector and any estimator, the uniform distribution over the same group of permutations, namely the complete randomization scheme, is minimax optimal. Second, under further assumptions on the class of distributions and the objective function, I show the minimax optimal estimation scheme involves completely randomizing an assignment vector, while the optimal estimator is the difference-in-means under complete invariance and a weighted average of within-block differences under a block structure, and the number of treated units is determined by the Neyman allocation.",http://www.sciencedirect.com/science/article/pii/S0304407621002566
Journal of Econometrics,2023,"Identification of time-varying transformation models with fixed effects, with an application to unobserved heterogeneity in resource shares","Irene Botosaru, Chris Muris and Krishna Pendakur","We provide new results showing identification of a large class of fixed-T panel models, where the response variable is an unknown, weakly monotone, time-varying transformation of a latent linear index of fixed effects, regressors, and an error term drawn from an unknown stationary distribution. Our results identify the transformation, the coefficient on regressors, and features of the distribution of the fixed effects. We then develop a full-commitment intertemporal collective household model, where the implied quantity demand equations are time-varying functions of a linear index. The fixed effects in this index equal logged resource shares, defined as the fractions of household expenditure enjoyed by each household member. Using Bangladeshi data, we show that women’s resource shares decline with household budgets and that half of the variation in women’s resource shares is due to unobserved household-level heterogeneity.",http://www.sciencedirect.com/science/article/pii/S0304407621002633
Journal of Econometrics,2023,Time series analysis of COVID-19 infection curve: A change-point perspective,"Feiyu Jiang, Zifeng Zhao and Xiaofeng Shao","In this paper, we model the trajectory of the cumulative confirmed cases and deaths of COVID-19 (in log scale) via a piecewise linear trend model. The model naturally captures the phase transitions of the epidemic growth rate via change-points and further enjoys great interpretability due to its semiparametric nature. On the methodological front, we advance the nascent self-normalization (SN) technique (Shao, 2010) to testing and estimation of a single change-point in the linear trend of a nonstationary time series. We further combine the SN-based change-point test with the NOT algorithm (Baranowski et al., 2019) to achieve multiple change-point estimation. Using the proposed method, we analyze the trajectory of the cumulative COVID-19 cases and deaths for 30 major countries and discover interesting patterns with potentially relevant implications for effectiveness of the pandemic responses by different countries. Furthermore, based on the change-point detection algorithm and a flexible extrapolation function, we design a simple two-stage forecasting scheme for COVID-19 and demonstrate its promising performance in predicting cumulative deaths in the U.S.",http://www.sciencedirect.com/science/article/pii/S0304407620302633
Journal of Econometrics,2023,Nowcasting the output gap,"Tino Berger, James Morley and Benjamin Wong","We propose a way to directly nowcast the output gap using the Beveridge–Nelson decomposition based on a mixed-frequency Bayesian VAR. The mixed-frequency approach produces similar but more timely estimates of the U.S. output gap compared to those based on a quarterly model, the CBO measure of potential, or the HP filter. We find that within-quarter nowcasts for the output gap are more reliable than for output growth, with monthly indicators for a credit risk spread, consumer sentiment, and the unemployment rate providing particularly useful new information about the final estimate of the output gap. An out-of-sample analysis of the COVID-19 crisis anticipates the exceptionally large negative output gap of −8.3% in 2020Q2 before the release of real GDP data for the quarter, with both conditional and scenario nowcasts tracking a dramatic decline in the output gap given the April data.",http://www.sciencedirect.com/science/article/pii/S0304407620303523
Journal of Econometrics,2023,Time varying Markov process with partially observed aggregate data: An application to coronavirus,C. Gourieroux and Joann Jasiak,"A major difficulty in the analysis of Covid-19 transmission is that many infected individuals are asymptomatic. For this reason, the total counts of infected individuals and of recovered immunized individuals are unknown, especially during the early phase of the epidemic. In this paper, we consider a parametric time varying Markov process of Coronavirus transmission and show how to estimate the model parameters and approximate the unobserved counts from daily data on infected and detected individuals and the total daily death counts. This model-based approach is illustrated in an application to French data, performed on April 6, 2020.",http://www.sciencedirect.com/science/article/pii/S0304407620303791
Journal of Econometrics,2023,Nowcasting in a pandemic using non-parametric mixed frequency VARs,"Florian Huber, Gary Koop, Luca Onorante, Michael Pfarrhofer and Josef Schreiner","This paper develops Bayesian econometric methods for posterior inference in non-parametric mixed frequency VARs using additive regression trees. We argue that regression tree models are ideally suited for macroeconomic nowcasting in the face of extreme observations, for instance those produced by the COVID-19 pandemic of 2020. This is due to their flexibility and ability to model outliers. In an application involving four major euro area countries, we find substantial improvements in nowcasting performance relative to a linear mixed frequency VAR.",http://www.sciencedirect.com/science/article/pii/S0304407620303936
Journal of Econometrics,2023,How to go viral: A COVID-19 model with endogenously time-varying parameters,"Paul Ho, Thomas A. Lubik and Christian Matthes","We estimate a panel model with endogenously time-varying parameters for COVID-19 cases and deaths in U.S. states. The functional form for infections incorporates important features of epidemiological models but is flexibly parameterized to capture different trajectories of the pandemic. Daily deaths are modeled as a spike-and-slab regression on lagged cases. Our Bayesian estimation reveals that social distancing and testing have significant effects on the parameters. For example, a 10 percentage point increase in the positive test rate is associated with a 2 percentage point increase in the death rate among reported cases. The model forecasts perform well, even relative to models from epidemiology and statistics.",http://www.sciencedirect.com/science/article/pii/S0304407621000105
Journal of Econometrics,2023,Nonparametric comparison of epidemic time trends: The case of COVID-19,Marina Khismatullina and Michael Vogt,"The COVID-19 pandemic is one of the most pressing issues at present. A question which is particularly important for governments and policy makers is the following: Does the virus spread in the same way in different countries? Or are there significant differences in the development of the epidemic? In this paper, we devise new inference methods that allow to detect differences in the development of the COVID-19 epidemic across countries in a statistically rigorous way. In our empirical study, we use the methods to compare the outbreak patterns of the epidemic in a number of European countries.",http://www.sciencedirect.com/science/article/pii/S030440762100155X
Journal of Econometrics,2023,Who should get vaccinated? Individualized allocation of vaccines over SIR network,Toru Kitagawa and Guanyi Wang,"How to allocate vaccines over heterogeneous individuals is one of the important policy decisions in pandemic times. This paper develops a procedure to estimate an individualized vaccine allocation policy under limited supply, exploiting social network data containing individual demographic characteristics and health status. We model the spillover effects of vaccination based on a Heterogeneous-Interacted-SIR network model and estimate an individualized vaccine allocation policy by maximizing an estimated social welfare (public health) criterion incorporating these spillovers. While this optimization problem is generally an NP-hard integer optimization problem, we show that the SIR structure leads to a submodular objective function, and provide a computationally attractive greedy algorithm for approximating a solution that has a theoretical performance guarantee. Moreover, we characterize a finite sample welfare regret bound and examine how its uniform convergence rate depends on the complexity and riskiness of the social network. In the simulation, we illustrate the importance of considering spillovers by comparing our method with targeting without network information.",http://www.sciencedirect.com/science/article/pii/S0304407621002219
Journal of Econometrics,2023,Sparse spatio-temporal autoregressions by profiling and bagging,"Yingying Ma, Shaojun Guo and Hansheng Wang","We consider a new class of spatio-temporal models with sparse autoregressive coefficient matrices and exogenous variable. To estimate the model, we first profile the exogenous variable out of the response. This leads to a profiled model structure. Next, to overcome endogeneity issue, we propose a class of generalized methods of moment (GMM) estimators to estimate the autoregressive coefficient matrices. A novel bagging-based estimator is further developed to conquer the over-determined issue which also occurs in Chang et al. (2015) and Dou et al. (2016). An adaptive forward–backward greedy algorithm is proposed to learn the sparse structure of the autoregressive coefficient matrices. A new BIC-type selection criteria is further developed to conduct variable selection for GMM estimators. Asymptotic properties are further studied. The proposed methodology is illustrated with extensive simulation studies. A social network dataset is analyzed for illustration purpose.",http://www.sciencedirect.com/science/article/pii/S030440762100035X
Journal of Econometrics,2023,Efficient closed-form estimation of large spatial autoregressions,Abhimanyu Gupta,"Newton-step approximations to pseudo maximum likelihood estimates of spatial autoregressive models with a large number of parameters are examined, in the sense that the parameter space grows slowly as a function of sample size. These have the same asymptotic efficiency properties as maximum likelihood under Gaussianity but are of closed form. Hence they are computationally simple and free from compactness assumptions, thereby avoiding two notorious pitfalls of implicitly defined estimates of large spatial autoregressions. When commencing from an initial least squares estimate, the Newton step can also lead to weaker regularity conditions for a central limit theorem than some extant in the literature. A simulation study demonstrates excellent finite sample gains from Newton iterations, especially in large multiparameter models for which grid search is costly. A small empirical illustration shows improvements in estimation precision with real data.",http://www.sciencedirect.com/science/article/pii/S0304407621001597
Journal of Econometrics,2023,Spatial econometrics for misaligned data,Guillaume Allaire Pouliot,"We produce methodology for regression analysis when the geographic locations of the independent and dependent variables do not coincide, in which case we speak of misaligned data. We develop and investigate two complementary methods for regression analysis with misaligned data that circumvent the need to estimate or specify the covariance of the regression errors. We carry out a detailed reanalysis of Maccini and Yang (2009) and find economically significant quantitative differences but sustain most qualitative conclusions.",http://www.sciencedirect.com/science/article/pii/S0304407621001627
Journal of Econometrics,2023,A spatial panel quantile model with unobserved heterogeneity,"Tomohiro Ando, Kunpeng Li and Lina Lu","This paper introduces a spatial panel quantile model with unobserved heterogeneity. The proposed model is capable of capturing high-dimensional cross-sectional dependence and allows heterogeneous regression coefficients. For estimating model parameters, a new estimation procedure is proposed. When both the time and cross-sectional dimensions of the panel go to infinity, the uniform consistency and the asymptotic normality of the estimated parameters are established. In order to determine the dimension of the interactive fixed effects, we propose a new information criterion. It is shown that the criterion asymptotically selects the true dimension. Monte Carlo simulations document the satisfactory performance of the proposed method. Finally, the method is applied to study the quantile co-movement structure of the U.S. stock market by taking into account the input–output linkages as firms are connected through the input–output production network.",http://www.sciencedirect.com/science/article/pii/S0304407621002323
Journal of Econometrics,2023,Estimation of spatial sample selection models: A partial maximum likelihood approach,Renata Rabovič and Pavel Cizek,"We study estimation of sample selection models with the spatially lagged latent dependent variable or spatial errors in both the selection and outcome equations under cross-sectional dependence. Since there is no estimation framework for the spatial-lag model and the existing estimators for the spatial-error model are computationally demanding or have poor small sample properties, we suggest to estimate these models by the partial maximum likelihood estimator. We show that the estimator is consistent and asymptotically normally distributed. To facilitate easy and precise estimation of the variance matrix, we propose the parametric bootstrap method. Simulations demonstrate the advantages of the estimators.",http://www.sciencedirect.com/science/article/pii/S0304407621002815
Journal of Econometrics,2023,Higher-order least squares inference for spatial autoregressions,Francesca Rossi and Peter M. Robinson,"We develop refined inference for spatial regression models with predetermined regressors. The ordinary least squares estimate of the spatial parameter is neither consistent nor asymptotically normal, unless the elements of the spatial weight matrix uniformly vanish as sample size diverges. We develop refined testing of the hypothesis of no spatial dependence, without requiring such negligibility of spatial weights, by formal Edgeworth expansions. We also develop such higher-order expansions for both an unstudentized and a studentized transformed estimate, where the studentized one can be used to provide refined interval estimates. A Monte Carlo study of finite sample performance is included.",http://www.sciencedirect.com/science/article/pii/S0304407622000458
