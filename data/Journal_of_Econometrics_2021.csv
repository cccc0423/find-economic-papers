journal,year,title,authors,abstract,url
Journal of Econometrics,2021,Identification in nonparametric models for dynamic treatment effects,Sukjin Han,"This paper develops a nonparametric model that represents how sequences of outcomes and treatment choices influence one another in a dynamic manner. In this setting, we are interested in identifying the average outcome for individuals in each period, had a particular treatment sequence been assigned. The identification of this quantity allows us to identify the average treatment effects (ATE’s) and the ATE’s on transitions, as well as the optimal treatment regimes, namely, the regimes that maximize the (weighted) sum of the average potential outcomes, possibly less the cost of the treatments. The main contribution of this paper is to relax the sequential randomization assumption widely used in the biostatistics literature by introducing a flexible choice-theoretic framework for a sequence of endogenous treatments. This framework allows non-compliance of subjects in experimental studies or endogenous treatment decisions in observational settings. We show that the parameters of interest are identified under each period’s exclusion restrictions, which are motivated by, e.g., a sequence of randomized treatment assignments or encouragements and a behavioral/information assumption on agents who receive treatments.",http://www.sciencedirect.com/science/article/pii/S0304407620303717
Journal of Econometrics,2021,Permutation test for heterogeneous treatment effects with a nuisance parameter,EunYi Chung and Mauricio Olivares,"This paper proposes an asymptotically valid permutation test for heterogeneous treatment effects in the presence of an estimated nuisance parameter. Not accounting for the estimation error of the nuisance parameter results in statistics that depend on the particulars of the data generating process, and the resulting permutation test fails to control the Type 1 error, even asymptotically.",http://www.sciencedirect.com/science/article/pii/S0304407621001561
Journal of Econometrics,2021,Estimating dynamic treatment effects in event studies with heterogeneous treatment effects,Liyang Sun and Sarah Abraham,"To estimate the dynamic effects of an absorbing treatment, researchers often use two-way fixed effects regressions that include leads and lags of the treatment. We show that in settings with variation in treatment timing across units, the coefficient on a given lead or lag can be contaminated by effects from other periods, and apparent pretrends can arise solely from treatment effects heterogeneity. We propose an alternative estimator that is free of contamination, and illustrate the relative shortcomings of two-way fixed effects regressions with leads and lags through an empirical application.",http://www.sciencedirect.com/science/article/pii/S030440762030378X
Journal of Econometrics,2021,Difference-in-Differences with multiple time periods,"Brantly Callaway and Sant’Anna, Pedro H.C.","In this article, we consider identification, estimation, and inference procedures for treatment effect parameters using Difference-in-Differences (DiD) with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the “parallel trends assumption” holds potentially only after conditioning on observed covariates. We show that a family of causal effect parameters are identified in staggered DiD setups, even if differences in observed characteristics create non-parallel outcome dynamics between groups. Our identification results allow one to use outcome regression, inverse probability weighting, or doubly-robust estimands. We also propose different aggregation schemes that can be used to highlight treatment effect heterogeneity across different dimensions as well as to summarize the overall effect of participating in the treatment. We establish the asymptotic properties of the proposed estimators and prove the validity of a computationally convenient bootstrap procedure to conduct asymptotically valid simultaneous (instead of pointwise) inference. Finally, we illustrate the relevance of our proposed tools by analyzing the effect of the minimum wage on teen employment from 2001–2007. Open-source software is available for implementing the proposed methods.",http://www.sciencedirect.com/science/article/pii/S0304407620303948
Journal of Econometrics,2021,The identification region of the potential outcome distributions under instrument independence,Toru Kitagawa,"This paper examines the identifying power of instrument exogeneity in the treatment effect model. We derive the identification region of the potential outcome distributions, which are the collection of distributions that are compatible with data and with the restrictions of the model. We consider identification when (i) the instrument is independent of each of the potential outcomes (marginal independence), (ii) the instrument is independent of the potential outcomes and selection heterogeneity jointly (joint independence), and (iii.) the instrument satisfies joint independence and monotonicity (the LATE restriction). By comparing the size of the identification region under each restriction, we show that joint independence provides more identifying information for the potential outcome distributions than marginal independence, but that the LATE restriction provides no identification gain beyond joint independence. We also, under each restriction, derive sharp bounds for the Average Treatment Effect and sharp testable implication to falsify the restriction. Our analysis covers discrete or continuous outcomes, and extends the Average Treatment Effect bounds of Balke and Pearl (1997) developed for the dichotomous outcome case to a more general setting.",http://www.sciencedirect.com/science/article/pii/S0304407621000968
Journal of Econometrics,2021,Difference-in-differences with variation in treatment timing,Andrew Goodman-Bacon,"The canonical difference-in-differences (DD) estimator contains two time periods, ”pre” and ”post”, and two groups, ”treatment” and ”control”. Most DD applications, however, exploit variation across groups of units that receive treatment at different times. This paper shows that the two-way fixed effects estimator equals a weighted average of all possible two-group/two-period DD estimators in the data. A causal interpretation of two-way fixed effects DD estimates requires both a parallel trends assumption and treatment effects that are constant over time. I show how to decompose the difference between two specifications, and provide a new analysis of models that include time-varying controls.",http://www.sciencedirect.com/science/article/pii/S0304407621001445
Journal of Econometrics,2021,Covariate-adjusted Fisher randomization tests for the average treatment effect,Anqi Zhao and Peng Ding,"Fisher’s randomization test (frt) delivers exact p-values under the strong null hypothesis of no treatment effect on any units whatsoever and allows for flexible covariate adjustment to improve the power. Of interest is whether the resulting covariate-adjusted procedure could also be valid for testing the weak null hypothesis of zero average treatment effect. To this end, we evaluate two general strategies for conducting covariate adjustment in frts: the pseudo-outcome strategy that uses the residuals from an outcome model with only the covariates as the pseudo, covariate-adjusted outcomes to form the test statistic, and the model-output strategy that directly uses the output from an outcome model with both the treatment and covariates as the covariate-adjusted test statistic. Based on theory and simulation, we recommend using the ordinary least squares (ols) fit of the observed outcome on the treatment, centered covariates, and their interactions for covariate adjustment, and conducting frt with the robust t-value of the treatment as the test statistic. The resulting frt is finite-sample exact for testing the strong null hypothesis, asymptotically valid for testing the weak null hypothesis, and more powerful than the unadjusted counterpart under alternatives, all irrespective of whether the linear model is correctly specified or not. We start with complete randomization, and then extend the theory to cluster randomization, stratified randomization, and rerandomization, respectively, giving a recommendation for the test procedure and test statistic under each design. Our theory is design-based, also known as randomization-based, in which we condition on the potential outcomes but average over the random treatment assignment.",http://www.sciencedirect.com/science/article/pii/S0304407621001457
Journal of Econometrics,2021,Matching estimators with few treated and many control observations,Bruno Ferman,"We analyze the properties of matching estimators when there are few treated, but many control observations. We show that, under standard assumptions, the nearest neighbor matching estimator for the average treatment effect on the treated is asymptotically unbiased in this framework. However, when the number of treated observations is fixed, the estimator is not consistent, and it is generally not asymptotically normal. Since standard inference methods are inadequate, we propose alternative inference methods, based on the theory of randomization tests under approximate symmetry, that are asymptotically valid in this framework. We show that these tests are valid under relatively strong assumptions when the number of treated observations is fixed, and under weaker assumptions when the number of treated observations increases, but at a lower rate relative to the number of control observations.",http://www.sciencedirect.com/science/article/pii/S0304407621001895
Journal of Econometrics,2021,Detecting groups in large vector autoregressions,Guðmundur Guðmundsson and Christian Brownlees,"This work introduces the stochastic block vector autoregressive (SB-VAR) model. In this class of vector autoregressions, the time series are partitioned into latent groups such that spillover effects are stronger among series that belong to the same group than otherwise. A key question that arises in this framework is how to detect the latent groups from a sample of observations generated by the model. To this end, we propose a group detection algorithm based on the eigenvectors of a function of the estimated autoregressive matrices. We establish that the proposed algorithm consistently detects the groups when the cross-sectional and time-series dimensions are sufficiently large. The methodology is applied to study the group structure of a panel of risk measures of top financial institutions in the United States and a panel of word counts extracted from Twitter.",http://www.sciencedirect.com/science/article/pii/S030440762100124X
Journal of Econometrics,2021,Identification of structural vector autoregressions through higher unconditional moments,Alain Guay,"This paper pursues two objectives. First, we determine the sufficient condition for local, statistical identification of SVAR processes through the third and fourth unconditional moments of the reduced-form innovations. Our findings provide novel insights when the entire system is not identified, as they highlight which subset of structural parameters is identified and which is not. Second, we elaborate a tractable testing procedure to verify whether the identification condition holds, prior to the estimation of the structural parameters of the SVAR process. To do so, we design a new bootstrap procedure that improves the small-sample properties of rank tests for the symmetry and kurtosis of the structural shocks.",http://www.sciencedirect.com/science/article/pii/S0304407620303651
Journal of Econometrics,2021,Using time-varying volatility for identification in Vector Autoregressions: An application to endogenous uncertainty,"Andrea Carriero, Todd Clark and Massimiliano Marcellino","We develop a structural vector autoregression with stochastic volatility in which one of the variables can impact both the mean and the variance of the other variables. We provide conditional posterior distributions for this model, develop an MCMC algorithm for estimation, and show how stochastic volatility can be used to provide useful restrictions for the identification of structural shocks. We then use the model with US data to show that some variables have a significant contemporaneous feedback effect on macroeconomic uncertainty, and overlooking this channel can lead to distortions in the estimated effects of uncertainty on the economy.",http://www.sciencedirect.com/science/article/pii/S0304407621001858
Journal of Econometrics,2021,Inference in Structural Vector Autoregressions identified with an external instrument,"José L. Montiel Olea, James H. Stock and Mark W. Watson","This paper studies Structural Vector Autoregressions in which a structural shock of interest (e.g., an oil supply shock) is identified using an external instrument. The external instrument is taken to be correlated with the target shock (the instrument is relevant) and to be uncorrelated with other shocks of the model (the instrument is exogenous). The potential weak correlation between the external instrument and the target structural shock compromises the large-sample validity of standard inference. We suggest a confidence set for impulse response coefficients that is not affected by the instrument strength (i.e., is weak-instrument robust) and asymptotically coincides with the standard confidence set when the instrument is strong.",http://www.sciencedirect.com/science/article/pii/S0304407620302311
Journal of Econometrics,2021,Inference in Bayesian Proxy-SVARs,"Jonas E. Arias, Juan F Rubio-Ramirez and Daniel Waggoner","Motivated by the increasing use of external instruments to identify structural vector autoregressions (SVARs), we develop an algorithm for exact finite sample inference in this class of time series models, commonly known as Proxy-SVARs. Our algorithm makes independent draws from any posterior distribution over the structural parameterization of a Proxy-SVAR. Our approach allows researchers to simultaneously use proxies and traditional zero and sign restrictions to identify structural shocks. We illustrate our methods with two applications. In particular, we show how to generalize the counterfactual analysis in Mertens and Montiel-Olea (2018) to identified structural shocks.",http://www.sciencedirect.com/science/article/pii/S0304407620303985
Journal of Econometrics,2021,Impulse response analysis for structural dynamic models with nonlinear regressors,"Silvia Goncalves, Ana María Herrera, Lutz Kilian and Elena Pesavento","We study the construction of nonlinear impulse responses in linear structural dynamic models that include nonlinearly transformed regressors. We derive the closed-form solution for the population impulse responses to a given shock and propose a control function approach to estimating these responses without taking a stand on how the remainder of the model is identified. Our plug-in estimator dispenses with the need for simulations and, unlike conventional local projection (LP) estimators, is consistent. A modified LP estimator is shown to be consistent in special cases, but less accurate in finite samples than the plug-in estimator.",http://www.sciencedirect.com/science/article/pii/S0304407621001810
Journal of Econometrics,2021,Diagnostic tests for homoskedasticity in spatial cross-sectional or panel models,"Badi Baltagi, Alain Pirotte and Zhenlin Yang","We propose an Adjusted Quasi-Score (AQS) method for constructing tests for homoskedasticity in spatial econometric models. We first obtain an AQS function by adjusting the score-type function from the given model to achieve unbiasedness, and then develop an Outer-Product-of-Martingale-Difference (OPMD) estimate of its variance. In standard problems where a genuine (quasi) score vector is available, the AQS–OPMD method leads to finite sample improved tests over the usual methods. More importantly in non-standard problems where a genuine (quasi) score is not available and the usual methods fail, the proposed AQS–OPMD method provides feasible solutions. The AQS tests are formally derived and asymptotic properties examined for three representative models: spatial cross-sectional, static and dynamic panel models. Monte Carlo results show that the proposed AQS tests have good finite sample properties.",http://www.sciencedirect.com/science/article/pii/S0304407620303444
Journal of Econometrics,2021,The medium-run efficiency consequences of unfair school matching: Evidence from Chinese college admissions,Xiaohan Zhong and Lin Zhu,"In this paper, we empirically examine how unfair (i.e. negatively assortive) matching between colleges and students affects the medium-run allocative efficiency of the college admissions system, as measured by total wage levels among college graduates. Using data from China College Student Survey, we find that unfair matching tends to increase the total wage level. The implication is that student ability and school quality tend to substitute for, rather than complement each other. We also find evidence that unfair matching leads to higher total human capital investment in English skills, leadership ability, and double majors, but not to higher GPAs. We interpret this finding as an indication that unfair matching, by increasing the inequality of ability within a college, encourages diversified human capital investment instead of monotonic competition. These findings also indicate possible channels of improvement in allocative efficiency.",http://www.sciencedirect.com/science/article/pii/S0304407620303675
Journal of Econometrics,2021,An empirical total survey error decomposition using data combination,Bruce D. Meyer and Nikolas Mittag,"Survey error is known to be pervasive and to bias even simple, but important, estimates of means, rates, and totals, such as the poverty and the unemployment rate. In order to summarize and analyze the extent, sources, and consequences of survey error, we define empirical counterparts of key components of the Total Survey Error Framework that can be estimated using data combination. Specifically, we estimate total survey error and decompose it into three high level sources of error: generalized coverage error, item non-response error and measurement error. We further decompose these sources into lower level sources such as failure to report a positive amount and errors in amounts conditional on reporting a positive value. For errors in dollars paid by two large government transfer programs, we use administrative records on the universe of program payments in New York State linked to three major household surveys to estimate the error components previously defined. We find that total survey error is large and varies in its size and composition, but measurement error is always by far the largest source of error. Our application shows that data combination makes it possible to routinely measure total survey error and its components. Our results allow survey producers to assess error reduction strategies and survey users to mitigate the consequences of survey errors or gauge the reliability of their conclusions.",http://www.sciencedirect.com/science/article/pii/S0304407620303687
Journal of Econometrics,2021,Adaptive inference for a semiparametric generalized autoregressive conditional heteroskedasticity model,"Feiyu Jiang, Dong Li and Ke Zhu","This paper considers a semiparametric generalized autoregressive conditional heteroskedasticity (S-GARCH) model. For this model, we first estimate the time-varying long run component for unconditional variance by the kernel estimator, and then estimate the non-time-varying parameters in GARCH-type short run component by the quasi maximum likelihood estimator (QMLE). We show that the QMLE is asymptotically normal with the parametric convergence rate. Next, we construct a Lagrange multiplier test for linear parameter constraint and a portmanteau test for model checking, and obtain their asymptotic null distributions. Our entire statistical inference procedure works for the non-stationary data with two important features: first, our QMLE and two tests are adaptive to the unknown form of the long run component; second, our QMLE and two tests share the same efficiency and testing power as those in variance targeting method when the S-GARCH model is stationary.",http://www.sciencedirect.com/science/article/pii/S0304407620303699
Journal of Econometrics,2021,Robust estimation with exponentially tilted Hellinger distance,Bertille Antoine and Prosper Dovonon,"This paper is concerned with estimation of parameters defined by moment equalities. We introduce the exponentially tilted Hellinger distance (ETHD) estimator which is efficient under correct specification, and robust to both global and local misspecification. In the spirit of Schennach (2007), ETHD combines the Hellinger distance and the Kullback–Leibler information criterion. We show that it achieves optimal minimax robust properties under local deviations from the model and remains well-behaved under global misspecification.",http://www.sciencedirect.com/science/article/pii/S0304407620303705
Journal of Econometrics,2021,An econometric model of network formation with an application to board interlocks between firms,Cristina Gualdani,"We study identification of the players’ preferences in a network formation game featuring complete information, nonreciprocal links, and a spillover effect. We decompose the network formation game into local games such that the network formation game is in equilibrium if and only if each local game is in equilibrium. This decomposition helps us prove equilibrium existence, reduce the number of moment inequalities characterising the identified set, and simplify the calculation of the integrals entering those moment inequalities. The developed methodology is used to investigate Italian firms’ incentives for having their executive directors sitting on competitors’ boards.",http://www.sciencedirect.com/science/article/pii/S0304407620303729
Journal of Econometrics,2021,An improved bootstrap test for restricted stochastic dominance,Thomas M. Lok and Rami V. Tabri,"Bootstrap Testing for restricted stochastic dominance of a pre-specified order between two distributions is of interest in many areas of economics. This paper develops a new method for improving the performance of such tests that employ a moment selection procedure: tilting the empirical distribution in the moment selection procedure. We propose that the amount of tilting be chosen to maximize the empirical likelihood subject to the restrictions of the null hypothesis, which are a continuum of unconditional moment inequality conditions. We characterize sets of population distributions on which a modified test is (i) asymptotically equivalent to its non-modified version to first-order, and (ii) superior to its non-modified version according to local power when the sample size is large enough. We report simulation results that show the modified versions of leading tests are noticeably less conservative than their non-modified counterparts and have improved power. Finally, an empirical example is discussed to illustrate the proposed method.",http://www.sciencedirect.com/science/article/pii/S0304407620303730
Journal of Econometrics,2021,Time-varying instrumental variable estimation,"Liudas Giraitis, George Kapetanios and Massimiliano Marcellino","We develop non-parametric instrumental variable estimation and inferential theory for econometric models with possibly endogenous regressors whose coefficients can vary over time either deterministically or stochastically, and the time-varying and uniform versions of the standard Hausman exogeneity test. After deriving the asymptotic properties of the proposed procedures, we assess their finite sample performance by means of a set of Monte Carlo experiments, and illustrate their application by means of an empirical example on the Phillips curve.",http://www.sciencedirect.com/science/article/pii/S0304407620303754
Journal of Econometrics,2021,Robust nonlinear regression estimation in null recurrent time series,"Francesco Bravo, Degui Li and Dag Tjøstheim","In this article, we study parametric robust estimation in nonlinear regression models with regressors generated by a class of non-stationary and null recurrent Markov processes. The nonlinear regression functions can be either integrable or asymptotically homogeneous, covering many commonly-used functional forms in parametric nonlinear regression. Under regularity conditions, we derive both the consistency and limit distribution results for the developed general robust estimators (including the nonlinear least squares, least absolute deviation and Huber’s M-estimators). The convergence rates of the estimation depend on not only the functional form of the nonlinear regression, but also on the recurrence rate of the Markov process. Some Monte-Carlo simulation studies are conducted to examine the numerical performance of the proposed estimators and verify the established asymptotic properties in finite samples. Finally two empirical applications illustrate the usefulness of the proposed robust estimation method.",http://www.sciencedirect.com/science/article/pii/S0304407620303766
Journal of Econometrics,2021,Recursive estimation in large panel data models: Theory and practice,"Bin Jiang, Yanrong Yang, Jiti Gao and Cheng Hsiao","Bai (2009) proposes recursive estimation for panel data models with interactive effects. We study the behaviours of this recursive estimator. The recursive formula is established that shows the behaviours of recursive estimators depend on the initial estimator, the population structure and the iterative steps. Under some general scenarios, we find that the recursive estimator becomes consistent after the first iteration from any initials. We also obtain the optimal number of iterative steps under some prescribed conditions. The central limit theorem of the recursive estimator is established when the initial estimator is OLS. Various simulations are conducted to support our theoretical findings.",http://www.sciencedirect.com/science/article/pii/S0304407620303870
Journal of Econometrics,2021,Continuous record Laplace-based inference about the break date in structural change models,Alessandro Casini and Pierre Perron,"Building upon the continuous record asymptotic framework recently introduced by Casini and Perron (2020a) for inference in structural change models, we propose a Laplace-based (Quasi-Bayes) procedure for the construction of the estimate and confidence set for the date of a structural change. It is defined by an integration rather than an optimization-based method. A transformation of the least-squares criterion function is evaluated in order to derive a proper distribution, referred to as the Quasi-posterior. For a given choice of a loss function, the Laplace-type estimator is the minimizer of the expected risk with the expectation taken under the Quasi-posterior. Besides providing an alternative estimate that is more precise—lower mean absolute error (MAE) and lower root-mean squared error (RMSE)—than the usual least-squares one, the Quasi-posterior distribution can be used to construct asymptotically valid inference using the concept of Highest Density Region. The resulting Laplace-based inferential procedure is shown to have lower MAE and RMSE, and the confidence sets strike a better balance between empirical coverage rates and average lengths of the confidence sets relative to traditional long-span methods, whether the break size is small or large.",http://www.sciencedirect.com/science/article/pii/S030440762030261X
Journal of Econometrics,2021,Statistical tests of a simple energy balance equation in a synthetic model of cotrending and cointegration,Josep Carrion-i-Silvestre and Dukpa Kim,"We develop new tests for the linear relationship between temperature and forcing, which is one of the most studied implications from a simple energy balance model. We consider a bivariate system of temperature and forcing where the time path of well-mixed-greenhouse-gases forcing is included as a potential common trend function in addition to a stochastic trend and a broken linear trend. Our test statistics are first devised as the likelihood ratio and then are modified to remove nuisance parameters in the asymptotic null distribution. The asymptotic null distribution and the required modification differ as to the existence of a stochastic trend. Thus, the test statistics are modified in two different ways and then are combined using the super-efficient estimator of the sum of autoregressive coefficients. The asymptotic critical values from the two cases remain close and we use the bigger one to control size for both cases. The proposed tests are applied to four temperature series and a forcing series. The null hypothesis of the linear relationship is not rejected with conventional sizes.",http://www.sciencedirect.com/science/article/pii/S0304407620303559
Journal of Econometrics,2021,Inference after estimation of breaks,"Isaiah Andrews, Toru Kitagawa and Adam McCloskey","In an important class of econometric problems, researchers select a target parameter by maximizing the Euclidean norm of a data-dependent vector. Examples that can be cast into this frame include threshold regression models with estimated thresholds and structural break models with estimated break dates. Estimation and inference procedures that ignore the randomness of the target parameter can be severely biased and misleading when this randomness is non-negligible. This paper studies conditional and unconditional inference in such settings, accounting for the data-dependent choice of target parameters. We detail the construction of quantile-unbiased estimators and confidence sets with correct coverage, and prove their asymptotic validity under data generating process such that the target parameter remains random in the limit. We also provide a novel sample splitting approach that improves on conventional split-sample inference.",http://www.sciencedirect.com/science/article/pii/S0304407620302591
Journal of Econometrics,2021,Boosting high dimensional predictive regressions with time varying parameters,Kashif Yousuf and Serena Ng,"High dimensional predictive regressions are useful in wide range of applications. However, the theory is mainly developed assuming that the model is stationary with time invariant parameters. This is at odds with the prevalent evidence for parameter instability in economic time series, but theories for parameter instability are mainly developed for models with a small number of covariates. In this paper, we present two L2 boosting algorithms for estimating high dimensional models in which the coefficients are modeled as functions evolving smoothly over time and the predictors are locally stationary. The first method uses componentwise local constant estimators as base learner, while the second relies on componentwise local linear estimators. We establish consistency of both methods, and address the practical issues of choosing the bandwidth for the base learners and the number of boosting iterations. In an extensive application to macroeconomic forecasting with many potential predictors, we find that the benefits to modeling time variation are substantial and they increase with the forecast horizon. Furthermore, the timing of the benefits suggests that the Great Moderation is associated with substantial instability in the conditional mean of various economic series.",http://www.sciencedirect.com/science/article/pii/S0304407620302827
Journal of Econometrics,2021,Sieve estimation of option-implied state price density,Junwen Lu and Zhongjun Qu,"This paper proposes a nonparametric estimator for the state price density implied by a single cross-section of European options with different strikes and the same maturity. The proposed estimator has two distinctive features. First, it extracts information from both call and put options, as opposed to only call options. Second, it does not require estimating any second-order derivative; instead, it solves a constrained and penalized linear regression. The asymptotic analysis faces two challenges because the state price density is defined by a Fredholm integral equation of the first kind with an unbounded support, and the kernel function is unbounded and non-differentiable. We address these challenges by exploiting the structure of the option pricing problem. After establishing the estimator’s consistency and convergence rate, we apply it to estimate the state price densities implied by the S&P500 index options and those by the VIX options. The sample period includes the recent financial crisis and the Great Recession, during which the turbulent market conditions imposed substantial challenges on the estimation. We show that the estimator can work with both daily and high-frequency observations. We also study whether the various features of this density can predict future asset returns and obtain positive findings. Finally, we apply the method to examine the causal effects of monetary policy announcements on the financial market, using high-frequency data.",http://www.sciencedirect.com/science/article/pii/S0304407621000737
Journal of Econometrics,2021,Inference in time series models using smoothed-clustered standard errors,Seunghwa Rho and Timothy Vogelsang,"This paper proposes a long run variance estimator for conducting inference in time series regression models that combines the nonparametric approach with a cluster approach. The basic idea is to divide the time periods into non-overlapping clusters. The long run variance estimator is constructed by first aggregating within clusters and then kernel smoothing across clusters or applying the nonparametric series method to the clusters with Type II discrete cosine transform. We develop an asymptotic theory for test statistics based on these “smoothed-clustered” long run variance estimators. We derive asymptotic results holding the number of clusters fixed and also treating the number of clusters as increasing with the sample size. For the kernel smoothing approach, these two asymptotic limits are different whereas for the cosine series approach, the two limits are the same. When clustering before kernel smoothing, we find that the “fixed-number-of-clusters” asymptotic approximation works well whether the number of clusters is small or large. Finite sample simulations suggest that the naive i.i.d. bootstrap mimics the fixed-number-of-clusters critical values. The simulations also suggest that clustering before kernel smoothing can reduce over-rejections caused by strong serial correlation at a cost of power. When there is a natural way of clustering, clustering can reduce over-rejection problems and achieve small gains in power for the kernel approach. In contrast, the cosine series approach does not benefit from clustering.",http://www.sciencedirect.com/science/article/pii/S0304407620302608
Journal of Econometrics,2021,Dynamic spatial panel data models with common shocks,Jushan Bai and Kunpeng Li,"This paper studies dynamic spatial panel data models with common shocks to deal with both weak and strong cross-sectional correlations. Weak correlations are captured by a spatial structure and strong correlations are captured by a factor structure. The proposed quasi-maximum likelihood estimator (QMLE) is capable of handling both types of cross sectional dependence. We provide a rigorous analysis for the asymptotic theory of the QMLE, demonstrating its desirable properties. Heteroskedasticity is explicitly allowed. This is important because QML is inconsistent in the presence of heteroskedasticity while homoskedasticity is imposed. We further show that when heteroskedasticity is estimated, the limiting variance of QMLE is not a sandwich form regardless of normality. Monte Carlo simulations show that the QMLE has good finite sample properties.",http://www.sciencedirect.com/science/article/pii/S0304407620303961
Journal of Econometrics,2021,Bootstrapping non-stationary stochastic volatility,"H. Peter Boswijk, Giuseppe Cavaliere, Iliyan Georgiev and Anders Rahbek","In this paper we investigate to what extent the bootstrap can be applied to conditional mean models, such as regression or time series models, when the volatility of the innovations is random and possibly non-stationary. In fact, the volatility of many economic and financial time series displays persistent changes and possible non-stationarity. However, the theory of the bootstrap for such models has focused on deterministic changes of the unconditional variance and little is known about the performance and the validity of the bootstrap when the volatility is driven by a non-stationary stochastic process. This includes near-integrated exogenous volatility processes as well as near-integrated GARCH processes, where the conditional variance has a diffusion limit; a further important example is the case where volatility exhibits infrequent jumps. This paper fills this gap in the literature by developing conditions for bootstrap validity in time series and regression models with non-stationary, stochastic volatility. We show that in such cases the distribution of bootstrap statistics (conditional on the data) is random in the limit. Consequently, the conventional approaches to proofs of bootstrap consistency, based on the notion of weak convergence in probability of the bootstrap statistic, fail to deliver the required validity results. Instead, we use the concept of ‘weak convergence in distribution’ to develop and establish novel conditions for validity of the wild bootstrap, conditional on the volatility process. We apply our results to several testing problems in the presence of non-stationary stochastic volatility, including testing in a location model, testing for structural change using CUSUM-type functionals, and testing for a unit root in autoregressive models. Importantly, we work under sufficient conditions for bootstrap validity that include the absence of statistical leverage effects, i.e., correlation between the error process and its future conditional variance. The results of the paper are illustrated using Monte Carlo simulations, which indicate that a wild bootstrap approach leads to size control even in small samples.",http://www.sciencedirect.com/science/article/pii/S0304407621000282
Journal of Econometrics,2021,Simple estimators and inference for higher-order stochastic volatility models,Md. Nazmul Ahsan and Jean-Marie Dufour,"We study the problem of estimating higher-order stochastic volatility [SV(p)] models. Due to the difficulty of evaluating the likelihood function, this remains a challenging problem, even in the relatively simple SV(1) case. We propose simple moment-based winsorized ARMA-type estimators, which are computationally inexpensive and remarkably accurate. The proposed estimators do not require choosing a sampling algorithm, initial parameter values, or an auxiliary model. We show that a Durbin–Levinson-type updating algorithm can be applied to recursively estimate models of increasing order p. The asymptotic distribution of the estimators is established. Due to their computational simplicity, the proposed estimators allow one to perform finite-sample Monte Carlo tests. Simulation results show that the proposed estimators have lower bias and mean squared error than all alternative estimators (including Bayes-type estimators). The proposed estimators are applied to S&P 500 daily returns (1928–2016). We find that an SV(3) model is preferable to an SV(1) model.",http://www.sciencedirect.com/science/article/pii/S0304407621001007
Journal of Econometrics,2021,Simple tests for stock return predictability with good size and power properties,"David Harvey, Stephen J. Leybourne and Robert Taylor","We develop easy-to-implement tests for return predictability which, relative to extant tests in the literature, display attractive finite sample size control and power across a wide range of persistence and endogeneity levels for the predictor. Our approach is based on the standard regression t-ratio and a variant where the predictor is quasi-GLS (rather than OLS) demeaned. In the strongly persistent near-unit root environment, the limiting null distributions of these statistics depend on the endogeneity and local-to-unity parameters characterising the predictor. Analysis of the asymptotic local power functions of feasible implementations of these two tests, based on asymptotically conservative critical values, motivates a switching procedure between the two, employing the quasi-GLS demeaned variant unless the magnitude of the estimated endogeneity correlation parameter is small. Additionally, if the data suggests the predictor is weakly persistent, our approach switches to the standard t-ratio test with reference to standard normal critical values.",http://www.sciencedirect.com/science/article/pii/S0304407621000270
Journal of Econometrics,2021,Consistent inference for predictive regressions in persistent economic systems,Torben Andersen and Rasmus T. Varneskov,"This paper studies standard predictive regressions in economic systems governed by persistent vector autoregressive dynamics for the state variables. In particular, all – or a subset – of the variables may be fractionally integrated, which induces a spurious regression problem. We propose a new inference and testing procedure – the Local speCtruM (LCM) approach – for joint significance of the regressors, that is robust against the variables having different integration orders and remains valid regardless of whether predictors are significant and, if they are, whether they induce cointegration. Specifically, the LCM procedure is based on fractional filtering and band spectrum regression using a suitably selected set of frequency ordinates. Contrary to existing procedures, we establish a uniform Gaussian limit theory and a standard χ2-distributed test statistic. Using the LCM inference and testing techniques, we explore predictive regressions for the realized return variation. Standard least squares inference indicates that popular financial and macroeconomic variables convey valuable information about future return volatility. In contrast, we find no significant evidence using our robust LCM procedure. If anything, our tests support a reverse chain of causality, with rising financial volatility predating adverse innovations to key macroeconomic variables. Simulations are employed to illustrate the relevance of the theoretical arguments for finite-sample inference.",http://www.sciencedirect.com/science/article/pii/S0304407620303547
Journal of Econometrics,2021,Sufficient statistics for unobserved heterogeneity in structural dynamic logit models,"Victor Aguirregabiria, Jiaying Gu and Yao Luo","We study the identification and estimation of structural parameters in dynamic panel data logit models where decisions are forward-looking and the joint distribution of unobserved heterogeneity and observable state variables is nonparametric, i.e., fixed-effects model. We consider models with two endogenous state variables: the lagged decision variable, and the time duration in the last choice. This class of models includes as particular cases important economic applications such as models of market entry–exit, occupational choice, machine replacement, inventory and investment decisions, or dynamic demand of differentiated products. We prove the identification of the structural parameters using a conditional likelihood approach. The structure of the model implies that there is a sufficient statistic such that the likelihood function conditional on this statistic no longer depends on the unobserved heterogeneity – neither through the current utility nor through the continuation value of the forward-looking decision problem – but still depends on the structural parameters. We apply this estimator to a machine replacement model.",http://www.sciencedirect.com/science/article/pii/S0304407620303286
Journal of Econometrics,2021,Semiparametric estimation of dynamic discrete choice models,"Nicholas Buchholz, Matthew Shum and Haiqing Xu","We consider the estimation of dynamic binary choice models in a semiparametric setting, in which the per-period utility functions are known up to a finite number of parameters, but the distribution of utility shocks is left unspecified. This semiparametric setup differs from most of the existing identification and estimation literature for dynamic discrete choice models. To show identification we derive and exploit a new recursive representation for the unknown quantile function of the utility shocks. Our estimators are straightforward to compute, and resemble classic closed-form estimators from the literature on semiparametric regression and average derivative estimation. Monte Carlo simulations demonstrate that our estimator performs well in small samples.",http://www.sciencedirect.com/science/article/pii/S0304407620303274
Journal of Econometrics,2021,Solving dynamic discrete choice models using smoothing and sieve methods,"Dennis Kristensen, Patrick K. Mogensen, Jong Myun Moon and Bertel Schjerning","We propose to combine smoothing, simulations and sieve approximations to solve for either the integrated or expected value function in a general class of dynamic discrete choice (DDC) models. We use importance sampling to approximate the Bellman operators defining the two functions. The random Bellman operators, and therefore also the corresponding solutions, are generally non-smooth which is undesirable. To circumvent this issue, we introduce smoothed versions of the random Bellman operators and solve for the corresponding smoothed value functions using sieve methods. We also show that one can avoid using sieves by generalizing and adapting the “self-approximating” method of Rust (1997b) to our setting. We provide an asymptotic theory for both approximate solution methods and show that they converge with N-rate, where N is number of Monte Carlo draws, towards Gaussian processes. We examine their performance in practice through a set of numerical experiments and find that both methods perform well with the sieve method being particularly attractive in terms of computational speed and accuracy.",http://www.sciencedirect.com/science/article/pii/S0304407620303298
Journal of Econometrics,2021,The likelihood of mixed hitting times,Jaap H. Abbring and Tim Salimans,"We present a method for computing the likelihood of a mixed hitting-time model that specifies durations as the first time a latent Lévy process crosses a heterogeneous threshold. This likelihood is not generally known in closed form, but its Laplace transform is. Our approach to its computation relies on numerical methods for inverting Laplace transforms that exploit special properties of the first passage times of Lévy processes. We use our method to implement a maximum likelihood estimator of the mixed hitting-time model in MATLAB. We illustrate the application of this estimator with an analysis of Kennan’s (1985) strike data.",http://www.sciencedirect.com/science/article/pii/S0304407621001093
Journal of Econometrics,2021,Bidding frictions in ascending auctions,"Aaron Barkley, Joachim R. Groeger and Robert A. Miller","This paper develops an approach for identifying and estimating the distribution of valuations in ascending auctions where an indeterminate number of bidders have an unknown number of bidding opportunities. To finesse the complications for identification and estimation due to multiple equilibria, our empirical analysis is based on the fact that bidders play undominated strategies in every equilibrium. We apply the model to a monthly financial market in which local banks compete for deposit securities. This market features frequent jump bidding and winning bids well above the highest losing bid, suggesting standard empirical approaches for ascending auctions may not be suitable. We find that frictions are costly both for revenue and allocative efficiency.",http://www.sciencedirect.com/science/article/pii/S0304407620303250
Journal of Econometrics,2021,"Effects of taxes and safety net pensions on life-cycle labor supply, savings and human capital: The case of Australia",Fedor Iskhakov and Michael Keane,"We structurally estimate a life-cycle model of consumption, labor supply and retirement, using data from the Australian HILDA panel. We use the model to evaluate effects of Australia’s Age Pension system and income tax policy on labor supply, consumption and retirement. Our model accounts for human capital, savings, uninsurable wage risk and credit constraints. We account for “bunching” of hours by assuming a discrete set of hours levels, and we investigate labor supply on both the intensive and extensive margins. Our model allows us to quantify the effects of anticipated and unanticipated tax and pension policy changes at different points of the life-cycle. Our results imply that Australia’s Age Pension system as currently designed is poorly targeted. Our simulations suggest that a doubling of taper rates, combined with a 5.9% reduction of income tax rates, would be budget neutral and Pareto improving.",http://www.sciencedirect.com/science/article/pii/S0304407620303262
Journal of Econometrics,2021,"Labor market search, informality, and on-the-job human capital accumulation","Matteo Bobba, Luca Flabbi, Santiago Levy and Mauricio Tejada","We develop a search and matching model where firms and workers produce output that depends both on match-specific productivity and worker-specific human capital. The human capital is accumulated while working but depreciates while searching for a job. Jobs can be formal or informal. The model is estimated on labor market data for Mexico. Human capital accumulation is responsible for more than half of the overall value of production, and upgrades more quickly while working formally than informally. Policy experiments reveal that human capital accumulation magnifies the negative impact on productivity of the labor market institutions that give rise to informality.",http://www.sciencedirect.com/science/article/pii/S0304407620303304
Journal of Econometrics,2021,"Illegal drugs, education, and labor market outcomes",Alvaro Mezza and Moshe Buchinsky,"In this paper we investigate the causal effects of consuming illegal drugs on educational attainment, employment, and wages. To identify these effects we develop and estimate a dynamic structural model to jointly consider decisions of whether to consume drugs, attend school, participate in the labor force, and save.",http://www.sciencedirect.com/science/article/pii/S0304407620303316
Journal of Econometrics,2021,Indirect inference for locally stationary models,David T. Frazier and Bonsoo Koo,"We propose the use of indirect inference estimation to conduct inference in complex locally stationary models. We develop a local indirect inference algorithm and establish the asymptotic properties of the proposed estimator. Due to the nonparametric nature of locally stationary models, the resulting indirect inference estimator exhibits nonparametric rates of convergence. We validate our methodology with simulation studies in the confines of a locally stationary moving average model and a new locally stationary multiplicative stochastic volatility model. Using this indirect inference methodology and the new locally stationary volatility model, we obtain evidence of non-linear, time-varying volatility trends for monthly returns on several Fama–French portfolios.",http://www.sciencedirect.com/science/article/pii/S0304407620303031
Journal of Econometrics,2021,Nonparametric regression with selectively missing covariates,Christoph Breunig and Peter Haan,"We consider the problem of regression with selectively observed covariates in a nonparametric framework. Our approach relies on instrumental variables that explain variation in the latent covariates but have no direct effect on selection. The regression function of interest is shown to be a weighted version of observed conditional expectation where the weighting function is a fraction of selection probabilities. Nonparametric identification of the fractional probability weight (FPW) function is achieved via a partial completeness assumption. We provide primitive functional form assumptions for partial completeness to hold. The identification result is constructive for the FPW series estimator. We derive the rate of convergence and also the pointwise asymptotic distribution. In both cases, the asymptotic performance of the FPW series estimator does not suffer from the inverse problem which derives from the nonparametric instrumental variable approach. In a Monte Carlo study, we analyze the finite sample properties of our estimator and we compare our approach to inverse probability weighting, which can be used alternatively for unconditional moment estimation. In the empirical application, we focus on two different applications. We estimate the association between income and health using linked data from the SHARE survey and administrative pension information and use pension entitlements as an instrument. In the second application we revisit the question how income affects the demand for housing based on data from the German Socio–Economic Panel Study (SOEP). In this application we use regional income information on the residential block level as an instrument. In both applications we show that income is selectively missing and we demonstrate that standard methods that do not account for the nonrandom selection process lead to significantly biased estimates for individuals with low income.",http://www.sciencedirect.com/science/article/pii/S0304407620303183
Journal of Econometrics,2021,Nonparametric estimation of large covariance matrices with conditional sparsity,"Hanchao Wang, Bin Peng, Degui Li and Chenlei Leng","This paper studies estimation of covariance matrices with conditional sparse structure. We overcome the challenge of estimating dense matrices using a factor structure, the challenge of estimating large-dimensional matrices by postulating sparsity on covariance of random noises, and the challenge of estimating varying matrices by allowing factor loadings to smoothly change. A kernel-weighted estimation approach combined with generalised shrinkage is proposed. Under some technical conditions, we derive uniform consistency for the developed estimation method and obtain convergence rates. Numerical studies including simulation and an empirical application are presented to examine the finite-sample performance of the developed methodology.",http://www.sciencedirect.com/science/article/pii/S0304407620303456
Journal of Econometrics,2021,Integrated likelihood based inference for nonlinear panel data models with unobserved effects,"Martin Schumann, Thomas A. Severini and Gautam Tripathi","We propose a new integrated likelihood based approach for estimating panel data models when the unobserved individual effects enter the model nonlinearly. Unlike existing integrated likelihoods in the literature, the one we propose is closer to a genuine likelihood. Although the statistical theory for the proposed estimator is developed in an asymptotic setting where the number of individuals and the number of time periods both approach infinity, results from a simulation study suggest that our methodology can work very well even in moderately sized panels of short duration in both static and dynamic models.",http://www.sciencedirect.com/science/article/pii/S0304407620303432
Journal of Econometrics,2021,Model selection in utility-maximizing binary prediction,Jiun-Hua Su,"The maximum utility estimation proposed by Elliott and Lieli (2013) can be viewed as cost-sensitive binary classification; thus, its in-sample overfitting issue is similar to that of perceptron learning. A utility-maximizing prediction rule (UMPR) is constructed to alleviate the in-sample overfitting of the maximum utility estimation. We establish non-asymptotic upper bounds on the difference between the maximal expected utility and the generalized expected utility of the UMPR. Simulation results show that the UMPR with an appropriate data-dependent penalty achieves larger generalized expected utility than common estimators in the binary classification if the conditional probability of the binary outcome is misspecified.",http://www.sciencedirect.com/science/article/pii/S0304407620303420
Journal of Econometrics,2021,Inference without smoothing for large panels with cross-sectional and temporal dependence,Javier Hidalgo and Marcia Schafgans,"This paper addresses inference in large panel data models in the presence of both cross-sectional and temporal dependence of unknown form. We are interested in making inferences that do not rely on the choice of any smoothing parameter as is the case with the often employed “HAC” estimator for the covariance matrix. To that end, we propose a cluster estimator for the asymptotic covariance of the estimators and valid bootstrap schemes that do not require the selection of a bandwidth or smoothing parameter and accommodate the nonparametric nature of both temporal and cross-sectional dependence. Our approach is based on the observation that the spectral representation of the fixed effect panel data model is such that the errors become approximately temporally uncorrelated. Our proposed bootstrap schemes can be viewed as wild bootstraps in the frequency domain. We present some Monte Carlo simulations to shed some light on the small sample performance of our inferential procedure.",http://www.sciencedirect.com/science/article/pii/S0304407620303481
Journal of Econometrics,2021,Shrinkage for categorical regressors,Phillip Heiler and Jana Mareckova,"This paper introduces a flexible regularization approach that reduces point estimation risk of group means stemming from e.g. categorical regressors, (quasi-)experimental data or panel data models. The loss function is penalized by adding weighted squared ℓ2-norm differences between group location parameters and informative first stage estimates. Under quadratic loss, the penalized estimation problem has a simple interpretable closed-form solution that nests methods established in the literature on ridge regression, discretized support smoothing kernels and model averaging methods. We derive risk-optimal penalty parameters and propose a plug-in approach for estimation. The large sample properties are analyzed in an asymptotic local to zero framework by introducing a class of sequences for close and distant systems of locations that is sufficient for describing a large range of data generating processes. We provide the asymptotic distributions of the shrinkage estimators under different penalization schemes. The proposed plug-in estimator uniformly dominates the ordinary least squares estimator in terms of asymptotic risk if the number of groups is larger than three. Monte Carlo simulations reveal robust improvements over standard methods in finite samples. Real data examples of estimating time trends in a panel and a difference-in-differences study illustrate potential applications.",http://www.sciencedirect.com/science/article/pii/S0304407620303407
Journal of Econometrics,2021,Model averaging prediction for time series models with a diverging number of parameters,"Jun Liao, Guohua Zou, Yan Gao and Xinyu Zhang","An important problem with the model averaging approach is the choice of weights. In this paper, a generalized Mallows model averaging (GMMA) criterion for choosing weights is developed in the context of an infinite order autoregressive (AR(∞)) process. The GMMA method adapts to the circumstances in which the dimensions of candidate models can be large and increase with the sample size. The GMMA method is shown to be asymptotically optimal in the sense of achieving the lowest out-of-sample mean squared prediction error (MSPE) for both the independent-realization and the same-realization predictions, which, as a byproduct, solves a conjecture put forward by Hansen (2008) that the well-known Mallows model averaging criterion from Hansen (2007) is asymptotically optimal for predicting the future of a time series. The rate of the GMMA-based weight estimator tending to the optimal weight vector minimizing the independent-realization MSPE is derived as well. Both simulation experiment and real data analysis illustrate the merits of the GMMA method in the prediction of an AR(∞) process.",http://www.sciencedirect.com/science/article/pii/S0304407620303493
Journal of Econometrics,2021,Macroeconomic uncertainty prices when beliefs are tenuous,Lars Hansen and Thomas Sargent,Investors face uncertainty over models when they do not know which member of a set of well-defined “structured models” is best. They face uncertainty about models when they suspect that all of the structured models might be misspecified. We refer to worries about the first type of ignorance as ambiguity concerns and worries about the second type as misspecification concerns. These two types of ignorance about probability distributions of risks add what we call uncertainty components to equilibrium prices of those risks. A quantitative example highlights a representative investor’s uncertainties about the size and persistence of macroeconomic growth rates. Our model of preferences under concerns about model ambiguity and misspecification puts nonlinearities into marginal valuations that induce time variations in market prices of uncertainty. These reflect the representative investor’s fears of high persistence of low growth rate states and low persistence of high growth rate states.,http://www.sciencedirect.com/science/article/pii/S0304407620303419
Journal of Econometrics,2021,Efficient estimation and filtering for multivariate jump–diffusions,François Guay and Gustavo Schwenkler,"This paper develops estimators of the transition density, filters, and parameters of multivariate jump–diffusion models. The drift, volatility, jump intensity, and jump magnitude are allowed to be state-dependent and non-affine. It is not necessary to diagonalize the volatility matrix. Our density and filter estimators converge at the canonical rate typically associated with exact Monte Carlo estimation. Our parameter estimators have the same asymptotic distribution as maximum likelihood estimators, which are often intractable for the class of models we consider. The results of this paper enable the empirical analysis of previously intractable models of asset prices and economic time series.",http://www.sciencedirect.com/science/article/pii/S0304407620303511
Journal of Econometrics,2021,Bounds on distributional treatment effect parameters using panel data with an application on job displacement,Brantly Callaway,"This paper develops new techniques to bound distributional treatment effect parameters that depend on the joint distribution of potential outcomes — an object not identified by standard identifying assumptions such as selection on observables or even when treatment is randomly assigned. I show that panel data and an additional assumption on the dependence between untreated potential outcomes for the treated group over time (i) provide more identifying power for distributional treatment effect parameters than existing bounds and (ii) provide a more plausible set of conditions than existing methods that obtain point identification. I apply these bounds to study heterogeneity in the effect of job displacement during the Great Recession. Using standard techniques, I find that workers who were displaced during the Great Recession lost on average 34% of their earnings relative to their counterfactual earnings had they not been displaced. Using the methods developed in the current paper, I also show that the average effect masks substantial heterogeneity across workers.",http://www.sciencedirect.com/science/article/pii/S0304407620302839
Journal of Econometrics,2021,Limit theorems for network dependent random variables,"Denis Kojevnikov, Vadim Marmer and Kyungchul Song","This paper is concerned with cross-sectional dependence arising because observations are interconnected through an observed network. Following (Doukhan and Louhichi, 1999), we measure the strength of dependence by covariances of nonlinearly transformed variables. We provide a law of large numbers and central limit theorem for network dependent variables. We also provide a method of calculating standard errors robust to general forms of network dependence. For that purpose, we rely on a network heteroskedasticity and autocorrelation consistent (HAC) variance estimator, and show its consistency. The results rely on conditions characterized by tradeoffs between the rate of decay of dependence across a network and network’s denseness. Our approach can accommodate data generated by network formation models, random fields on graphs, conditional dependency graphs, and large functional-causal systems of equations.",http://www.sciencedirect.com/science/article/pii/S0304407620302402
Journal of Econometrics,2021,A weighted sieve estimator for nonparametric time series models with nonstationary variables,"Chaohua Dong, Oliver Linton and Bin Peng",We study a class of nonparametric regression models that includes deterministic time trends and both stationary and nonstationary stochastic processes (whose shocks are allowed to be mutually correlated). We propose a unified approach to estimation based on the weighted sieve method to tackle the issue of unbounded support of the covariates. This approach improves on the existing technology in terms of some key regularity conditions such as moment conditions and the α-mixing coefficients for the stationary process. We establish self-normalized central limit theorems for the sieve estimator and other related quantities. Monte Carlo simulation confirms the theoretical results. We use our methodology to study the effect of CO2 and solar irradiance on global sea level rise.,http://www.sciencedirect.com/science/article/pii/S0304407620303079
Journal of Econometrics,2021,A Bayesian robust chi-squared test for testing simple hypotheses,"Osman Doğan, Süleyman Taşpınar and Anil K. Bera","In this paper, we introduce a new Bayesian chi-squared test based on an adjusted quadratic loss function for testing a simple null hypothesis. We show that the asymptotic null distribution of our suggested test is a central chi-squared distribution under some assumptions required for the Bayesian large sample theory. We refer to our test as the Bayesian robust chi-squared test, since it is robust to parametric misspecification in the alternative model. That is, the limiting null distribution of our test is a central chi-squared distribution irrespective of parametric misspecification in the alternative model. In addition to being robust to parametric misspecification, our test also shares properties of the test suggested by Li et al. (2015) based on a quadratic loss function. We provide four examples to illustrate the implementation of our suggested Bayesian test statistic.",http://www.sciencedirect.com/science/article/pii/S0304407620303018
Journal of Econometrics,2021,Uncovering heterogeneous social effects in binary choices,"Zhongjian Lin, Xun Tang and Ning Neil Yu","We identify and estimate heterogeneous social effects within groups of individuals that make binary choices. These heterogeneous social effects, which include peer and contextual effects, are modeled through unobserved influence matrices that summarize how the members within each group affect each other’s outcomes. We recover parameters in social effects as well as the unknown influence matrices by exploiting how these matrices are linked to the reduced-form effects of multiple characteristics. Monte Carlo experiments show that a nested fixed-point maximum-likelihood estimator for the social effects has good finite-sample performance. Using a new dataset, we analyze how college roommates influence each other’s decisions to participate in volunteering activities. Our estimates reveal substantial heterogeneity in the social effects among these students.",http://www.sciencedirect.com/science/article/pii/S0304407620303080
Journal of Econometrics,2021,Time-varying model averaging,"Yuying Sun, Yongmiao Hong, Tae Hwy Lee, Shouyang Wang and Xinyu Zhang","Structural changes often occur in economics and finance due to changes in preferences, technologies, institutional arrangements, policies, crises, etc. Improving forecast accuracy of economic time series with structural changes is a long-standing problem. Model averaging aims at providing an insurance against selecting a poor forecast model. All existing model averaging approaches in the literature are designed with constant (non-time-varying) combination weights. Little attention has been paid to time-varying model averaging, which is more realistic in economics under structural changes. This paper proposes a novel model averaging estimator which selects optimal time-varying combination weights by minimizing a local jackknife criterion. It is shown that the proposed time-varying jackknife model averaging (TVJMA) estimator is asymptotically optimal in the sense of achieving the lowest possible local squared error loss in a class of time-varying model averaging estimators. Under a set of regularity assumptions, the TVJMA estimator is Th-consistent. A simulation study and an empirical application highlight the merits of the proposed TVJMA estimator relative to a variety of popular estimators with constant model averaging weights and model selection.",http://www.sciencedirect.com/science/article/pii/S0304407620303067
Journal of Econometrics,2021,Simple and trustworthy cluster-robust GMM inference,Jungbin Hwang,"This paper develops a new asymptotic theory for GMM estimation and inference in the presence of clustered dependence. The key feature of our alternative asymptotics is that the number of clusters G is regarded as fixed as the sample size increases. Under the fixed-G asymptotics, we show that the Wald and t tests in two-step GMM are asymptotically pivotal only if we recenter the estimated moment process in the clustered covariance estimator (CCE). Also, the J statistic, the trinity of two-step GMM statistics (QLR, LM, and Wald), and the t statistic can be modified to have an asymptotic standard F distribution or t distribution. We suggest a finite-sample variance correction to further improve the accuracy of the F and t approximations. The proposed tests are very appealing to practitioners because the test statistics are simple modifications of conventional GMM test statistics, and critical values are readily available from F and t tables. No further simulations or resampling methods are needed. A Monte Carlo study shows that our proposed tests are more accurate than the conventional large-G asymptotic inferences.",http://www.sciencedirect.com/science/article/pii/S0304407620303043
Journal of Econometrics,2021,Solving Euler equations via two-stage nonparametric penalized splines,"Liyuan Cui, Yongmiao Hong and Yingxing Li","This study proposes a novel estimation-based approach to solving asset pricing models for both stationary and time-varying observations. Our method is robust to misspecification errors while inheriting a closed-form solution. By representing the Euler equation into a well-posed integral equation of the second kind, we propose a penalized two-stage nonparametric estimation method and establish its optimal convergence under mild conditions. With the merit of penalized splines, our estimate is less sensitive to the spline setting and we also design a fast data-driven algorithm to effectively tune the key smoother, i.e. the penalty amount. Our approach exhibits excellent finite sample performance. Using the US data from 1947 to 2017, we reinvestigate the return predictability and find that the estimated implied dividend yield significantly predicts lower future cash flows and higher interest rates at short horizons.",http://www.sciencedirect.com/science/article/pii/S0304407620302323
Journal of Econometrics,2021,Bounding the difference between true and nominal rejection probabilities in tests of hypotheses about instrumental variables models,Joel L. Horowitz,"This paper presents a simple method for carrying out inference in a wide variety of possibly nonlinear IV models under weak assumptions. The method provides a finite-sample bound on the difference between the true and nominal probabilities of rejecting a correct null hypothesis. The method is a non-Studentized version of the S test of Stock and Wright (2000) but is implemented and analyzed differently. It does not require restrictive distributional assumptions, linearity of the estimated model, simultaneous equations, or information about whether the instruments are strong or weak. It can be applied to quantile IV models that may be nonlinear and can be used to test a parametric IV model against a nonparametric alternative. It provides information about the relation between the “degree of weakness” of the instruments and the power of the test. The bound presented here holds in finite samples, regardless of the strength of the instruments.",http://www.sciencedirect.com/science/article/pii/S0304407620303353
Journal of Econometrics,2021,Valid inference for treatment effect parameters under irregular identification and many extreme propensity scores,Phillip Heiler and Ekaterina Kazak,"This paper provides a framework for conducting valid inference for causal parameters without imposing strong variance or support restrictions on the propensity score. In particular, it covers the case of irregularly identified treatment effect parameters. We provide limit theorems for inverse probability weighting and doubly robust estimation of causal or counterfactual parameters that do not rely on trimming approaches. By construction the limiting distributions of these estimators belong to the alpha-stable class which implies that standard inference methods such as the nonparametric bootstrap are inconsistent. We propose an adaptive version of the m-out-of-n bootstrap that is robust to all types of identification and a bootstrap aggregation method for the optimal m choice. Monte Carlo simulations suggest that the modified resampling method compares favorably to conventional methods in finite samples. The method is applied to a re-analysis of the causal impact of right heart catheterization on survival rates.",http://www.sciencedirect.com/science/article/pii/S0304407620303377
Journal of Econometrics,2021,A survey of preference estimation with unobserved choice set heterogeneity,"Gregory S. Crawford, Rachel Griffith and Alessandro Iaria","We provide an introduction to the estimation of discrete choice models when choice sets are heterogeneous and unobserved to the econometrician. We survey the two most popular approaches: “integrating over” and “differencing out” unobserved choice sets. Inspired by Chamberlain (1980)’s original idea of constructing sufficient statistics from observed choices, we introduce the term “sufficient set” to refer to any combination of observed choices that lies within the true but unobserved choice set. The concept of sufficient set helps to unify notation and organize our thinking, to map econometric assumptions onto economic models, and to implement both methods in practice.",http://www.sciencedirect.com/science/article/pii/S0304407620302463
Journal of Econometrics,2021,Using penalized likelihood to select parameters in a random coefficients multinomial logit model,Joel L. Horowitz and Lars Nesheim,"This paper is about estimating a random coefficients logit model in which the distribution of each coefficient is characterized by finitely many parameters, some of which may be zero. The paper gives conditions under which, with probability approaching 1 as the sample size increases, penalized maximum likelihood (PML) estimation with the adaptive LASSO (AL) penalty distinguishes correctly between zero and non-zero parameters. The paper also gives conditions under which PML reduces the asymptotic mean-square estimation error of any continuously differentiable function of the model’s parameters. The paper describes a method for computing PML estimates and presents the results of Monte Carlo experiments that illustrate their performance. It also presents the results of PML estimation of a random coefficients logit model of choice among brands of butter and margarine in the British groceries market.",http://www.sciencedirect.com/science/article/pii/S0304407620300427
Journal of Econometrics,2021,BLP estimation using Laplace transformation and overlapping simulation draws,"Han Hong, Huiyu Li and Jessie Li","We derive the asymptotic distribution of the parameters of the Berry et al. (1995) (BLP) model in a many markets setting which takes into account simulation noise under the assumption of overlapping simulation draws. We show that as long as the number of simulation draws R and the number of markets T approach infinity, our estimator is m=min(R,T) consistent and asymptotically normal. We do not impose any relationship between the rates at which R and T go to infinity, thus allowing for the case of R≪T. We provide a consistent estimate of the asymptotic variance which can be used to form asymptotically valid confidence intervals. Instead of directly minimizing the BLP GMM objective function, we propose using Hamiltonian Markov Chain Monte Carlo methods to implement a Laplace-type estimator which is asymptotically equivalent to the GMM estimator.",http://www.sciencedirect.com/science/article/pii/S0304407620302487
Journal of Econometrics,2021,"Control variables, discrete instruments, and identification of structural functions",Whitney Newey and Sami Stouli,"Control variables provide an important means of controlling for endogeneity in econometric models with nonseparable and/or multidimensional heterogeneity. We allow for discrete instruments, giving identification results under a variety of restrictions on the way the endogenous variable and the control variables affect the outcome. We consider many structural objects of interest, such as average or quantile treatment effects. We illustrate our results with an empirical application to Engel curve estimation.",http://www.sciencedirect.com/science/article/pii/S0304407620302499
Journal of Econometrics,2021,Assessing consumer demand with noisy neural measurements,"Ryan Webb, Nitin Mehta and Ifat Levy","Recent studies have used the random utility framework to examine whether neural data can assess and predict demand for consumer products, both within and across individuals. However the effectiveness of this methodology has been limited by the large degree of measurement error in neural data. The resulting “error-in-variables” problem severely biases the estimates of the relationship between neural measurements and choice behaviour, thus limiting the role such data can play in assessing marginal contributions to utility. In this article, we propose a method for controlling for this large degree of measurement error in value regions of the brain. We propose that additional neural variables from areas of the brain that are unrelated to valuation can serve as “proxies” for the measurement error in value regions, substantially alleviating the bias in model estimates. We demonstrate the feasibility of our proposed method on an existing dataset of fMRI measurements and consumer choices. We find a substantial reduction in the bias of model estimates compared to existing baseline methods (the estimated coefficients roughly double), leading to improved inference and out-of-sample demand prediction. After controlling for measurement error, we also find a considerable reduction in the variation of model estimates across consumers.",http://www.sciencedirect.com/science/article/pii/S0304407620302505
Journal of Econometrics,2021,Evaluating consumers’ choices of Medicare Part D plans: A study in behavioral welfare economics,"Michael Keane, Jonathan Ketcham, Nicolai Kuminoff and Timothy Neal","We propose new methods to model behavior and conduct welfare analysis in complex environments where some choices are unlikely to reveal preferences. We develop a mixture-of-experts model that incorporates heterogeneity in consumers’ preferences and in their choice processes. We also develop a method to decompose logit errors into latent preferences versus optimization errors. Applying these methods to Medicare beneficiaries’ prescription drug insurance choices suggests that: (1) average welfare losses from suboptimal choices are small, (2) beneficiaries with dementia and depression have larger losses, and (3) policies that simplify choice sets offer small average benefits, helping some people but harming others.",http://www.sciencedirect.com/science/article/pii/S0304407620302517
Journal of Econometrics,2021,Disentangling moral hazard and adverse selection in private health insurance,David Powell and Dana Goldman,"Moral hazard and adverse selection create inefficiencies in private health insurance markets and understanding the relative importance of each factor is critical for addressing these inefficiencies. We use claims data from a large firm which changed health insurance plan options to isolate moral hazard from plan selection, estimating a discrete choice model to predict household plan preferences and attrition. Variation in plan preferences identifies the differential causal impact of each health insurance plan on the entire distribution of medical expenditures. Our estimates imply that 53% of the additional medical spending observed in the most generous plan in our data relative to the least generous is due to adverse selection. We find that quantifying adverse selection by using prior medical expenditures overstates the true magnitude of selection due to mean reversion. We also statistically reject that individual health care consumption responds solely to the end-of-the-year marginal price.",http://www.sciencedirect.com/science/article/pii/S0304407620302529
Journal of Econometrics,2021,How well do structural demand models work? Counterfactual predictions in school choice,Parag Pathak and Peng Shi,"This paper investigates the prediction accuracy of discrete choice models of school demand, using a policy reform in Boston that altered where applicants can apply under school choice. We find that the discrete choice models do not consistently outperform a much simpler heuristic, but their inconsistent performance largely arises from prediction errors in applicant characteristics, which are auxiliary inputs. Once we condition on the correct inputs, the discrete choice models consistently outperform, and their accuracy does not significantly improve upon refitting using post-reform data, suggesting that the choice models capture stable components of the preference distribution across policy regimes.",http://www.sciencedirect.com/science/article/pii/S0304407620302530
Journal of Econometrics,2021,Vehicle size choice and automobile externalities: A dynamic analysis,Clifford Winston and Jia Yan,"We study the effect of highway congestion on the “arms race” on American roads, which has led to larger and more powerful vehicles that reduce safety and increase fuel consumption. We develop a new methodology to estimate a dynamic vehicle size choice and replacement model and find that congestion delays affect vehicle sizes. We then show that by addressing complementary externalities – congestion and the externalities associated with larger vehicle sizes – congestion pricing could reduce the vehicle fatality rate, generating $25 billion in annual benefits, and could improve vehicle fleet fuel efficiency, generating roughly $10 billion in annual operating cost savings.",http://www.sciencedirect.com/science/article/pii/S0304407620302542
Journal of Econometrics,2021,Estimation of endogenously sampled time series: The case of commodity price speculation in the steel market,George Hall and John Rust,"We consider the problem of estimating a Markov process that is endogenously sampled. We observe a discrete-time Markov process {pt} at a set of random times {t1,…,tn} that depend on the outcome of a probabilistic sampling rule that depends on the state of the process and other observed covariates xt. We focus on a particular example where pt is the daily wholesale price of a standardized steel product. The endogenous sampling problem arises from the fact that we only observe pt on the days the firm purchases steel. We show how to solve this problem under two different assumptions about firm behavior: (1) optimality: the timing of steel purchases is governed by an optimal purchasing strategy that maximizes expected discounted profits, and (2) potential suboptimality: we allow the firm to use any randomized, Markovian purchasing strategy. In the latter case, the estimation problem becomes semi-parametric and we use the method of sieves to estimate a flexible parametric approximation to the firm’s purchasing behavior that best fits the data without imposing optimality. We show how estimation of this model becomes tractable under either of these assumptions using the method of simulated moments (MSM). We simulate realizations of wholesale steel prices and sample them in the same way as they are sampled in the actual data, i.e. only on days where purchases occur. We use the MSM estimator to estimate a truncated lognormal AR(1) model of the wholesale price processes for particular types of steel plate and test and reject the assumption that the firm is behaving optimally.",http://www.sciencedirect.com/science/article/pii/S0304407620302554
Journal of Econometrics,2021,The browser war — Analysis of Markov Perfect Equilibrium in markets with dynamic demand effects,"Mark Jenkins, Paul Liu, Rosa Matzkin and Daniel L. McFadden","When a concentrated market for differentiated products exhibits dynamic demand effects due to inertia, contagion, or network externalities, forward-looking firms consider the strategic impact of investment, pricing, and other conduct that can tip the market by grabbing market share. When the contested market provides a line of defense for a more lucrative core market, a firm can have a powerful strategic incentive to capture and control the contested market, as a “loss leader” if necessary. We use this framework to analyze the browser war between Netscape and Microsoft. Adopting a Markov Perfect Equilibrium model to capture firms’ strategic behavior, we discuss the steps needed for such analysis. We compare as-is market trajectories with but-for trajectories under a counterfactual without “anticompetitive acts” deemed in violation of anti-trust law. Our empirical analysis uses incomplete and noisy public data. Consequently, specifications and results should be viewed only as illustrating the method in a highly parametrized model with restrictions that might not be robust. Access to complete and confidential firm data would result in a less restricted model with potentially different results.",http://www.sciencedirect.com/science/article/pii/S0304407620302566
Journal of Econometrics,2021,Augmented factor models with applications to validating market risk factors and forecasting bond risk premia,"Jianqing Fan, Yuan Ke and Yuan Liao","We study factor models augmented by observed covariates that have explanatory powers on the unknown factors. In financial factor models, the unknown factors can be reasonably well explained by a few observable proxies, such as the Fama–French factors. In diffusion index forecasts, identified factors are strongly related to several directly measurable economic variables such as consumption-wealth variable, financial ratios, and term spread. With those covariates, both the factors and loadings are identifiable up to a rotation matrix even only with a finite dimension. To incorporate the explanatory power of these covariates, we propose a smoothed or projected principal component analysis (PCA): (i) regress the data onto the observed covariates, and (ii) take the principal components of the fitted data to estimate the loadings and factors. This allows us to more accurately estimate the percentage of both explained and unexplained components in factors and thus to assess the explanatory power of covariates. We show that both the estimated factors and loadings can be estimated with improved rates of convergence compared to the benchmark method. The degree of improvement depends on the strength of the signals, representing the explanatory power of the covariates on the factors. The proposed estimator is robust to possibly heavy-tailed distributions. We apply the model to forecast US bond risk premia, and find that the observed macroeconomic characteristics contain strong explanatory powers of the factors. The gain of forecast is more substantial when the characteristics are incorporated to estimate the common factors than directly used for forecasts.",http://www.sciencedirect.com/science/article/pii/S0304407620301925
Journal of Econometrics,2021,Estimation and inference in semiparametric quantile factor models,"Shujie Ma, Oliver Linton and Jiti Gao","We consider a semiparametric quantile factor panel model that allows observed stock-specific characteristics to affect stock returns in a nonlinear time-varying way, extending Connor, Hagmann, and Linton (2012) to the quantile restriction case. We propose a sieve-based estimation methodology that is easy to implement. We provide tools for inference that are robust to the existence of moments and to the form of weak cross-sectional dependence in the idiosyncratic error term. We apply our method to daily stock return data where we find significant evidence of nonlinearity in many of the characteristic exposure curves.",http://www.sciencedirect.com/science/article/pii/S0304407620301937
Journal of Econometrics,2021,Time-varying general dynamic factor models and the measurement of financial connectedness,"Matteo Barigozzi, Marc Hallin, Stefano Soccorsi and Rainer von Sachs","We propose a new time-varying Generalized Dynamic Factor Model for high-dimensional, locally stationary time series. Estimation is based on dynamic principal component analysis jointly with singular VAR estimation, and extends to the locally stationary case the one-sided estimation method proposed by Forni et al. (2017) for stationary data. We prove consistency of our estimators of time-varying impulse response functions as both the sample size T and the dimension n of the time series grow to infinity. This approach is used in an empirical application in order to construct a time-varying measure of financial connectedness for a large panel of adjusted intra-day log ranges of stocks. We show that large increases in long-run connectedness are associated with the main financial turmoils. Moreover, we provide evidence of a significant heterogeneity in the dynamic responses to common shocks in time and over different scales, as well as across industrial sectors.",http://www.sciencedirect.com/science/article/pii/S0304407620301949
Journal of Econometrics,2021,Tail risk and return predictability for the Japanese equity market,"Torben Andersen, Viktor Todorov and Masato Ubukata","This paper studies the predictability of the Japanese equity market, focusing on the forecasting power of nonparametric volatility and tail risk measures obtained from options data on the S&P 500 and Nikkei 225 market indices. The Japanese market is notoriously difficult to forecast using standard predictive indicators. We confirm that country-specific regressions for Japan – contrary to existing evidence for other national equity indices – produce insignificant predictability patterns. However, we also find that the U.S. option-implied tail risk measure provides significant forecast power both for the dollar–yen exchange rate and the Japanese excess returns, especially when measured in U.S. dollars. Thus, the dollar-denominated Japanese returns are, in fact, predictable through the identical mechanism as for other equity market indices, suggesting a high degree of global integration for the Japanese financial market.",http://www.sciencedirect.com/science/article/pii/S0304407620301950
Journal of Econometrics,2021,Closed-form implied volatility surfaces for stochastic volatility models with jumps,"Yacine Ait-Sahalia, Chenxu Li and Chen Xu Li",We develop a closed-form bivariate expansion of the shape characteristics of the implied volatility surface generated by a stochastic volatility model with jumps in returns. We use the expansion to analyse the impact on the shape of the implied volatility surface of the various features of the stochastic volatility model and to determine which stochastic volatility models are capable of reproducing the observed characteristics of the implied volatility market data.,http://www.sciencedirect.com/science/article/pii/S0304407620301962
Journal of Econometrics,2021,Volatility analysis with realized GARCH-Itô models,"Xinyu Song, Donggyu Kim, Huiling Yuan, Xiangyu Cui, Zhiping Lu, Yong Zhou and Yazhen Wang","This paper introduces a unified approach for modeling high-frequency financial data that can accommodate both the continuous-time jump–diffusion and discrete-time realized GARCH model by embedding the discrete realized GARCH structure in the continuous instantaneous volatility process. The key feature of the proposed model is that the corresponding conditional daily integrated volatility adopts an autoregressive structure, where both integrated volatility and jump variation serve as innovations. We name it as the realized GARCH-Itô model. Given the autoregressive structure in the conditional daily integrated volatility, we propose a quasi-likelihood function for parameter estimation and establish its asymptotic properties. To improve the parameter estimation, we propose a joint quasi-likelihood function that is built on the marriage of daily integrated volatility estimated by high-frequency data and nonparametric volatility estimator obtained from option data. We conduct a simulation study to check the finite sample performance of the proposed methodologies and an empirical study with the S&P500 stock index and option data.",http://www.sciencedirect.com/science/article/pii/S0304407620301974
Journal of Econometrics,2021,"The Observed Asymptotic Variance: Hard edges, and a regression approach",Per A. Mykland and Lan Zhang,"High frequency financial data has become an essential component of the digital economy, yielding an increasing number of estimators. However, it is hard to reliably assess the uncertainty of such estimators. The Observed Asymptotic Variance (observed AVAR) is a non-parametric estimator for (squared) standard error in high frequency data. The device is related to observed information in likelihood theory, but in this case it is non-parametric and uses the high-frequency data structure. An earlier paper has developed the estimator in the case where edge effects are small to moderate. In practical data, it is often more realistic to assume that edge effects can be large, and this is the problem that we tackle in the current paper. We here find a regression approach to observed AVAR which is highly robust to large edges. This approach covers most high frequency estimators.",http://www.sciencedirect.com/science/article/pii/S0304407620301986
Journal of Econometrics,2021,Autoencoder asset pricing models,"Shihao Gu, Bryan Kelly and Dacheng Xiu","We propose a new latent factor conditional asset pricing model. Like Kelly, Pruitt, and Su (KPS, 2019), our model allows for latent factors and factor exposures that depend on covariates such as asset characteristics. But, unlike the linearity assumption of KPS, we model factor exposures as a flexible nonlinear function of covariates. Our model retrofits the workhorse unsupervised dimension reduction device from the machine learning literature – autoencoder neural networks – to incorporate information from covariates along with returns themselves. This delivers estimates of nonlinear conditional exposures and the associated latent factors. Furthermore, our machine learning framework imposes the economic restriction of no-arbitrage. Our autoencoder asset pricing model delivers out-of-sample pricing errors that are far smaller (and generally insignificant) compared to other leading factor models.",http://www.sciencedirect.com/science/article/pii/S0304407620301998
Journal of Econometrics,2021,Generalized aggregation of misspecified models: With an application to asset pricing,Nikolay Gospodinov and Esfandiar Maasoumi,"We propose a generalized aggregation approach for model averaging. The entropy-based optimal criterion is a natural choice for aggregating information from many “globally” misspecified models as it adapts better to the underlying model uncertainty and obtains more robust approximations. Unlike almost all other approaches in the existing literature, we do not require a “reference model,” or a true data generation process contained in the set of models — neither implicitly nor in otherwise popular limiting forms. This shift in paradigm prioritizes stochastic optimization and aggregation of information about outcomes over parameter estimation of an optimally selected model. Stochastic optimization is based on a risk function of aggregators across models that satisfies oracle inequalities. Our generalized aggregators relax the common perfect substitutability of the candidate models, implicit in linear averaging and pooling. The aggregation weights are data-driven and obtained from a proper (Hellinger) distance measure. The empirical results illustrate the performance and economic significance of the aggregation approach in the context of stochastic discount factor models and inflation forecasting.",http://www.sciencedirect.com/science/article/pii/S0304407620302001
Journal of Econometrics,2021,The implied arbitrage mechanism in financial markets,"Shiyi Chen, Michael T. Chng and Qingfu Liu","The no-arbitrage condition is a cornerstone concept in financial market research. However, the arbitrage mechanism that is inherent in the trading process for related securities, is not readily observable. We develop a generalized smooth-transition vector error-correction model, or GST-VECM, to estimate the arbitrage mechanism from financial market data. The GST-VECM can (i) back out the implied no-arbitrage band, (ii) estimate arbitrage intensity for upper and lower bound violations, and (iii) accommodate convergence risk for statistical arbitrage. Using the introduction of CSI300 ETF trading in China as a natural experiment, we estimate the GST-VECM to reveal some insight into how a microstructural policy, by altering the index arbitrage mechanism, affects the pricing link between spot and futures markets.",http://www.sciencedirect.com/science/article/pii/S0304407620302013
Journal of Econometrics,2021,Efficient estimation of multivariate semi-nonparametric GARCH filtered copula models,"Xiaohong Chen, Zhuo Huang and Yanping Yi","This paper considers estimation of semi-nonparametric GARCH filtered copula models in which the individual time series are modeled by semi-nonparametric GARCH and the joint distributions of the multivariate standardized innovations are characterized by parametric copulas with nonparametric marginal distributions. The models extend those of Chen and Fan (2006) to allow for semi-nonparametric conditional means and volatilities, which are estimated via the method of sieves. The fitted residuals are then used to estimate the copula parameters and the marginal densities of the standardized innovations jointly via the sieve maximum likelihood (SML). We show that, even using nonparametric filtered data, the copula parameters estimated via our SML and the two-step procedure of Chen and Fan (2006) are still root-n consistent and asymptotically normal, and the asymptotic variances of both estimators do not depend on the nonparametric filtering errors. Even more surprisingly, our SML copula estimator using the filtered data achieves the full semiparametric efficiency bound as if the standardized innovations were directly observed. These nice properties lead to simple and more accurate estimation of Value-at-Risk (VaR) for multivariate financial data with flexible dynamics, contemporaneous tail dependence and asymmetric distributions of innovations. Monte Carlo studies demonstrate that our SML estimators of the copula parameters and the marginal distributions of the standardized innovations have smaller variances and smaller mean squared errors compared to those of the two-step estimators in finite samples. A real data application is presented.",http://www.sciencedirect.com/science/article/pii/S0304407620302025
Journal of Econometrics,2021,High dimensional minimum variance portfolio estimation under statistical factor models,"Yi Ding, Yingying Li and Xinghua Zheng","We propose a high dimensional minimum variance portfolio estimator under statistical factor models, and show that our estimated portfolio enjoys sharp risk consistency. Our approach relies on properly integrating ℓ1 constraint on portfolio weights with an appropriate covariance matrix estimator. In terms of covariance matrix estimation, we extend the theoretical results of POET (Fan et al., 2013) to a setting that is coherent with principal component analysis. Simulation and extensive empirical studies on S&P 100 Index constituent stocks demonstrate favorable performance of our MVP estimator compared with benchmark portfolios.",http://www.sciencedirect.com/science/article/pii/S0304407620302037
Journal of Econometrics,2021,New testing approaches for mean–variance predictability,Gabriele Fiorentini and Enrique Sentana,"We propose parametric tests for serial correlation in levels and squares that exploit the non-normality of financial returns. Our tests are robust to distributional misspecification. Furthermore, our mean predictability tests can be robustified against time-varying volatility. Local power analyses confirm their gains over existing methods, while Monte Carlo exercises assess their finite sample reliability. We apply our tests to quarterly returns on the five Fama–French factors for international stocks, whose distributions are mostly symmetric but fat-tailed. Our results highlight noticeable differences across regions and factors and confirm the numerical sensitivity of the usual tests to influential observations.",http://www.sciencedirect.com/science/article/pii/S0304407620302049
Journal of Econometrics,2021,Autoregressive models for matrix-valued time series,"Rong Chen, Han Xiao and Dan Yang","In finance, economics and many other fields, observations in a matrix form are often generated over time. For example, a set of key economic indicators are regularly reported in different countries every quarter. The observations at each quarter neatly form a matrix and are observed over consecutive quarters. Dynamic transport networks with observations generated on the edges can be formed as a matrix observed over time. Although it is natural to turn the matrix observations into long vectors, then use the standard vector time series 2 models for analysis, it is often the case that the columns and rows of the matrix represent different types of structures that are closely interplayed. In this paper we follow the autoregression for modeling time series and propose a novel matrix autoregressive model in a bilinear form that maintains and utilizes the matrix structure to achieve a substantial dimensional reduction, as well as more interpretability. Probabilistic properties of the models are investigated. Estimation procedures with their theoretical properties are presented and demonstrated with simulated and real examples.",http://www.sciencedirect.com/science/article/pii/S0304407620302050
Journal of Econometrics,2021,The wisdom of the crowd and prediction markets,"Min Dai, Yanwei Jia and Steven Kou","Thanks to digital innovation, the wisdom of the crowd, which aims at gathering information (e.g. Wikipedia) and making a prediction (e.g. using prediction markets) from a group’s aggregated inputs, has been widely appreciated. An innovative survey design, based on a Bayesian learning framework, called the Bayesian truth serum (BTS), was proposed previously to reduce the bias in the simple majority rule by asking additional survey questions. A natural question is whether we can extend the BTS framework to prediction markets (not just polls). To do so, this paper proposes two estimators, one based on a prediction market alone and the other based on both the market and a poll question. We show that both estimators are consistent within the BTS framework, under different sets of regularity conditions. Simulations are conducted to examine the convergence of different estimators. A real data set of sports betting is used to demonstrate the effectiveness of one estimator.",http://www.sciencedirect.com/science/article/pii/S0304407620302062
Journal of Econometrics,2021,Max-linear regression models with regularization,"Qiurong Cui, Yuqing Xu, Zhengjun Zhang and Vincent Chan","Motivated by the newly developed max-linear competing copula factor models and max-stable nonlinear time series models, we propose a new class of max-linear regression models to take advantages of easy interpretable features embedded in linear regression models. It can be seen that linear relation is a special case of max-linear relation. We develop an EM algorithm based maximum likelihood estimation procedure. The consistency and asymptotics of the estimators for parameters are proved. To advance max-linear models to deal with high dimensional predictors, we adopt the common strategy of regularization in the high dimensional regression literature. We demonstrate the broad applicability of max-linear models using simulation examples and real applications in econometric and business modeling. The results, in terms of predictability, show a significant improvement compared with solely using regular regression models and other existing machine learning methods. The results enhance our understanding of the relationship between the response variable and the predictors, and among the predictors as well.",http://www.sciencedirect.com/science/article/pii/S0304407620302074
Journal of Econometrics,2021,Testing for observation-dependent regime switching in mixture autoregressive models,Mika Meitz and Pentti Saikkonen,"Testing for regime switching when the regime switching probabilities are specified either as constants (‘mixture models’) or are governed by a finite-state Markov chain (‘Markov switching models’) are long-standing problems that have also attracted recent interest. This paper considers testing for regime switching when the regime switching probabilities are time-varying and depend on observed data (‘observation-dependent regime switching’). Specifically, we consider the likelihood ratio test for observation-dependent regime switching in mixture autoregressive models. The testing problem is highly nonstandard, involving unidentified nuisance parameters under the null, parameters on the boundary, singular information matrices, and higher-order approximations of the log-likelihood. We derive the asymptotic null distribution of the likelihood ratio test statistic in a general mixture autoregressive setting using high-level conditions that allow for various forms of dependence of the regime switching probabilities on past observations, and we illustrate the theory using two particular mixture autoregressive models. The likelihood ratio test has a nonstandard asymptotic distribution that can easily be simulated, and Monte Carlo studies show the test to have good finite sample size and power properties.",http://www.sciencedirect.com/science/article/pii/S0304407620302438
Journal of Econometrics,2021,Testing constancy in varying coefficient models,Miguel Delgado and Luis A. Arteaga-Molina,"This article proposes a coefficients constancy test in semi-varying coefficient models that only needs to estimate the restricted coefficients under the null hypothesis. The test statistic resembles the union-intersection test after ordering the data according to the varying coefficients’ explanatory variable. This statistic depends on a trimming parameter that can be chosen by a data-driven calibration method we propose. A bootstrap test is justified under fairly general regularity conditions. Under more restrictive assumptions, the critical values can be tabulated, and trimming is unnecessary. The finite sample performance is studied by means of Monte Carlo experiments, and a real data application for modeling education returns.",http://www.sciencedirect.com/science/article/pii/S0304407620302657
Journal of Econometrics,2021,Dynamic decisions under subjective expectations: A structural analysis,"Yonghong An, Yingyao Hu and Ruli Xiao","We study dynamic discrete choice models without assuming rational expectations. Agents’ beliefs about state transitions are subjective, unknown, and may differ from their objective counterparts. We show that agents’ preferences and subjective beliefs are identified in both finite and infinite horizon models. We estimate the model primitives via maximum likelihood estimation and demonstrate the good performance of the estimator by Monte Carlo experiments. Using the Panel Study of Income Dynamics (PSID) data, we illustrate our method in an analysis of women’s labor participation. We find that workers do not hold rational expectations about income transitions.",http://www.sciencedirect.com/science/article/pii/S0304407620302414
Journal of Econometrics,2021,On the validity of Akaike’s identity for random fields,Carsten Jentsch and Marco Meyer,"For univariate stationary and centered time series (Xt)t∈Z, Akaike’s identity links the inverse of the Yule–Walker matrix Γ(p)=E(XX′), where X=(Xt−1,…,Xt−p)′, to the corresponding finite predictor coefficients. It reads as a Cholesky-type factorization Γ(p)−1=L(p)′Σ(p)−1L(p), where L(p) is lower-triangular and Σ(p)−1 is diagonal. Whereas this Cholesky-type factorization exists whenever Γ(p) is positive definite, Akaike derived a meaningful interpretation of L(p) and Σ(p)−1 in terms of finite predictor coefficients. It is useful in many applications and is particularly crucial to derive asymptotic theory for Berk’s spectral density estimator.",http://www.sciencedirect.com/science/article/pii/S0304407620302347
Journal of Econometrics,2021,Nonparametric estimation of jump diffusion models,Joon Y. Park and Bin Wang,"This paper develops the asymptotics for nonparametric kernel estimators of local time, drift and volatilities, and Lévy measure in jump diffusion models. Our asymptotics are developed in a very general set-up, allowing the sample span to increase as the sampling interval decreases, and without assuming stationarity. For drift and volatilities, we analyze both local constant and local linear estimators. We consider not only estimators for instantaneous conditional second moment, but also threshold estimators to disentangle diffusive and jump volatilities. The optimal bandwidths are provided for all these estimators.",http://www.sciencedirect.com/science/article/pii/S0304407620302189
Journal of Econometrics,2021,(Machine) learning parameter regions,José Luis Montiel Olea and James Nesbit,"How many random points from an identified set, a confidence set, or a highest posterior density set suffice to describe them? This paper argues that taking random draws from a parameter region in order to approximate its shape is a supervised learning problem (analogous to sampling pixels of an image to recognize it). Misclassification error – a common criterion in machine learning – provides an off-the-shelf tool to assess the quality of a given approximation. We say a parameter region can be learned if there is an algorithm that yields a misclassification error of at most ϵ with probability at least 1−δ, regardless of the sampling distribution. We show that learning a parameter region is possible if and only if its potential shapes are not too complex. Moreover, the tightest band that contains ad-dimensional parameter region is always learnable from the inside (in a sense we make precise), with at least max(1−ϵ)ln1∕δ,(3∕16)d∕ϵ draws, but at most min{2dln(2d∕δ),exp(1)(2d+ln(1∕δ))}∕ϵ. These bounds grow linearly in the dimension of the parameter region, and are uniform with respect to its true shape. We illustrate the usefulness of our results using structural vector autoregressions. We show how many orthogonal matrices are necessary/sufficient to evaluate the impulse responses’ identified set and how many ‘shotgun plots’ to report when conducting joint inference on impulse responses.",http://www.sciencedirect.com/science/article/pii/S0304407620302177
Journal of Econometrics,2021,"On factor models with random missing: EM estimation, inference, and cross validation","Sainan Jin, Ke Miao and Liangjun Su","We consider the estimation and inference in approximate factor models with random missing values. We show that with the low rank structure of the common component, we can estimate the factors and factor loadings consistently with the missing values replaced by zeros. We establish the asymptotic distributions of the resulting estimators and those based on the EM algorithm. We also propose a cross-validation-based method to determine the number of factors in factor models with or without missing values and justify its consistency. Simulations demonstrate that our cross validation method is robust to fat tails in the error distribution and significantly outperforms some existing popular methods in terms of correct percentage in determining the number of factors. An application to the factor-augmented regression models shows that a proper treatment of the missing values can improve the out-of-sample forecast of some macroeconomic variables.",http://www.sciencedirect.com/science/article/pii/S0304407620302815
Journal of Econometrics,2021,Linear IV regression estimators for structural dynamic discrete choice models,"Myrto Kalouptsidi, Paul T. Scott and Eduardo Souza-Rodrigues","In structural dynamic discrete choice models, unobserved or mis-measured state variables may lead to biased parameter estimates and misleading inference. In this paper, we show that instrumental variables can address such measurement problems when they relate to state variables that evolve exogenously from the perspective of individual agents (i.e., market-level states). We define a class of linear instrumental variables estimators that rely on Euler equations expressed in terms of conditional choice probabilities (ECCP estimators). These estimators do not require observing or modeling the agent’s entire information set, nor solving or simulating a dynamic program. As such, they are simple to implement and computationally light. We provide constructive arguments for the identification of model primitives, and establish the estimator’s consistency and asymptotic normality. Four applied examples serve to illustrate the ECCP approach’s implementation, advantages, and limitations: dynamic demand for durable goods, agricultural land use change, technology adoption, and dynamic labor supply. We illustrate the estimator’s good finite-sample performance in a Monte Carlo study, and we estimate a labor supply model empirically for taxi drivers in New York City.",http://www.sciencedirect.com/science/article/pii/S0304407620302578
Journal of Econometrics,2021,Empirical asset pricing with multi-period disaster risk: A simulation-based approach,Jantje Sönksen and Joachim Grammig,We propose a simulation-based strategy to estimate and empirically assess a class of asset pricing models that account for rare but severe consumption contractions that can extend over multiple periods. Our approach expands the scope of prevalent calibration studies and tackles the inherent sample selection problem associated with measuring the effect of rare disaster risk on asset prices. An analysis based on postwar U.S. and historical multi-country panel data yields estimates of investor preference parameters that are economically plausible and robust with respect to alternative specifications. The estimated model withstands tests of validity; the model-implied key financial indicators and timing premium all have reasonable magnitudes. These findings suggest that the rare disaster hypothesis can help restore the nexus between the real economy and financial markets when allowing for multi-period disaster events. Our methodological contribution is a new econometric framework for empirical asset pricing with rare disaster risk.,http://www.sciencedirect.com/science/article/pii/S0304407620302748
Journal of Econometrics,2021,"Bayesian MIDAS penalized regressions: Estimation, selection, and prediction",Matteo Mogliani and Anna Simoni,"We propose a new approach to mixed-frequency regressions in a high-dimensional environment that resorts to Group Lasso penalization and Bayesian estimation and inference. In particular, to improve the prediction properties of the model and its sparse recovery ability, we consider a Group Lasso with a spike-and-slab prior. Penalty hyper-parameters governing the model shrinkage are automatically tuned via an adaptive MCMC algorithm. We establish good frequentist asymptotic properties of the posterior prediction error, we recover the optimal posterior contraction rate, and we show optimality of the posterior predictive density. Simulations show that the proposed models have good selection and forecasting performance in small samples, even when the design matrix presents cross-correlation. When applied to forecasting U.S. GDP, our penalized regressions can outperform many strong competitors. Results suggest that financial variables may have some, although very limited, short-term predictive content.",http://www.sciencedirect.com/science/article/pii/S0304407620302207
Journal of Econometrics,2021,Estimation of dynamic panel spatial vector autoregression: Stability and spatial multivariate cointegration,Kai Yang and Lung-fei Lee,"This paper introduces dynamic panel spatial vector autoregressive models. We study features of dynamics and spatial interactions that an SVAR model can generate and classify the model into stable or unstable cases by partitioning parameter spaces. For stable, spatial cointegration, and mixed cointegration cases, we investigate identification and QML estimation of the models to take into account simultaneity and correlated relationships. Asymptotic properties and bias-corrected estimators are presented. To detect unknown cointegration relationships, we introduce a sequential likelihood ratio testing procedure. Simulations show the advantage of QMLEs on bias reduction and efficiency gains. The empirical application provides evidences on ancient China’s market integration.",http://www.sciencedirect.com/science/article/pii/S030440762030227X
Journal of Econometrics,2021,Robust and optimal estimation for partially linear instrumental variables models with partial identification,Qihui Chen,"This paper studies robust and optimal estimation of the slope coefficients in a partially linear instrumental variables model with nonparametric partial identification. We establish the root-n asymptotic normality of a penalized sieve minimum distance estimator of the slope coefficients. We show that the asymptotic normality holds regardless of whether the nonparametric function is point identified or only partially identified. However, in the presence of nonparametric partial identification, the slope coefficients may not be continuous in the underlying distribution and the asymptotic variance matrix may depend on the penalty, so classical efficiency analysis does not apply. We instead develop an optimally penalized estimator that minimizes the asymptotic variance of a linear functional of the slope coefficients estimator by employing an optimal penalty for a given weight, and propose a feasible two-step procedure. We also propose an iterated procedure to address how to choose both penalty and weight optimally and further improve efficiency. To conduct inference, we provide a consistent variance matrix estimator. Monte Carlo simulations examine the finite sample performance of our estimators.",http://www.sciencedirect.com/science/article/pii/S0304407620302293
Journal of Econometrics,2021,Varying random coefficient models,Christoph Breunig,"This paper analyzes unobserved heterogeneity when observed characteristics are modeled nonlinearly. The proposed model builds on varying random coefficients (VRC) that are determined by nonlinear functions of observed regressors and additively separable unobservables. This paper proposes a novel estimator of the VRC density based on weighted sieve minimum distance. The main example of sieve bases are Hermite functions which yield a numerically stable estimation procedure. This paper shows inference results that go beyond what has been shown in ordinary RC models. We provide in each case rates of convergence and also establish pointwise limit theory of linear functionals, where a prominent example is the density of potential outcomes. In addition, a multiplier bootstrap procedure is proposed to construct uniform confidence bands. A Monte Carlo study examines finite sample properties of the estimator and shows that it performs well even when the regressors associated to RC are far from being heavy tailed. Finally, the methodology is applied to analyze heterogeneity in income elasticity of demand for housing.",http://www.sciencedirect.com/science/article/pii/S030440762030244X
Journal of Econometrics,2021,Testing high-dimensional covariance matrices under the elliptical distribution and beyond,"Xinxin Yang, Xinghua Zheng and Jiaqi Chen","We develop tests for high-dimensional covariance matrices under a generalized elliptical model. Our tests are based on a central limit theorem for linear spectral statistics of the sample covariance matrix based on self-normalized observations. For testing sphericity, our tests neither assume specific parametric distributions nor involve the kurtosis of data. More generally, we can test against any non-negative definite matrix that can even be not invertible. As an interesting application, we illustrate in empirical studies that our tests can be used to test uncorrelatedness among idiosyncratic returns.",http://www.sciencedirect.com/science/article/pii/S0304407620302384
Journal of Econometrics,2021,Spatial dynamic panel data models with correlated random effects,Liyao Li and Zhenlin Yang,"In this paper, M-estimation and inference methods are developed for spatial dynamic panel data models with correlated random effects, based on short panels. The unobserved individual-specific effects are assumed to be correlated with the observed time-varying regressors linearly or in a linearizable way, giving the so-called correlated random effects model, which allows the estimation of effects of time-invariant regressors. The unbiased estimating functions are obtained by adjusting the conditional quasi-scores given the initial observations, leading to M-estimators that are consistent, asymptotically normal, and free from the initial conditions except the process starting time. By decomposing the estimating functions into sums of terms uncorrelated given idiosyncratic errors, a hybrid method is developed for consistently estimating the variance–covariance matrix of the M-estimators, which again depends only on the process starting time. Monte Carlo results demonstrate that the proposed methods perform well in finite sample. An empirical application on the political competition in China is presented.",http://www.sciencedirect.com/science/article/pii/S0304407620302372
Journal of Econometrics,2021,Large-dimensional Dynamic Factor Models: Estimation of Impulse–Response Functions with I(1) cointegrated factors,"Matteo Barigozzi, Marco Lippi and Matteo Luciani","We study a large-dimensional Dynamic Factor Model where: (i) the vector of factors Ft is I(1) and driven by a number of shocks that is smaller than the dimension of Ft; and, (ii) the idiosyncratic components are either I(1) or I(0). Under (i), the factors Ft are cointegrated and can be modeled as a Vector Error Correction Model (VECM). Under (i) and (ii), we provide consistent estimators, as both the cross-sectional size n and the time dimension T go to infinity, for the factors, the loadings, the shocks, the coefficients of the VECM and therefore the Impulse–Response Functions (IRF) of the observed variables to the shocks. Furthermore, possible deterministic linear trends are fully accounted for, and the case of an unrestricted VAR in the levels Ft, instead of a VECM, is also studied. The finite-sample properties the proposed estimators are explored by means of a MonteCarlo exercise. Finally, we revisit two distinct and widely studied empirical applications. By correctly modeling the long-run dynamics of the factors, our results partly overturn those obtained by recent literature. Specifically, we find that: (i) oil price shocks have just a temporary effect on US real activity; and, (ii) in response to a positive news shock, the economy first experiences a significant boom, and then a milder recession.",http://www.sciencedirect.com/science/article/pii/S0304407620302219
Journal of Econometrics,2021,Revisiting the location of FDI in China: A panel data approach with heterogeneous shocks,"Lei Hou, Kunpeng Li, Qi Li and Min Ouyang","Foreign Direct Investment (FDI) is viewed as a primary driving force in shaping the global economy and receives particular attention in empirical studies. In this paper, we argue that many of the existing studies ignore endogeneities that arise from shocks in source and destination countries. To address this endogeneity issue, we take the “controlling through estimating” idea from the econometric literature and propose using panel data models with heterogeneous shocks to deal with it. We consider the quasi maximum likelihood (QML) method to estimate our proposed model. We investigate the asymptotic properties of the QML estimator, including the consistency, the asymptotic representation, and the limiting distribution. We also propose new statistics to test the validity of the use of traditional dynamic and static panel data estimation methods. Applying it to the location determinants of inward FDI in China, we find that the endogeneity issue does exist, and that controlling for heterogeneous shocks helps to improve the estimation results.",http://www.sciencedirect.com/science/article/pii/S0304407620302426
Journal of Econometrics,2021,Detection of units with pervasive effects in large panel data models,"George Kapetanios, Mohammad Pesaran and S. Reese","The importance of units that influence a large number of other units in a network has become increasingly recognized in the literature. In this paper we propose a new method to detect such pervasive units by basing our analysis on unit-specific residual error variances subject to suitable adjustments due to the multiple testing issues involved. Accordingly, a sequential multiple testing (SMT) procedure is proposed, which allows identification of pervasive units (if any) without a priori knowledge of the interconnections amongst cross-section units or availability of a short list of candidate units to search over. The proposed method is applicable even if the cross-section dimension exceeds the time series dimension, and most importantly it could end up with none of the units selected as pervasive when this is in fact the case. The SMT procedure exhibits satisfactory small-sample performance in Monte Carlo simulations and compares well relative to existing approaches. We apply the SMT detection method to sectoral indices of U.S. industrial production, U.S. house price changes by states, and the rates of change of real GDP and real equity prices across the world’s largest economies.",http://www.sciencedirect.com/science/article/pii/S0304407620302141
Journal of Econometrics,2021,Missing observations in observation-driven time series models,"Francisco Blasques, P. Gorgi and Siem Jan Koopman","We argue that existing methods for the treatment of missing observations in time-varying parameter observation-driven models lead to inconsistent inference. We provide a formal proof of this inconsistency for a Gaussian model with time-varying mean. A Monte Carlo simulation study supports this theoretical result and illustrates how the inconsistency problem extends to score-driven and, more generally, to observation-driven models, which include well-known models for conditional volatility. To overcome the problem of inconsistent inference, we propose a novel estimation procedure based on indirect inference. This easy-to-implement method delivers consistent inference. The asymptotic properties of the new method are formally derived. Our proposed estimation procedure shows a promising performance in a Monte Carlo simulation exercise as well as in an empirical study concerning the measurement of conditional volatility from financial returns data.",http://www.sciencedirect.com/science/article/pii/S0304407620302670
Journal of Econometrics,2021,The factor analytical approach in near unit root interactive effects panels,Milda Norkutė and Joakim Westerlund,"In a recent study, Bai (2013) proposes a new factor analytical (FA) method for estimation of stationary dynamic panel data models with fixed effects. Our interest in this method originates with the fact it does not require explicit demeaning of the data, a practice that is known to cause problems of bias and low power in near unit root panels. The purpose is to study the properties of FA when applied to such panels when the common component admits to a interactive effects representation, which is more general than fixed effects. It is shown that the estimator of the autoregressive parameter is consistent with a well centered asymptotic normal distribution, leading to unit root tests with maximal achievable power. In fact, FA is consistent and asymptotically normal regardless of whether the data are near unit root non-stationary or stationary. It is therefore very general and hence widely applicable.",http://www.sciencedirect.com/science/article/pii/S0304407620302682
Journal of Econometrics,2021,Estimation and inference in spatial models with dominant units,Mohammad Pesaran and Cynthia Fan Yang,"In spatial econometrics literature estimation and inference are carried out assuming that the matrix of spatial or network connections has uniformly bounded absolute column sums in the number of units, n, in the network. This paper relaxes this restriction and allows for one or more units to have pervasive effects in the network. The linear–quadratic central limit theorem of Kelejian and Prucha (2001) is generalized to allow for such dominant units, and the asymptotic properties of the GMM estimators are established in this more general setting. A new bias-corrected method of moments (BMM) estimator is also proposed that avoids the problem of weak instruments by self-instrumenting the spatially lagged dependent variable. Both cases of homoskedastic and heteroskedastic errors are considered and the associated estimators are shown to be consistent and asymptotically normal, depending on the rate at which the maximum column sum of the weights matrix rises with n. The small sample properties of GMM and BMM estimators are investigated by Monte Carlo experiments and shown to be satisfactory. An empirical application to sectoral price changes in the US over the pre- and post-2008 financial crisis is also provided. It is shown that the share of capital can be estimated reasonably well from the degree of sectoral interdependence using the input–output tables, despite the evidence of dominant sectors being present in the US economy.",http://www.sciencedirect.com/science/article/pii/S0304407620302360
Journal of Econometrics,2021,Diffusion copulas: Identification and estimation,"Ruijun Bu, Kaddour Hadri and Dennis Kristensen","We propose a new semiparametric approach for modelling nonlinear univariate diffusions, where the observed process is a nonparametric transformation of an underlying parametric diffusion (UPD). This modelling strategy yields a general class of semiparametric Markov diffusion models with parametric dynamic copulas and nonparametric marginal distributions. We provide primitive conditions for the identification of the UPD parameters together with the unknown transformations from discrete samples. Likelihood-based estimators of both parametric and nonparametric components are developed and we analyse their asymptotic properties. Kernel-based drift and diffusion estimators are also proposed and shown to be normally distributed in large samples. A simulation study investigates the finite sample performance of our estimators in the context of modelling US short-term interest rates. We also present a simple application of the proposed method for modelling the CBOE volatility index data.",http://www.sciencedirect.com/science/article/pii/S0304407620302104
Journal of Econometrics,2021,Overlap in observational studies with high-dimensional covariates,"D’Amour, Alexander, Peng Ding, Avi Feller, Lihua Lei and Jasjeet Sekhon","Estimating causal effects under exogeneity hinges on two key assumptions: unconfoundedness and overlap. Researchers often argue that unconfoundedness is more plausible when more covariates are included in the analysis. Less discussed is the fact that covariate overlap is more difficult to satisfy in this setting. In this paper, we explore the implications of overlap in observational studies with high-dimensional covariates and formalize curse-of-dimensionality argument, suggesting that these assumptions are stronger than investigators likely realize. Our key innovation is to explore how strict overlap restricts global discrepancies between the covariate distributions in the treated and control populations. Exploiting results from information theory, we derive explicit bounds on the average imbalance in covariate means under strict overlap and show that these bounds become more restrictive as the dimension grows large. We discuss how these implications interact with assumptions and procedures commonly deployed in observational causal inference, including sparsity and trimming.",http://www.sciencedirect.com/science/article/pii/S0304407620302694
Journal of Econometrics,2021,The continuous-time limit of score-driven volatility models,"Giuseppe Buccheri, Fulvio Corsi, Franco Flandoli and Giulia Livieri","We provide general conditions under which a class of discrete-time volatility models driven by the score of the conditional density converges in distribution to a stochastic differential equation as the interval between observations goes to zero. We show that the form of the diffusion limit depends on: (i) the link function, (ii) the conditional second moment of the score, (iii) the normalization of the score. Interestingly, the properties of the stochastic differential equation are strictly entangled with those of the discrete-time counterpart. Score-driven models with fat-tailed densities lead to continuous-time processes with finite volatility of volatility, as opposed to fat-tailed models with a GARCH update, for which the volatility of volatility is explosive. We examine in simulations the implications of such results on approximate estimation and filtering of diffusion processes. An extension to models with a time-varying conditional mean and to conditional covariance models is also developed.",http://www.sciencedirect.com/science/article/pii/S0304407620302669
Journal of Econometrics,2021,Bootstrap based probability forecasting in multiplicative error models,Indeewara Perera and Mervyn J. Silvapulle,"As evidenced by an extensive empirical literature, multiplicative error models (MEM) show good performance in capturing the stylized facts of nonnegative time series; examples include, trading volume, financial durations, and volatility. This paper develops a bootstrap based method for producing multi-step-ahead probability forecasts for a nonnegative valued time-series obeying a parametric MEM. In order to test the adequacy of the underlying parametric model, a class of bootstrap specification tests is also developed. Rigorous proofs are provided for establishing the validity of the proposed bootstrap methods. The paper also establishes the validity of a bootstrap based method for producing probability forecasts in a class of semiparametric MEMs. Monte Carlo simulations suggest that our methods perform well in finite samples. A real data example illustrates the methods.",http://www.sciencedirect.com/science/article/pii/S0304407620300440
Journal of Econometrics,2021,Computing semiparametric efficiency bounds in discrete choice models with strategic-interactions and rational expectations,Andres Aradillas-Lopez,"This paper computes semiparametric efficiency bounds for finite-dimensional parameters in discrete choice models with nonparametric regressors in the form of conditional expectations. These can include expectations about exogenous events as well as expectations about the choices of other agents. Thus, the models studied here include incomplete-information games, social-interactions models as well as single-agent discrete choice models with uncertainty as special cases. Our bounds rely on the assumption of rational expectations and on regularity conditions of equilibrium beliefs. The paper focuses on binary-choice models but the derivation of the bounds illustrates how our approach can be extended to multinomial choice cases. Explicit efficiency bound expressions for the models examined here had not been derived before. Furthermore, since we also characterize the efficient influence functions, our results can also potentially be used to construct semiparametrically efficient estimators for these models.",http://www.sciencedirect.com/science/article/pii/S0304407620300452
Journal of Econometrics,2021,A general semiparametric approach to inference with marker-dependent hazard rate models,"Gerard. J. van den Berg, Lena Janys, Enno Mammen and Jens Perch Nielsen","We examine a new general class of hazard rate models for duration data, containing a parametric and a nonparametric component. Both can be a mix of a time effect and possibly time-dependent covariate effects. A number of well-known models are special cases. In a counting process framework, a general profile likelihood estimator is developed and the parametric component of the model is shown to be asymptotically normal and efficient. Finite sample properties are investigated in simulations. The estimator is applied to investigate the long-run relationship between birth weight and later-life mortality.",http://www.sciencedirect.com/science/article/pii/S0304407620300439
Journal of Econometrics,2021,Jackknife empirical likelihood for inequality constraints on regular functionals,Ruxin Chen and Rami V. Tabri,"Empirical likelihood is effective in many different practical situations involving moment equality and/or inequality restrictions. However, in applications with nonlinear functionals of the underlying distribution, it becomes computationally more difficult to implement. We propose the use of jackknife empirical likelihood (Jing et al., 2009) to circumvent the computational difficulties with nonlinear inequality constraints and establish the chi-bar-square distribution as the limiting null distribution of the resulting empirical likelihood-ratio statistic, where a finite number of inequalities on functionals that are regular in the sense of Hoeffding (1948), defines the null hypothesis. The class of regular functionals includes many nonlinear functionals that arise in practice and has moments as a special case. To overcome the implementation challenges with this non-pivotal asymptotic null distribution, we propose an empirical likelihood bootstrap procedure that is valid with uniformity. Finally, we investigate the finite-sample properties of the bootstrap procedure using Monte Carlo simulations and find that the results are promising.",http://www.sciencedirect.com/science/article/pii/S0304407620300373
Journal of Econometrics,2021,Efficient size correct subset inference in homoskedastic linear instrumental variables regression,Frank Kleibergen,We show that Moreira’s (2003) conditional critical value function for likelihood ratio (LR) tests on the structural parameter in homoskedastic linear instrumental variables (IV) regression provides a bounding critical value function for subset LR tests on one structural parameter of several for general homoskedastic linear IV regression. The resulting subset LR test is size correct under weak identification and efficient under strong identification. A power study shows that it outperforms the subset Anderson–Rubin test with conditional critical values from Guggenberger et al. (2019a) when the structural parameters are reasonably identified and has slightly less power when identification is weak.,http://www.sciencedirect.com/science/article/pii/S030440762030052X
Journal of Econometrics,2021,"ExpectHill estimation, extreme risk and heavy tails","Abdelaati Daouia, Stéphane Girard and Gilles Stupfler","Risk measures of a financial position are, from an empirical point of view, mainly based on quantiles. Replacing quantiles with their least squares analogues, called expectiles, has recently received increasing attention. The novel expectile-based risk measures satisfy all coherence requirements. We revisit their extreme value estimation for heavy-tailed distributions. First, we estimate the underlying tail index via weighted combinations of top order statistics and asymmetric least squares estimates. The resulting expectHill estimators are then used as the basis for estimating tail expectiles and Expected Shortfall. The asymptotic theory of the proposed estimators is provided, along with numerical simulations and applications to actuarial and financial data.",http://www.sciencedirect.com/science/article/pii/S0304407620300543
Journal of Econometrics,2021,Hierarchical Markov-switching models for multivariate integer-valued time-series,Leopoldo Catania and Roberto Di Mari,"We propose a new flexible dynamic model for multivariate nonnegative integer-valued time-series. Observations are assumed to depend on the realization of two unobserved integer-valued stochastic variables which control for the time- and cross-dependence of the data. We provide conditional and unconditional (cross)-moments implied by the model, as well as the limiting distribution of the series. An Expectation–Maximization algorithm for maximum likelihood estimation of the model parameters is derived, and an extensive Monte Carlo experiment investigates the finite sample properties of the resulting maximum likelihood estimator. Constrained specifications of the model are also formulated by modifying the assumptions about the dependence structure of the latent variables, and model identification is discussed accordingly. An application by means of a crime data set from the New South Wales (NSW) Bureau Of Crime Statistics And Research with observations spanning beyond 20 years is reported to illustrate the methodology. Results indicate that the proposed approach provides a good description of the conditional distribution of crime records, outperforming the standard hidden Markov model.",http://www.sciencedirect.com/science/article/pii/S0304407620300531
Journal of Econometrics,2021,Testing continuity of a density via g-order statistics in the regression discontinuity design,Federico Bugni and Ivan Canay,"In the regression discontinuity design (RDD), it is common practice to assess the credibility of the design by testing the continuity of the density of the running variable at the cut-off, e.g., McCrary (2008). In this paper we propose an approximate sign test for continuity of a density at a point based on the so-called g-order statistics, and study its properties under two complementary asymptotic frameworks. In the first asymptotic framework, the number q of observations local to the cut-off is fixed as the sample size n diverges to infinity, while in the second framework q diverges to infinity slowly as n diverges to infinity. Under both of these frameworks, we show that the test we propose is asymptotically valid in the sense that it has limiting rejection probability under the null hypothesis not exceeding the nominal level. More importantly, the test is easy to implement, asymptotically valid under weaker conditions than those used by competing methods, and exhibits finite sample validity under stronger conditions than those needed for its asymptotic validity. In a simulation study, we find that the approximate sign test provides good control of the rejection probability under the null hypothesis while remaining competitive under the alternative hypothesis. We finally apply our test to the design in Lee (2008), a well-known application of the RDD to study incumbency advantage.",http://www.sciencedirect.com/science/article/pii/S0304407620300579
Journal of Econometrics,2021,Likelihood inference and the role of initial conditions for the dynamic panel data model,José Diogo Barbosa and Marcelo Moreira,"Lancaster (2002) proposes an estimator for the dynamic panel data model with homoskedastic errors and zero initial conditions. In this paper, we show this estimator is invariant to orthogonal transformations, but is inefficient because it ignores additional information available in the data. The zero initial condition is trivially satisfied by subtracting initial observations from the data. We show that differencing out the data further erodes efficiency compared to drawing inference conditional on the first observations.",http://www.sciencedirect.com/science/article/pii/S0304407620301652
Journal of Econometrics,2021,Estimation of a SAR model with endogenous spatial weights constructed by bilateral variables,"Xi Qu, Lung-fei Lee and Chao Yang","This paper studies the estimation of a cross-sectional spatial autoregressive (SAR) model with spatial weights constructed by bilateral variables like the trade or investment between regions. We model the possible endogeneity in spatial weights due to the correlation between the error term in the SAR model and unobserved interactive fixed effects in bilateral variables. Using a control function approach, we propose two-stage estimation methods and establish their consistency and asymptotic normality. Finite sample properties are investigated by a Monte Carlo study. We further apply our method to an empirical study of interactions among different US industries through production networks.",http://www.sciencedirect.com/science/article/pii/S0304407620302281
Journal of Econometrics,2021,Nonstationary panel models with latent group structures and cross-section dependence,"Wenxin Huang, Sainan Jin, Peter Phillips and Liangjun Su","This paper proposes a novel Lasso-based approach to handle unobserved parameter heterogeneity and cross-section dependence in nonstationary panel models. In particular, a penalized principal component (PPC) method is developed to estimate group-specific long-run relationships and unobserved common factors and jointly to identify the unknown group membership. The PPC estimators are shown to be consistent under weakly dependent innovation processes. But they suffer an asymptotically non-negligible bias from correlations between the nonstationary regressors and unobserved stationary common factors and/or the equation errors. To remedy these shortcomings we provide three bias-correction procedures under which the estimators are re-centered about zero as both dimensions (N and T) of the panel tend to infinity. We establish a mixed normal limit theory for the estimators of the group-specific long-run coefficients, which permits inference using standard test statistics. Simulations suggest good finite sample performance. An empirical application applies the methodology to study international R&D spillovers and the results offer a convincing explanation for the growth convergence puzzle through the heterogeneous impact of R&D spillovers.",http://www.sciencedirect.com/science/article/pii/S0304407620302165
Journal of Econometrics,2021,Optimal Linear Instrumental Variables Approximations,Juan Carlos Escanciano and Wei Li,"This paper studies the identification and estimation of the optimal linear approximation of a structural regression function. The parameter in the linear approximation is called the Optimal Linear Instrumental Variables Approximation (OLIVA). This paper shows that a necessary condition for standard inference on the OLIVA is also sufficient for the existence of an IV estimand in a linear model. The instrument in the IV estimand is unknown and may not be identified. A Two-Step IV (TSIV) estimator based on Tikhonov regularization is proposed, which can be implemented by standard regression routines. We establish the asymptotic normality of the TSIV estimator assuming neither completeness nor identification of the instrument. As an important application of our analysis, we robustify the classical Hausman test for exogeneity against misspecification of the linear structural model. We also discuss extensions to weighted least squares criteria. Monte Carlo simulations suggest an excellent finite sample performance for the proposed inferences. Finally, in an empirical application estimating the elasticity of intertemporal substitution (EIS) with US data, we obtain TSIV estimates that are much larger than their standard IV counterparts, with our robust Hausman test failing to reject the null hypothesis of exogeneity of real interest rates.",http://www.sciencedirect.com/science/article/pii/S0304407620302153
Journal of Econometrics,2021,An automated approach towards sparse single-equation cointegration modelling,Stephan Smeekes and Etienne Wijler,"In this paper we propose the Single-equation Penalized Error Correction Selector (SPECS) as an automated estimation procedure for dynamic single-equation models with a large number of potentially (co)integrated variables. By extending the classical single-equation error correction model, SPECS enables the researcher to model large cointegrated datasets without necessitating any form of pre-testing for the order of integration or cointegrating rank. Under an asymptotic regime in which both the number of parameters and time series observations jointly diverge to infinity, we show that SPECS is able to consistently estimate an appropriate linear combination of the cointegrating vectors that may occur in the underlying DGP. In addition, SPECS is shown to enable the correct recovery of sparsity patterns in the parameter space and to possess the same limiting distribution as the OLS oracle procedure. A simulation study shows strong selective capabilities, as well as superior predictive performance in the context of nowcasting compared to high-dimensional models that ignore cointegration. An empirical application to nowcasting Dutch unemployment rates using Google Trends confirms the strong practical performance of our procedure.",http://www.sciencedirect.com/science/article/pii/S0304407620302190
Journal of Econometrics,2021,Estimating multiple breaks in nonstationary autoregressive models,"Tianxiao Pang, Lingjie Du and Terence Tai Leung Chong","Chong (1995) and Bai (1997) proposed a sample-splitting method to estimate a multiple-break model. However, their studies focused on stationary time series models, in which the identification of the first break depends on the magnitude and the duration of the break, and a testing procedure is needed to assist the estimation of the remaining breaks in subsamples split by the break points found earlier. In this paper, we focus on nonstationary multiple-break autoregressive models. Unlike the stationary case, we show that the duration of a break does not affect whether it will be identified first. Rather, it depends on the stochastic order of magnitude of signal strength of the break under the case of constant break magnitude and also the square of the magnitude of the break under the case of shrinking break magnitude. Since the subsamples usually have different stochastic orders in nonstationary autoregressive models with breaks, one can therefore determine which break will be identified first. We apply this finding to the models proposed in Phillips and Yu (2011) and Phillips et al. (2011, 2015a, 2015b). We propose an estimation procedure as well as the asymptotic theory for the model. Some extensions to more general models are provided, and the hypothesis test with the null hypothesis being the unit root model is examined. Results of numerical simulations and an empirical study are given to illustrate the finite-sample performance.",http://www.sciencedirect.com/science/article/pii/S0304407620302116
Journal of Econometrics,2021,Frequentist properties of Bayesian inequality tests,David Kaplan and Longhao Zhuo,"Bayesian and frequentist criteria fundamentally differ, but often posterior and sampling distributions agree asymptotically. For the corresponding single-draw experiment, we characterize the frequentist size of a certain Bayesian hypothesis test of (possibly nonlinear) inequalities. If the null hypothesis is that the parameter lies in a specified half-space, then the Bayesian test’s size equals α; if the null hypothesis is a subset of a half-space, then size is above α; otherwise, size may be equal to, above, or below α. Rejection probabilities at certain points are also characterized. Two examples illustrate our results: translog cost function curvature and ordinal distribution relationships.",http://www.sciencedirect.com/science/article/pii/S0304407620302359
Journal of Econometrics,2021,Second-order corrected likelihood for nonlinear panel models with fixed effects,Geert Dhaene and Yutao Sun,"We propose a second-order correction for nonlinear fixed-effect panel models. The correction is made via the log-likelihood function. It removes the two leading terms of the bias of the log-likelihood that arises from estimating the fixed effects. Maximizing the corrected likelihood gives a second-order bias-corrected estimator, with bias OT−3, where T is the number of time periods. The corrected likelihood also gives second-order corrected test statistics. The correction applies to general nonlinear fixed-effect models with independent observations. The bias correction properties are confirmed in simulations for binary-choice models.",http://www.sciencedirect.com/science/article/pii/S0304407620301196
Journal of Econometrics,2021,Semiparametric identification in panel data discrete response models,Eleni Aristodemou,"This paper studies semiparametric identification in linear index discrete response panel data models with fixed effects. Departing from the classic binary response static panel data model, this paper examines identification in the binary response dynamic panel data model and the ordered response static panel data model. It is shown that under mild distributional assumptions on the fixed effect and the time-varying unobservables point-identification fails, but informative bounds on the regression coefficients can still be derived. Partial identification is achieved by eliminating the fixed effect and discovering features of the distribution of the unobservable time-varying components that do not depend on the unobserved heterogeneity. Numerical analyses illustrate how the identification bounds change as the support of the explanatory variables varies.",http://www.sciencedirect.com/science/article/pii/S0304407620301202
Journal of Econometrics,2021,Identifying latent group structures in nonlinear panels,Wuyi Wang and Liangjun Su,"We propose a procedure to identify latent group structures in nonlinear panel data models where some regression coefficients are heterogeneous across groups but homogeneous within a group and the group number and membership are unknown. To identify the group structures, we consider the order statistics for the preliminary unconstrained consistent estimators of the regression coefficients and translate the problem of classification into the problem of break detection. Then we extend the sequential binary segmentation algorithm of Bai (1997) for break detection from the time series setup to the panel data framework. We demonstrate that our method is able to identify the true latent group structures with probability approaching one and the post-classification estimators are oracle-efficient. The method has the advantage of more convenient implementation compared with some alternative methods, which is a desirable feature in nonlinear panel applications. To improve the finite sample performance, we also consider an alternative version based on the spectral decomposition of certain estimated matrix and link the group identification issue to the community detection problem in the network literature. Simulations show that our method has good finite sample performance. We apply this method to explore how individuals’ portfolio choices respond to their financial status and other characteristics using the Netherlands household panel data from year 1993 to 2015, and find three latent groups.",http://www.sciencedirect.com/science/article/pii/S0304407620301214
Journal of Econometrics,2021,Nonlinear factor models for network and panel data,"Mingli Chen, Ivan Fernandez-Val and Martin Weidner","Factor structures or interactive effects are convenient devices to incorporate latent variables in panel data models. We consider fixed effect estimation of nonlinear panel single-index models with factor structures in the unobservables, which include logit, probit, ordered probit and Poisson specifications. We establish that fixed effect estimators of model parameters and average partial effects have normal distributions when the two dimensions of the panel grow large, but might suffer from incidental parameter bias. We also show how models with factor structures can be applied to capture important features of network data such as reciprocity, degree heterogeneity, homophily in latent variables, and clustering. We illustrate this applicability with an empirical example to the estimation of a gravity equation of international trade between countries using a Poisson model with multiple factors.",http://www.sciencedirect.com/science/article/pii/S0304407620301238
Journal of Econometrics,2021,On the robustness of the pooled CCE estimator,"Artūras Juodis, Hande Karabiyik and Joakim Westerlund","Among the existing estimators of factor-augmented regressions, the CCE approach is the most popular. A major reason for this popularity is the simplicity and good small-sample performance of the approach, making it very attractive from an empirical point of view. The main drawback is that most of the available asymptotic theory is based on quite restrictive assumptions, such as that the common factor component should be independent of the regressors. The present paper can be seen as a reaction to this. The purpose is to study the asymptotic properties of the pooled CCE estimator under more realistic conditions. In particular, the common factor component may be correlated with the regressors, and the true number of common factors, r, can be larger than the number of estimated factors, which in CCE is given by k+1, where k is the number of regressors. The main conclusion is that while the estimator is generally consistent, asymptotic normality can fail when r>k+1.",http://www.sciencedirect.com/science/article/pii/S0304407620301834
Journal of Econometrics,2021,Estimating and testing high dimensional factor models with multiple structural changes,"Badi Baltagi, Chihwa Kao and Fa Wang","This paper considers multiple changes in the factor loadings of a high dimensional factor model occurring at dates that are unknown but common to all subjects. Since the factors are unobservable, the problem is converted to estimating and testing structural changes in the second moments of the pseudo factors. We consider both joint and sequential estimation of the change points and show that the distance between the estimated and the true change points is Op(1). We find that the estimation error contained in the estimated pseudo factors has no effect on the asymptotic properties of the estimated change points as the cross-sectional dimension N and the time dimension T go to infinity jointly. No N-T ratio condition is needed. We also propose (i) tests for no change versus l changes (ii) tests for l changes versus l+1 changes, and show that using estimated factors asymptotically has no effect on their limit distributions if T∕N→0. These tests allow us to make inference on the presence and number of structural changes. Simulation results show good performance of the proposed procedure. In an application to US quarterly macroeconomic data we detect two possible breaks.",http://www.sciencedirect.com/science/article/pii/S030440762030124X
Journal of Econometrics,2021,Predicting the VIX and the volatility risk premium: The role of short-run funding spreads Volatility Factors,Elena Andreou and Eric Ghysels,"This paper presents an innovative approach to extract Volatility Factors which predict the VIX, the S&P500 Realized Volatility (RV) and the Variance Risk Premium (VRP). The approach is innovative along two different dimensions, namely: (1) we extract Volatility Factors from panels of filtered volatilities — in particular large panels of univariate ARCH-type models and propose methods to estimate common Volatility Factors in the presence of estimation error and (2) we price equity volatility risk using factors which go beyond the equity class namely Volatility Factors extracted from panels of volatilities of short-run funding spreads. The role of these Volatility Factors is compared with the corresponding factors extracted from the panels of the above spreads as well as related factors proposed in the literature. Our monthly short-run funding spreads Volatility Factors provide both in- and out-of-sample predictive gains for forecasting the monthly VIX, RV as well as the equity premium, while the corresponding daily volatility factors via Mixed Data Sampling (MIDAS) models provide further improvements.",http://www.sciencedirect.com/science/article/pii/S0304407620301251
Journal of Econometrics,2021,Estimation of heterogeneous panels with systematic slope variations,Jörg Breitung and Nazarii Salish,"We analyse estimation procedures for the panel data models with heterogeneous slopes. Specifically we take into account a possible dependence between regressors and heterogeneous slope coefficients, which is referred to as systematic variation. It is shown that under relevant forms of systematic slope variations (i) the pooled OLS estimator is severely biased, (ii) Swamy’s GLS estimator is inconsistent if the number of time periods T is fixed, whereas (iii) the mean-group estimator always provides consistent estimators at the risk of high variances. Following Mundlak (1978) we propose an augmentated regression which results in a simple and robust version of the pooled estimator. The latter approach avoids the risk of large standard errors of the mean-group estimator, whenever T is small. We also propose two test statistics for systematic slope variation using the Lagrange multiplier and Hausman principles. We derive their asymptotic properties and provide a local power analysis of both test statistics. Monte Carlo experiments corroborate our theoretical findings and show that for all combinations of N and T the Mundlak-type GLS estimator outperform all other estimators.",http://www.sciencedirect.com/science/article/pii/S0304407620301263
Journal of Econometrics,2021,Instrumental variable estimation of dynamic linear panel data models with defactored regressors and a multifactor error structure,"Milda Norkutė, Vasilis Sarafidis, Takashi Yamagata and Guowei Cui","This paper develops two instrumental variable (IV) estimators for dynamic panel data models with exogenous covariates and a multifactor error structure when both the cross-sectional and time series dimensions, N and T respectively, are large. The main idea is to project out the common factors from the exogenous covariates of the model, and to construct instruments based on defactored covariates. For models with homogeneous slope coefficients, we propose a two-step IV estimator. In the first step, the model is estimated consistently by employing defactored covariates as instruments. In the second step, the entire model is defactored based on estimated factors extracted from the residuals of the first-step estimation, after which an IV regression is implemented using the same instruments as in step one. For models with heterogeneous slope coefficients, we propose a mean-group-type estimator, which involves the averaging of first-step IV estimates of cross-section-specific slopes. The proposed estimators do not need to seek for instrumental variables outside the model. Furthermore, these estimators are linear, and therefore computationally robust and inexpensive. Notably, they require no bias correction. We investigate the finite sample performances of the proposed estimators and associated statistical tests, and the results show that the estimators and the tests perform well even for small N and T.",http://www.sciencedirect.com/science/article/pii/S0304407620301275
Journal of Econometrics,2021,Heterogeneous structural breaks in panel data models,Ryo Okui and Wendun Wang,"This paper develops a new model and estimation procedure for panel data that allows us to identify heterogeneous structural breaks. We model individual heterogeneity using a grouped pattern. For each group, we allow common structural breaks in the coefficients. However, the number, timing, and size of these breaks can differ across groups. We develop a hybrid estimation procedure of the grouped fixed effects approach and adaptive group fused Lasso. We show that our method can consistently identify the latent group structure, detect structural breaks, and estimate the regression parameters. Monte Carlo results demonstrate the good performance of the proposed method in finite samples. An empirical application to the relationship between income and democracy illustrates the importance of considering heterogeneous structural breaks.",http://www.sciencedirect.com/science/article/pii/S0304407620301287
Journal of Econometrics,2021,Inferential theory for heterogeneity and cointegration in large panels,Lorenzo Trapani,"This paper provides an estimation and testing framework to assess the presence and the extent of slope heterogeneity and cointegration when the units are a mixture of spurious and/or cointegrating regressions. We propose two moment estimators for the degree of heterogeneity (measured by the dispersion of the slope coefficients around their average) and for the fraction of spurious regressions, which are found to be consistent in the whole parameter space. Based on these estimators, two tests for the null hypotheses of slope homogeneity and for cointegration are proposed. Monte Carlo simulations show that both tests have the correct size and satisfactory power.",http://www.sciencedirect.com/science/article/pii/S0304407620301299
Journal of Econometrics,2021,Estimation and inference for multi-dimensional heterogeneous panel datasets with hierarchical multi-factor error structure,"George Kapetanios, Laura Serlenga and Yongcheol Shin","Given the growing availability of large datasets and following recent research trends on multi-dimensional modelling, we develop three dimensional (3D) panel data models with hierarchical error components that allow for strong cross-section dependence through unobserved heterogeneous global and local factors. We propose consistent estimation procedures by extending the common correlated effects (CCE) estimation approach proposed by Pesaran (2006). The standard CCE approach needs to be modified in order to account for the hierarchical factor structure in 3D panels. Further, we provide asymptotic theory, including new nonparametric variance estimators. The validity of the proposed approach is confirmed by Monte Carlo simulation studies. We demonstrate the empirical usefulness of the proposed framework through an application to a 3D panel gravity model of bilateral export flows.",http://www.sciencedirect.com/science/article/pii/S0304407620301305
Journal of Econometrics,2021,An econometric approach to the estimation of multi-level models,Yimin Yang and Peter Schmidt,"In this paper we consider “multidimensional” or “hierarchical” or “multilevel” models that are popular in the educational and economics literatures. Instead of two levels (individuals over time in the standard panel data model), we now have multiple levels (e.g. students in classrooms in schools in districts). We apply standard methods of analysis for econometric panel data to multilevel models. Specifically, we generalize the results of Hausman and Taylor and the subsequent literature to these models. This is a non-trivial extension because we now have more than one kind of time-invariant effect and more than one kind of “between” regression. We discuss estimation by GMM both with and without the assumption of no conditional heteroskedasticity. We also discuss endogeneity and dynamic models, and we generalize the concept of testing the exogeneity assumptions using a variable addition test.",http://www.sciencedirect.com/science/article/pii/S0304407620301317
Journal of Econometrics,2021,Detecting granular time series in large panels,Christian Brownlees and Geert Mesters,"Large economic and financial panels can include time series that influence the entire cross-section. We name such series granular. In this paper we introduce a panel data model that allows to formalize the notion of granular time series. We then propose a methodology, which is inspired by the network literature in statistics and econometrics, to detect the set of granulars when such set is unknown. The influence of the ith series in the panel is measured by the norm of the ith column of the inverse covariance matrix. We show that a detection procedure based on the column norms allows to consistently select granular series when the cross-section and time series dimensions are large. Importantly, the methodology allows to consistently detect granulars also when the series in the panel are influenced by common factors. A simulation study shows that the proposed procedures perform satisfactorily in finite samples. Our empirical study shows the granular influence of the automobile sector in US industrial production.",http://www.sciencedirect.com/science/article/pii/S0304407620301329
Journal of Econometrics,2021,Estimation of a nonparametric model for bond prices from cross-section and time series information,"Bonsoo Koo, Davide La Vecchia and Oliver Linton","We develop a novel estimation methodology for an additive nonparametric panel model that is suitable for capturing the pricing of coupon-paying government bonds followed over many time periods. We use our model to estimate the discount function and yield curve of nominally riskless government bonds. The novelty of our approach is the combination of two different techniques: cross-sectional nonparametric methods and kernel estimation for time varying dynamics in the time series context. The resulting estimator is used for predicting individual bond prices given the full schedule of their future payments. In addition, it is able to capture the yield curve shapes and dynamics commonly observed in the fixed income markets. We establish the consistency, the rate of convergence, and the asymptotic normality of the proposed estimator. A Monte Carlo exercise illustrates the good performance of the method under different scenarios. We apply our methodology to the daily CRSP bond market dataset, and compare ours with the popular Diebold and Li (2006) method.",http://www.sciencedirect.com/science/article/pii/S0304407620301330
Journal of Econometrics,2021,"Dynamic panels with MIDAS covariates: Nonlinearity, estimation and fit","Lynda Khalaf, Maral Kichian, Charles J. Saunders and Marcel Voia","This paper introduces Mixed Data Sampling (MIDAS) into the panel data context. To address the unidentified nuisance parameter problem, we propose to invert model specification tests for inference on the MIDAS parameter along with bounds tests for model coefficients. Illustrative identification, simulation and empirical analyses are conducted in the dynamic GMM framework. Our framework allows for departures from i.i.d errors such as clustering and dynamic specifications. A simulation study and an application to a model of reserve holdings illustrate the usefulness of the proposed methods, and more broadly set a promising template for shrinkage approaches.",http://www.sciencedirect.com/science/article/pii/S0304407620301342
Journal of Econometrics,2021,Panel forecasts of country-level Covid-19 infections,"Laura Liu, Hyungsik Roger Moon and Frank Schorfheide","We use a dynamic panel data model to generate density forecasts for daily active Covid-19 infections for a panel of countries/regions. Our specification that assumes the growth rate of active infections can be represented by autoregressive fluctuations around a downward sloping deterministic trend function with a break. Our fully Bayesian approach allows us to flexibly estimate the cross-sectional distribution of slopes and then implicitly use this distribution as prior to construct Bayes forecasts for the individual time series. We find some evidence that information from locations with an early outbreak can sharpen forecast accuracy for late locations. There is generally a lot of uncertainty about the evolution of active infection, due to parameter and shock uncertainty, in particular before and around the peak of the infection path. Over a one-week horizon, the empirical coverage frequency of our interval forecasts is close to the nominal credible level. Weekly forecasts from our model are published at https://laurayuliu.com/covid19-panel-forecast/.",http://www.sciencedirect.com/science/article/pii/S030440762030347X
Journal of Econometrics,2021,"Causal impact of masks, policies, behavior on early covid-19 pandemic in the U.S","Victor Chernozhukov, Hiroyuki Kasahara and Paul Schrimpf","The paper evaluates the dynamic impact of various policies adopted by US states on the growth rates of confirmed Covid-19 cases and deaths as well as social distancing behavior measured by Google Mobility Reports, where we take into consideration people’s voluntarily behavioral response to new information of transmission risks in a causal structural model framework. Our analysis finds that both policies and information on transmission risks are important determinants of Covid-19 cases and deaths and shows that a change in policies explains a large fraction of observed changes in social distancing behavior. Our main counterfactual experiments suggest that nationally mandating face masks for employees early in the pandemic could have reduced the weekly growth rate of cases and deaths by more than 10 percentage points in late April and could have led to as much as 19 to 47 percent less deaths nationally by the end of May, which roughly translates into 19 to 47 thousand saved lives. We also find that, without stay-at-home orders, cases would have been larger by 6 to 63 percent and without business closures, cases would have been larger by 17 to 78 percent. We find considerable uncertainty over the effects of school closures due to lack of cross-sectional variation; we could not robustly rule out either large or small effects. Overall, substantial declines in growth rates are attributable to private behavioral response, but policies played an important role as well. We also carry out sensitivity analyses to find neighborhoods of the models under which the results hold robustly: the results on mask policies appear to be much more robust than the results on business closures and stay-at-home orders. Finally, we stress that our study is observational and therefore should be interpreted with great caution. From a completely agnostic point of view, our findings uncover predictive effects (association) of observed policies and behavioral changes on future health outcomes, controlling for informational and other confounding variables.",http://www.sciencedirect.com/science/article/pii/S0304407620303468
Journal of Econometrics,2021,Identification and estimation of the SEIRD epidemic model for COVID-19,Ivan Korolev,"This paper studies the SEIRD epidemic model for COVID-19. First, I show that the model is poorly identified from the observed number of deaths and confirmed cases. There are many sets of parameters that are observationally equivalent in the short run but lead to markedly different long run forecasts. Second, I show that the basic reproduction number R0 can be identified from the data, conditional on epidemiologic parameters, and propose several nonlinear SUR approaches to estimate R0. I examine the performance of these methods using Monte Carlo studies and demonstrate that they yield fairly accurate estimates of R0. Next, I apply these methods to estimate R0 for the US, California, and Japan, and document heterogeneity in the value of R0 across regions. My estimation approach accounts for possible underreporting of the number of cases. I demonstrate that if one fails to take underreporting into account and estimates R0 from the reported cases data, the resulting estimate of R0 may be biased downward and the resulting forecasts may exaggerate the long run number of deaths. Finally, I discuss how auxiliary information from random tests can be used to calibrate the initial parameters of the model and narrow down the range of possible forecasts of the future number of deaths.",http://www.sciencedirect.com/science/article/pii/S0304407620302621
Journal of Econometrics,2021,Consumer panic in the COVID-19 pandemic,Michael Keane and Timothy Neal,"We develop an econometric model of consumer panic (or panic buying) during the COVID-19 pandemic. Using Google search data on relevant keywords, we construct a daily index of consumer panic for 54 countries from January 1st to April 30th 2020. We also assemble data on government policy announcements and daily COVID-19 cases for all countries. Our panic index reveals widespread consumer panic in most countries, primarily during March, but with significant variation in the timing and severity of panic between countries. Our model implies that both domestic and world virus transmission contribute significantly to consumer panic. But government policy is also important: Internal movement restrictions – whether announced by domestic or foreign governments – generate substantial short run panic that largely vanishes in a week to ten days. Internal movement restrictions announced early in the pandemic generated more panic than those announced later. Stimulus announcements had smaller impacts, and travel restrictions do not appear to generate consumer panic.",http://www.sciencedirect.com/science/article/pii/S0304407620302840
Journal of Econometrics,2021,Estimating the fraction of unreported infections in epidemics with a known epicenter: An application to COVID-19,"Ali Hortacsu, Jiarui Liu and Timothy Schwieg","We develop an analytically tractable method to estimate the fraction of unreported infections in epidemics with a known epicenter and estimate the number of unreported COVID-19 infections in the U.S. during the first half of March 2020. Our method utilizes the covariation in initial reported infections across U.S. regions and the number of travelers to these regions from the epicenter, along with the results of an early randomized testing study in Iceland. Using our estimates of the number of unreported infections, which are substantially larger than the number of reported infections, we also provide estimates for the infection fatality rate using data on reported COVID-19 fatalities from U.S. counties.",http://www.sciencedirect.com/science/article/pii/S030440762030302X
Journal of Econometrics,2021,When will the Covid-19 pandemic peak?,Shaoran Li and Oliver Linton,"We carry out some analysis of the daily data on the number of new cases and the number of new deaths by (191) countries as reported to the European Centre for Disease Prevention and Control (ECDC). Our benchmark model is a quadratic time trend model applied to the log of new cases for each country. We use our model to predict when the peak of the epidemic will arise in terms of new cases or new deaths in each country and the peak level. We also predict how long the number of new daily cases in each country will fall by an order of magnitude. Finally, we also forecast the total number of cases and deaths for each country. We consider two models that link the joint evolution of new cases and new deaths.",http://www.sciencedirect.com/science/article/pii/S0304407620303055
Journal of Econometrics,2021,Sparse HP filter: Finding kinks in the COVID-19 contact rate,"Sokbae (Simon) Lee, Yuan Liao, Myung Hwan Seo and Youngki Shin","In this paper, we estimate the time-varying COVID-19 contact rate of a Susceptible–Infected–Recovered (SIR) model. Our measurement of the contact rate is constructed using data on actively infected, recovered and deceased cases. We propose a new trend filtering method that is a variant of the Hodrick–Prescott (HP) filter, constrained by the number of possible kinks. We term it the sparse HP filter and apply it to daily data from five countries: Canada, China, South Korea, the UK and the US. Our new method yields the kinks that are well aligned with actual events in each country. We find that the sparse HP filter provides a fewer kinks than the ℓ1 trend filter, while both methods fitting data equally well. Theoretically, we establish risk consistency of both the sparse HP and ℓ1 trend filters. Ultimately, we propose to use time-varying contact growth rates to document and monitor outbreaks of COVID-19.",http://www.sciencedirect.com/science/article/pii/S0304407620303365
Journal of Econometrics,2021,Estimating the COVID-19 infection rate: Anatomy of an inference problem,Charles Manski and Francesca Molinari,"As a consequence of missing data on tests for infection and imperfect accuracy of tests, reported rates of cumulative population infection by the SARS CoV-2 virus are lower than actual rates of infection. Hence, reported rates of severe illness conditional on infection are higher than actual rates. Understanding the time path of the COVID-19 pandemic has been hampered by the absence of bounds on infection rates that are credible and informative. This paper explains the logical problem of bounding these rates and reports illustrative findings, using data from Illinois, New York, and Italy. We combine the data with assumptions on the infection rate in the untested population and on the accuracy of the tests that appear credible in the current context. We find that the infection rate might be substantially higher than reported. We also find that, assuming accurate reporting of deaths, the infection fatality rates in Illinois, New York, and Italy are substantially lower than reported.",http://www.sciencedirect.com/science/article/pii/S0304407620301676
Journal of Econometrics,2021,Estimation of Covid-19 prevalence from serology tests: A partial identification approach,Panos Toulis,"We propose a partial identification method for estimating disease prevalence from serology studies. Our data are results from antibody tests in some population sample, where the test parameters, such as the true/false positive rates, are unknown. Our method scans the entire parameter space, and rejects parameter values using the joint data density as the test statistic. The proposed method is conservative for marginal inference, in general, but its key advantage over more standard approaches is that it is valid in finite samples even when the underlying model is not point identified. Moreover, our method requires only independence of serology test results, and does not rely on asymptotic arguments, normality assumptions, or other approximations. We use recent Covid-19 serology studies in the US, and show that the parameter confidence set is generally wide, and cannot support definite conclusions. Specifically, recent serology studies from California suggest a prevalence anywhere in the range 0%-2% (at the time of study), and are therefore inconclusive. However, this range could be narrowed down to 0.7%–1.5% if the actual false positive rate of the antibody test was indeed near its empirical estimate (∼0.5%). In another study from New York state, Covid-19 prevalence is confidently estimated in the range 13%–17% in mid-April of 2020, which also suggests significant geographic variation in Covid-19 exposure across the US. Combining all datasets yields a 5%–8% prevalence range. Our results overall suggest that serology testing on a massive scale can give crucial information for future policy design, even when such tests are imperfect and their parameters unknown.",http://www.sciencedirect.com/science/article/pii/S030440762030350X
