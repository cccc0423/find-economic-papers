journal,year,title,authors,abstract,url
Journal of Econometrics,2024,Pseudo-variance quasi-maximum likelihood estimation of semi-parametric time series models,Mirko Armillotta and Paolo Gorgi,"We propose a novel estimation approach for a general class of semi-parametric time series models where the conditional expectation is modeled through a parametric function. The proposed class of estimators is based on a Gaussian quasi-likelihood function and it relies on the specification of a parametric pseudo-variance that can contain parametric restrictions with respect to the conditional expectation. The specification of the pseudo-variance and the parametric restrictions follow naturally in observation-driven models with bounds in the support of the observable process, such as count processes and double-bounded time series. We derive the asymptotic properties of the estimators and a validity test for the parameter restrictions. We show that the results remain valid irrespective of the correct specification of the pseudo-variance. The key advantage of the restricted estimators is that they can achieve higher efficiency compared to alternative quasi-likelihood methods that are available in the literature. Furthermore, the testing approach can be used to build specification tests for parametric time series models. We illustrate the practical use of the methodology in a simulation study and two empirical applications featuring integer-valued autoregressive processes, where assumptions on the dispersion of the thinning operator are formally tested, and autoregressions for double-bounded data with application to a realized correlation time series.",http://www.sciencedirect.com/science/article/pii/S0304407624002458
Journal of Econometrics,2024,From LATE to ATE: A Bayesian approach,Isaac Opper,"We develop a Bayesian model that produces a posterior distribution of the marginal treatment effect (MTE) function. The method provides researchers with a principled way to extrapolate from the observed moments using flexible assumptions, thereby allowing researchers to generate plausible ranges of important and potentially policy-relevant quantities of interest. We then use the model to propose a natural decomposition of the posterior variance into “statistical uncertainty,” i.e., variance that stems from the imprecise estimation of the observed moments, and “extrapolation uncertainty,” i.e., variance that stems from uncertainty in how to extrapolate away from the observed moments. We conclude by showing that under our preferred priors, even in an experiment as large as the Oregon Health Insurance Experiment, the main source of uncertainty in the ATE comes from uncertainty in the true values of the observed moments.",http://www.sciencedirect.com/science/article/pii/S030440762400246X
Journal of Econometrics,2024,Estimating and testing for smooth structural changes in moment condition models,"Haiqi Li, Jin Zhou and Yongmiao Hong","Numerous studies have been devoted to estimating and testing for moment condition models. Most existing studies assume that structural parameters are either fixed or change abruptly over time. This study considers estimating and testing for smooth structural changes in moment condition models where the data-generating process is locally stationary. A novel local generalized method of moments estimator and its boundary-corrected counterpart are proposed to estimate the smoothly changing parameters. Consistency and asymptotic normality are established, and an optimal weighting matrix and its consistent estimator are obtained. Moreover, we propose a consistent test to detect both smooth changes and abrupt breaks, as well as a consistent test for a parametric functional form of time-varying parameters. The tests are asymptotically pivotal and do not require prior information about the alternatives. Monte Carlo simulation studies show that the proposed estimators and tests have superior finite-sample performance. In an empirical application, we document the time-varying features of the risk aversion parameter in an asset pricing model, indicating that investors’ risk aversion is counter-cyclical.",http://www.sciencedirect.com/science/article/pii/S0304407624002471
Journal of Econometrics,2024,Multivariate spatiotemporal models with low rank coefficient matrix,"Dan Pu, Kuangnan Fang, Wei Lan, Jihai Yu and Qingzhao Zhang","Multivariate spatiotemporal data arise frequently in practical applications, often involving complex dependencies across cross-sectional units, time points and multivariate variables. In the literature, few studies jointly model the dependence in three dimensions. To simultaneously model the cross-sectional, dynamic and cross-variable dependence, we propose a multivariate reduced-rank spatiotemporal model. By imposing the low-rank assumption on the spatial influence matrix, the proposed model achieves substantial dimension reduction and has a nice interpretation, especially for financial data. Due to the innate endogeneity, we propose the quasi-maximum likelihood estimator (QMLE) to estimate the unknown parameters. A ridge-type ratio estimator is also developed to determine the rank of the spatial influence matrix. We establish the asymptotic distribution of the QMLE and the rank selection consistency of the ridge-type ratio estimator. The proposed methodology is further illustrated via extensive simulation studies and two applications to a stock market dataset and an air pollution dataset.",http://www.sciencedirect.com/science/article/pii/S0304407624002483
Journal of Econometrics,2024,Validating approximate slope homogeneity in large panels,Tim Kutta and Holger Dette,"In this paper, we introduce new inference methods for slope homogeneity in large regression panels. While most existing tests are developed for the hypothesis of slope homogeneity (equality of all individual slopes), we propose to test the more realistic relaxation of approximate slope homogeneity (similarity of all slopes). We present new test statistics for dense and sparse alternatives to approximate homogeneity. In the dense setting, the main focus of this paper, we develop statistics that converge to pivotal limits even under simultaneous temporal and intersectional dependence. We also demonstrate uniform consistency of these statistics against large classes of local alternatives. As a complementary diagnostic tool, we propose tests against sparse alternatives that are sensitive to excessive heterogeneity in a minority of slopes. Such tests can play an important role in the analysis of populations with diverse but small subgroups. A simulation study and a data example underline the usefulness of our approach.",http://www.sciencedirect.com/science/article/pii/S0304407624002495
Journal of Econometrics,2024,GLS under monotone heteroskedasticity,"Yoichi Arai, Taisuke Otsu and Mengshan Xu","The generalized least square (GLS) is one of the most basic tools in regression analyses. A major issue in implementing the GLS is estimation of the conditional variance function of the error term, which typically requires a restrictive functional form assumption for parametric estimation or smoothing parameters for nonparametric estimation. In this paper, we propose an alternative approach to estimate the conditional variance function under nonparametric monotonicity constraints by utilizing the isotonic regression method. Our GLS estimator is shown to be asymptotically equivalent to the infeasible GLS estimator with knowledge of the conditional error variance, and involves only some tuning to trim boundary observations, not only for point estimation but also for interval estimation or hypothesis testing. Simulation studies and an empirical example illustrate excellent finite sample performances of the proposed method.",http://www.sciencedirect.com/science/article/pii/S0304407624002501
Journal of Econometrics,2024,Variable selection in high dimensional linear regressions with parameter instability,"Alexander Chudik, Mohammad Pesaran and Mahrad Sharifvaghefi","This paper considers the problem of variable selection allowing for parameter instability. It distinguishes between signal and pseudo-signal variables that are correlated with the target variable, and noise variables that are not, and investigate the asymptotic properties of the One Covariate at a Time Multiple Testing (OCMT) method proposed by Chudik et al. (2018) under parameter insatiability. It is established that OCMT continues to asymptotically select an approximating model that includes all the signals and none of the noise variables. Properties of post selection regressions are also investigated, and in-sample fit of the selected regression is shown to have the oracle property. The theoretical results support the use of unweighted observations at the selection stage of OCMT, whilst applying down-weighting of observations only at the forecasting stage. Monte Carlo and empirical applications show that OCMT without down-weighting at the selection stage yields smaller mean squared forecast errors compared to Lasso, Adaptive Lasso, and boosting.",http://www.sciencedirect.com/science/article/pii/S0304407624002513
Journal of Econometrics,2024,Consistent causal inference for high-dimensional time series,Francesco Cordoni and Alessio Sancetta,"A methodology for high-dimensional causal inference in a time series context is introduced. Time series dynamics are captured by a Gaussian copula, and estimation of the marginal distribution of the data is not required. The procedure can consistently identify the parameters that describe the dynamics of the process and the conditional causal relations among the possibly high-dimensional variables, under sparsity conditions. Identification of the causal relations is in the form of a directed acyclic graph, which is equivalent to identifying the structural VAR model for the transformed variables. As illustrative applications, we consider the impact of supply-side oil shocks on the economy and the causal relations between aggregated variables constructed from the limit order book for four stock constituents of the S&P500.",http://www.sciencedirect.com/science/article/pii/S0304407624002537
Journal of Econometrics,2024,Why are replication rates so low?,Patrick Vu,"Many explanations have been offered for why replication rates are low in the social sciences, including selective publication, p-hacking, and treatment effect heterogeneity. This article emphasizes that issues with the most commonly used approach for setting sample sizes in replication studies may also play an important role. Theoretically, I show in a simple model of the publication process that we should expect the replication rate to fall below its nominal target, even when original studies are unbiased. The main mechanism is that the most commonly used approach for setting the replication sample size does not properly account for the fact that original effect sizes are estimated. Specifically, it sets the replication sample size to achieve a nominal power target under the assumption that estimated effect sizes correspond to fixed true effects. However, since there are non-linearities in the replication power function linking original effect sizes to power, ignoring the fact that effect sizes are estimated leads to systematically lower replication rates than intended. Empirically, I find that a parsimonious model accounting only for these issues can fully explain observed replication rates in experimental economics and social science, and two-thirds of the replication gap in psychology. I conclude with practical recommendations for replicators.",http://www.sciencedirect.com/science/article/pii/S0304407624002136
Journal of Econometrics,2024,On the spectral density of fractional Ornstein–Uhlenbeck processes,"Shuping Shi, Jun Yu and Chen Zhang","This paper introduces a novel and easy-to-implement method for accurately approximating the spectral density of discretely sampled fractional Ornstein–Uhlenbeck (fOU) processes. The method offers a substantial reduction in approximation error, particularly within the rough region of the fractional parameter H∈(0,0.5). This approximate spectral density has the potential to enhance the performance of estimation methods and hypothesis testing that make use of spectral densities. We introduce the approximate Whittle maximum likelihood (AWML) method for discretely sampled fOU processes, utilizing the approximate spectral density, and demonstrate that the AWML estimator exhibits properties of consistency and asymptotic normality when H∈(0,1), akin to the conventional Whittle maximum likelihood method. Through extensive simulation studies, we show that AWML outperforms existing methods in terms of estimation accuracy in finite samples. We then apply the AWML method to the trading volume of 40 financial assets. Our empirical findings reveal that the estimated Hurst parameters for these assets fall within the range of 0.10 to 0.21, indicating a rough dynamic.",http://www.sciencedirect.com/science/article/pii/S0304407624002173
Journal of Econometrics,2024,Inference in cluster randomized trials with matched pairs,"Yuehao Bai, Jizhou Liu, Azeem Shaikh and Max Tabord-Meehan","This paper studies inference in cluster randomized trials where treatment status is determined according to a “matched pairs” design. Here, by a cluster randomized experiment, we mean one in which treatment is assigned at the level of the cluster; by a “matched pairs” design, we mean that a sample of clusters is paired according to baseline, cluster-level covariates and, within each pair, one cluster is selected at random for treatment. We study the large-sample behavior of a weighted difference-in-means estimator and derive two distinct sets of results depending on if the matching procedure does or does not match on cluster size. We then propose a single variance estimator which is consistent in either regime. Combining these results establishes the asymptotic exactness of tests based on these estimators. Next, we consider the properties of two common testing procedures based on t-tests constructed from linear regressions, and argue that both are generally conservative in our framework. We additionally study the behavior of a randomization test which permutes the treatment status for clusters within pairs, and establish its finite-sample and asymptotic validity for testing specific null hypotheses. Finally, we propose a covariate-adjusted estimator which adjusts for additional baseline covariates not used for treatment assignment, and establish conditions under which such an estimator leads to strict improvements in precision. A simulation study confirms the practical relevance of our theoretical results.",http://www.sciencedirect.com/science/article/pii/S0304407624002185
Journal of Econometrics,2024,Inference in predictive quantile regressions,"Alex Maynard, Katsumi Shimotsu and Nina Kuriyama","This paper studies inference in predictive quantile regressions when the predictive regressor has a near-unit root. We derive asymptotic distributions for the quantile regression estimator and its heteroskedasticity and autocorrelation consistent (HAC) t-statistic in terms of functionals of Ornstein–Uhlenbeck processes. We then propose a switching-fully modified (FM) predictive test for quantile predictability. The proposed test employs an FM style correction with a Bonferroni bound for the local-to-unity parameter when the predictor has a near unit root. It switches to a standard predictive quantile regression test with a slightly conservative critical value when the largest root of the predictor lies in the stationary range. Simulations indicate that the test has a reliable size in small samples and good power. We employ this new methodology to test the ability of three commonly employed, highly persistent and endogenous lagged valuation regressors – the dividend price ratio, earnings price ratio, and book-to-market ratio – to predict the median, shoulders, and tails of the stock return distribution.",http://www.sciencedirect.com/science/article/pii/S0304407624002203
Journal of Econometrics,2024,Testing for strong exogeneity in Proxy-VARs,Martin Bruns and Sascha A. Keweloh,"Proxy variables have gained widespread prominence as indispensable tools for identifying structural VAR models. Analogous to instrumental variables, proxies need to be exogenous, i.e. uncorrelated with all non-target shocks. Assessing the exogeneity of proxies has traditionally relied on economic arguments rather than statistical tests. We argue that the economic rationale underlying the construction of commonly used proxy variables aligns with a stronger form of exogeneity. Specifically, proxies are typically constructed as variables not containing any information on the expected value of non-target shocks. We show conditions under which this enhanced concept of proxy exogeneity is testable without additional identifying assumptions.",http://www.sciencedirect.com/science/article/pii/S0304407624002215
Journal of Econometrics,2024,Varying-coefficient spatial dynamic panel data models with fixed effects: Theory and application,"Han Hong, Gaosheng Ju, Qi Li and Karen X. Yan",This paper considers a varying-coefficient spatial dynamic panel data model with fixed effects. We show that a two-point approximation method poses a potential weak identification problem. We propose a robust modified estimator to address this issue. Our two-step estimation procedure incorporates both linear and quadratic moment conditions. We also extend our analysis to a partially linear varying-coefficient model and develop a consistent test for this specification. We establish the asymptotic properties of the proposed estimators. Simulations indicate that our estimators and the test statistic perform well in finite samples. We apply the partially linear varying-coefficient model to study how the sales of liquor producers respond to those of neighboring competitors in China. We find spatial dependence among liquor producers and show that the spatial effects vary with competition intensity.,http://www.sciencedirect.com/science/article/pii/S0304407624002288
Journal of Econometrics,2024,Polar amplification in a moist energy balance model: A structural econometric approach to estimation and testing,William A. Brock and J. Miller,"Poleward transport of atmospheric moisture and heat play major roles in the magnification of warming in poleward latitudes per degree of global warming, a phenomenon known as polar amplification (PA). We derive a time series econometric framework using a system of equations that have error-correction mechanisms restricted across equations to estimate and an identification strategy to recover the parameters of a moist energy balance model (MEBM) similar to those in the recent climate science literature. This framework enables the climate econometrician to estimate and forecast temperature rise in latitude belts as cumulative emissions continue to grow as well as account for effects of increases in atmospheric moisture suggested by the Clausius–Clapeyron equation, a driver of spatial non-uniformity in climate change. Non-uniformity is important for two reasons: climate change has unequal economic consequences that need to be better understood and amplification of temperatures in polar latitudes may trigger irreversible climate tipping points, which are disproportionately located in those regions.",http://www.sciencedirect.com/science/article/pii/S0304407624002306
Journal of Econometrics,2024,Target PCA: Transfer learning large dimensional panel data,"Junting Duan, Markus Pelger and Ruoxuan Xiong","This paper develops a novel method to estimate a latent factor model for a large target panel with missing observations by optimally using the information from auxiliary panel data sets. We refer to our estimator as target-PCA. Transfer learning from auxiliary panel data allows us to deal with a large fraction of missing observations and weak signals in the target panel. We show that our estimator is more efficient and can consistently estimate weak factors, which are not identifiable with conventional methods. We provide the asymptotic inferential theory for target-PCA under very general assumptions on the approximate factor model and missing patterns. In an empirical study of imputing data in a mixed-frequency macroeconomic panel, we demonstrate that target-PCA significantly outperforms all benchmark methods.",http://www.sciencedirect.com/science/article/pii/S0304407623002373
Journal of Econometrics,2024,State-dependent local projections,"Sílvia Gonçalves, Ana María Herrera, Lutz Kilian and Elena Pesavento","Do state-dependent local projections asymptotically recover the population responses of macroeconomic aggregates to structural shocks? The answer to this question depends on how the state of the economy is determined and on the magnitude of the shocks. When the state is exogenous, the local projection estimator recovers the population response regardless of the shock size. When the state depends on macroeconomic shocks, as is common in empirical work, local projections only recover the conditional response to an infinitesimal shock, but not the responses to larger shocks of interest in many applications. Simulations suggest that impulse responses may be off by as much as 82 percent and fiscal multipliers by as much as 40 percent.",http://www.sciencedirect.com/science/article/pii/S0304407624000484
Journal of Econometrics,2024,Local projections vs. VARs: Lessons from thousands of DGPs,"Dake Li, Mikkel Plagborg-Moller and Christian K. Wolf","We conduct a simulation study of Local Projection (LP) and Vector Autoregression (VAR) estimators of structural impulse responses across thousands of data generating processes, designed to mimic the properties of the universe of U.S. macroeconomic data. Our analysis considers various identification schemes and several variants of LP and VAR estimators, employing bias correction, shrinkage, or model averaging. A clear bias–variance trade-off emerges: LP estimators have lower bias than VAR estimators, but they also have substantially higher variance at intermediate and long horizons. Bias-corrected LP is the preferred method if and only if the researcher overwhelmingly prioritizes bias. For researchers who also care about precision, VAR methods are the most attractive—Bayesian VARs at short and long horizons, and least-squares VARs at intermediate and long horizons.",http://www.sciencedirect.com/science/article/pii/S030440762400068X
Journal of Econometrics,2024,Local projections in unstable environments,"Atsushi Inoue, Barbara Rossi and Yiru Wang","This paper develops a local projection estimator for estimating impulse responses in the presence of time variation. Importantly, we allow local instabilities in both slope coefficients and variances. Monte Carlo simulations illustrate that the method performs well in practice. Using our proposed estimator, we shed new light on the effects of fiscal policy shocks and the size of government spending multipliers. Our analysis uncovers the existence of instabilities that were unaccounted for in previous studies, and links time variation in the multipliers to the size of government debt.",http://www.sciencedirect.com/science/article/pii/S0304407624000721
Journal of Econometrics,2024,Reprint of: Robust inference on correlation under general heterogeneity,"Liudas Giraitis, Yufei Li and Peter Phillips","Considerable evidence in past research shows size distortion in standard tests for zero autocorrelation or zero cross-correlation when time series are not independent identically distributed random variables, pointing to the need for more robust procedures. Recent tests for serial correlation and cross-correlation in Dalla, Giraitis, and Phillips (2022) provide a more robust approach, allowing for heteroskedasticity and dependence in uncorrelated data under restrictions that require a smooth, slowly-evolving deterministic heteroskedasticity process. The present work removes those restrictions and validates the robust testing methodology for a wider class of innovations and regression residuals allowing for heteroscedastic uncorrelated and non-stationary data settings. The updated analysis given here enables more extensive use of the methodology in practical applications. Monte Carlo experiments confirm excellent finite sample performance of the robust test procedures even for extremely complex white noise processes. The empirical examples show that use of robust testing methods can materially reduce spurious evidence of correlations found by standard testing procedures.",http://www.sciencedirect.com/science/article/pii/S0304407624000903
Journal of Econometrics,2024,Reprint of: The likelihood ratio test for structural changes in factor models,"Jushan Bai, Jiangtao Duan and Xu Han","A factor model with a break in its factor loadings is observationally equivalent to a model without changes in the loadings but with a change in the variance of its factors. This approach effectively transforms a high-dimensional structural change problem into a low-dimensional problem. This paper considers the likelihood ratio (LR) test for a variance change in the estimated factors. The LR test implicitly explores a special feature of the estimated factors: the pre-break and post-break variances can be a singular matrix under the alternative hypothesis, making the LR test diverging faster and thus more powerful than Wald-type tests. The better power property of the LR test is also confirmed by simulations. We also consider mean changes and multiple breaks. We apply this procedure to the factor modeling of the US employment and study the structural change problem using monthly industry-level data.",http://www.sciencedirect.com/science/article/pii/S0304407624000915
Journal of Econometrics,2024,Reprint of: Out-of-sample tests for conditional quantile coverage: An application to Growth-at-Risk,"Valentina Corradi, Jack Fosten and Daniel Gutknecht","This paper proposes tests for out-of-sample comparisons of interval forecasts based on parametric conditional quantile models. The tests rank the distance between actual and nominal conditional coverage with respect to the set of conditioning variables from all models, for a given loss function. We propose a pairwise test to compare two models for a single predictive interval. The set-up is then extended to a comparison across multiple models and/or intervals. The limiting distribution varies depending on whether models are strictly non-nested or overlapping. In the latter case, degeneracy may occur. We establish the asymptotic validity of wild bootstrap based critical values across all cases. An empirical application to Growth-at-Risk (GaR) uncovers situations in which a richer set of financial indicators are found to outperform a commonly-used benchmark model when predicting downside risk to economic activity.",http://www.sciencedirect.com/science/article/pii/S0304407624000927
Journal of Econometrics,2024,Vector autoregressions with dynamic factor coefficients and conditionally heteroskedastic errors,"Paolo Gorgi, Siem Jan Koopman and Julia Schaumburg","We introduce a new and general methodology for analyzing vector autoregressive models with time-varying coefficient matrices and conditionally heteroskedastic disturbances. The proposed approach is transparent and simple to implement. It allows the derivation of well-defined impulse response functions that rely on the overall stability of the system. We present the finite sample properties of the model in a simulation study. In an empirical illustration we investigate the possibly time-varying relationships between U.S. industrial production, inflation, and bond spread. We empirically identify a time-varying linkage between economic and financial variables which are effectively described by a common dynamic factor. The impulse response analysis identifies substantial differences in the effects of financial shocks on output and inflation during crisis and non-crisis periods. The results also illustrate how the widely-used approach of fixing the VAR coefficients in the derivation of the impulse responses leads to a sizeable underestimation of the impact of a financial shock on output and inflation during some of the crises in our sample.",http://www.sciencedirect.com/science/article/pii/S0304407624000964
Journal of Econometrics,2024,Functional quantile autoregression,"Chaohua Dong, Rong Chen, Zhijie Xiao and Weiyi Liu","This paper proposes a new class of time series models, the functional quantile autoregression (FQAR) models, in which the conditional distribution of the observation at the current time point is affected by its past distributional information, and is expressed as a functional of the past conditional quantile functions. Different from the conventional functional time series models which are based on functionally observed data, the proposed FQAR method studies functional dynamics in traditional time series data. We propose a sieve estimator for the model. Asymptotic properties of the estimators are derived. Numerical investigations are conducted to highlight the proposed method.",http://www.sciencedirect.com/science/article/pii/S0304407624001118
Journal of Econometrics,2024,Some fixed-b results for regressions with high frequency data over long spans,Taeyoon Hwang and Timothy J. Vogelsang,"This paper develops fixed-b asymptotic results for heteroskedasticity autocorrelation robust (HAR) Wald tests for high frequency data using the continuous time framework of Chang et al. (2023) (CLP). It is shown that the fixed-b limit of HAR Wald tests for high frequency stationary regressions is the same as the standard fixed-b limit in Kiefer and Vogelsang (2005). For the case of cointegrating regression the form of the fixed-b limits are different from the stationary case and may or may not be pivotal but also have the same fixed-b limits that have been obtained for tests based on ordinary least squares (OLS) (Bunzel, 2006) and integrated modified OLS (Vogelsang and Wagner, 2014). A simulation study shows that fixed-b critical values provide rejection probabilities closer to nominal levels than traditional chi-square critical values when using data-dependent bandwidths. The Andrews (1991) data-dependent method works reasonably well for a wider range of persistence parameters than those considered by CLP. In contrast, the Newey and West (1994) data-dependent method is sensitive to the choice of pre-tuning parameters. The data-dependent method of Sun et al. (2008) give results similar to the Andrews (1991) method with slightly less over-rejection problems when used with fixed-b critical values. Our results for bandwidth choice reinforce the importance of high frequency compatibility of bandwidths as emphasized by CLP. Regardless of the bandwidth method used in practice, it is clear that fixed-b critical values can and should be used for high frequency data whenever HAR tests are based on kernel estimators of long run variances. Our results complement the analysis of Pellatt and Sun (2023) who focused on HAR tests based on orthonormal series estimators of long run variance estimator.",http://www.sciencedirect.com/science/article/pii/S0304407624001192
Journal of Econometrics,2024,Scenario-based quantile connectedness of the U.S. interbank liquidity risk network,"Tomohiro Ando, Jushan Bai, Lina Lu and Cindy M. Vojtech","We characterize the U.S. interbank liquidity risk network based on a supervisory dataset, using a scenario-based quantile network connectedness approach. In terms of methodology, we consider a quantile vector autoregressive model with unobserved heterogeneity and propose a Bayesian nuclear norm estimation method. A common factor structure is employed to deal with unobserved heterogeneity that may exhibit endogeneity within the network. Then we develop a scenario-based quantile network connectedness framework by accommodating various economic scenarios, through a scenario-based moving average expression of the model where forecast error variance decomposition under a future pre-specified scenario is derived. The methodology is used to study the quantile-dependent liquidity risk network among large U.S. bank holding companies. The estimated quantile liquidity risk network connectedness measures could be useful for bank supervision and financial stability monitoring by providing leading indicators of the system-wide liquidity risk connectedness not only at the median but also at the tails or even under a pre-specified scenario. The measures also help identify systemically important banks and vulnerable banks in the liquidity risk transmission of the U.S. banking system.",http://www.sciencedirect.com/science/article/pii/S0304407624001325
Journal of Econometrics,2024,Specification tests for non-Gaussian structural vector autoregressions,"Dante Amengual, Gabriele Fiorentini and Enrique Sentana","We propose specification tests for independent component analysis and structural vector autoregressions that assess the cross-sectional independence of non-Gaussian shocks by comparing their joint cumulative distribution with the product of their marginals at both discrete and continuous grids of argument values, the latter yielding a consistent test. We explicitly consider the sampling variability from computing the shocks using consistent estimators. We study the finite sample size of resampled versions of our tests in simulation exercises and show their non-negligible power against a variety of empirically plausible alternatives. Finally, we apply them to a dynamic model for three popular volatility indices.",http://www.sciencedirect.com/science/article/pii/S0304407624001490
Journal of Econometrics,2024,Introduction to the Themed Issue: Macroeconometrics,Zhongjun Qu,,http://www.sciencedirect.com/science/article/pii/S030440762400215X
Journal of Econometrics,2024,Estimation of continuous-time linear DSGE models from discrete-time measurements,"Bent Jesper Christensen, Luca Neri and Juan Parra-Alvarez","We provide a general state space framework for estimation of the parameters of continuous-time linear DSGE models from discrete-time data. Our approach relies on the exact discrete-time representation of the equilibrium dynamics, hence avoiding discretization errors. We construct the exact likelihood for data sampled either as stocks or flows, based on the Kalman filter, and provide necessary and sufficient conditions for local identification of the frequency-invariant structural parameters of the underlying continuous-time model. We recover the unobserved structural shocks at measurement times from the reduced-form residuals in the state space representation by exploiting the underlying causal links implied by the economic model. We illustrate our approach using an off-the-shelf real business cycle model. Extensive Monte Carlo experiments show that the finite sample properties of our estimator are superior to those of an estimator relying on a naive Euler–Maruyama discretization of the economic model. In an application to postwar U.S. macroeconomic data, we estimate the model using series sampled at mixed frequencies, and combinations of series sampled as stocks and flows, and we provide a historical decomposition of the effects of shocks on observables into those stemming from structural supply and demand shocks.",http://www.sciencedirect.com/science/article/pii/S0304407624002161
Journal of Econometrics,2024,Tuning-parameter-free propensity score matching approach for causal inference under shape restriction,Yukun Liu and Jing Qin,"Propensity score matching (PSM) is a pseudo-experimental method that uses statistical techniques to construct an artificial control group by matching each treated unit with one or more untreated units of similar characteristics. To date, the problem of determining the optimal number of matches per unit, which plays an important role in PSM, has not been adequately addressed. We propose a tuning-parameter-free PSM approach to causal inference based on the nonparametric maximum-likelihood estimation of the propensity score under the monotonicity constraint. The estimated propensity score is piecewise constant, and therefore automatically groups data. Hence, our proposal is free of tuning parameters. The proposed causal effect estimator is asymptotically semiparametric efficient when the covariate is univariate or the outcome and the propensity score depend on the covariate through the same index X⊤β. We conclude that matching methods based on the propensity score alone cannot, in general, be efficient.",http://www.sciencedirect.com/science/article/pii/S030440762400174X
Journal of Econometrics,2024,Fixed-b asymptotics for panel models with two-way clustering,Kaicheng Chen and Timothy J. Vogelsang,"This paper studies a cluster robust variance estimator proposed by Chiang, Hansen and Sasaki (2024) for linear panels. First, we show algebraically that this variance estimator (CHS estimator, hereafter) is a linear combination of three common variance estimators: the one-way unit cluster estimator, the “HAC of averages” estimator, and the “average of HACs” estimator. Based on this finding, we obtain a fixed-b asymptotic result for the CHS estimator and corresponding test statistics as the cross-section and time sample sizes jointly go to infinity. Furthermore, we propose two simple bias-corrected versions of the variance estimator and derive the fixed-b limits. In a simulation study, we find that the two bias-corrected variance estimators along with fixed-b critical values provide improvements in finite sample coverage probabilities. We illustrate the impact of bias-correction and use of the fixed-b critical values on inference in an empirical example on the relationship between industry profitability and market concentration.",http://www.sciencedirect.com/science/article/pii/S0304407624001763
Journal of Econometrics,2024,An unbounded intensity model for point processes,Kim Christensen and Aleksey Kolokolov,"We develop a model for point processes on the real line, where the intensity can be locally unbounded without inducing an explosion. In contrast to an orderly point process, for which the probability of observing more than one event over a short time interval is negligible, the bursting intensity causes an extreme clustering of events around the singularity. We propose a nonparametric approach to detect such bursts in the intensity. It relies on a heavy traffic condition, which admits inference for point processes over a finite time interval. With Monte Carlo evidence, we show that our testing procedure exhibits size control under the null, whereas it has high rejection rates under the alternative. We implement our approach on high-frequency data for the EUR/USD spot exchange rate, where the test statistic captures abnormal surges in trading activity. We detect a nontrivial amount of intensity bursts in these data and describe their basic properties. Trading activity during an intensity burst is positively related to volatility, illiquidity, and the probability of observing a drift burst. The latter effect is reinforced if the order flow is imbalanced or the price elasticity of the limit order book is large.",http://www.sciencedirect.com/science/article/pii/S0304407624001854
Journal of Econometrics,2024,Threshold spatial autoregressive model,Kunpeng Li and Wei Lin,"In this paper, we consider the estimation and inferential issues of the threshold spatial autoregressive (TSAR) model, which is a hybrid of the threshold and spatial autoregressive models. We use the quasi maximum likelihood (QML) method to estimate the model. In addition, we prove the tightness and the Hájek–Rényi type inequality for a quadratic form and establish a full inferential theory of the QML estimator under the setup that threshold effect shrinks to zero as the sample size increases. We conduct hypothesis testing on the presence of the threshold effect, using three super-type statistics. Their asymptotic behaviors are studied under the Pitman local alternatives. A bootstrap procedure is applied to obtain the asymptotically correct critical value. We also consider hypothesis testing on the threshold value set equal to a prespecified one. We run Monte Carlo simulations to investigate the finite sample performance of the QML estimators and find that the estimators perform well. In an empirical application, we apply the proposed TSAR model to study the relationship between financial development and economic growth, and we find firm evidence to support the TSAR model.",http://www.sciencedirect.com/science/article/pii/S0304407624001866
Journal of Econometrics,2024,Measuring diagnostic test performance using imperfect reference tests: A partial identification approach,Filip Obradović,"Diagnostic tests are almost never perfect. Studies quantifying their performance use knowledge of the true health status, measured with a reference diagnostic test. Researchers commonly assume that the reference test is perfect, which is often not the case in practice. When the assumption fails, conventional studies identify “apparent” performance or performance with respect to the reference, but not true performance. This paper provides the smallest possible bounds on the measures of true performance — sensitivity (true positive rate) and specificity (true negative rate), or equivalently false positive and negative rates, in standard settings. Implied bounds on policy-relevant parameters are derived: (1) Prevalence in screened populations; (2) Predictive values. Methods for inference based on moment inequalities are used to construct uniformly consistent confidence sets in level over a relevant family of data distributions. Emergency Use Authorization (EUA) and independent study data for the BinaxNOW COVID-19 antigen test demonstrate that the bounds can be very informative. Analysis reveals that the estimated false negative rates for symptomatic and asymptomatic patients are up to 3.17 and 4.59 times higher than the frequently cited “apparent” false negative rate. Further applicability of the results in the context of imperfect proxies such as survey responses and imputed protected classes is indicated.",http://www.sciencedirect.com/science/article/pii/S0304407624001878
Journal of Econometrics,2024,Latent utility and permutation invariance: A revealed preference approach,Roy Allen and John Rehbeck,"This paper provides partial identification results for latent utility models that satisfy an invariance property on unobservables such as exchangeability. We employ a simple revealed preference argument to “difference out” unobservables, obtaining identifying inequalities for utility indices. We show the differencing argument is also useful for counterfactual analysis. The framework generalizes existing work in discrete choice by allowing latent feasibility sets and by allowing individuals to purchase multiple (possibly continuous) goods. We present a new framework leveraging nesting structures that generalizes nested logit. In a panel setting, we innovate by allowing preferences for variety.",http://www.sciencedirect.com/science/article/pii/S0304407624001891
Journal of Econometrics,2024,Testing for sparse idiosyncratic components in factor-augmented regression models,Jad Beyhum and Jonas Striaukas,"We propose a novel bootstrap test of a dense model, namely factor regression, against a sparse plus dense alternative model augmented with sparse idiosyncratic components. The asymptotic properties of the test are established under time series dependence and polynomial tails. We outline a data-driven rule to select the tuning parameter and prove its theoretical validity. In simulation experiments, our procedure exhibits high power against sparse alternatives and low power against dense deviations from the null. Moreover, we apply our test to various datasets in macroeconomics and finance and often reject the null. This suggests the presence of sparsity — on top of a dense component — in commonly studied economic applications. The R package ‘FAS’ implements our approach.",http://www.sciencedirect.com/science/article/pii/S0304407624001908
Journal of Econometrics,2024,A method of moments approach to asymptotically unbiased Synthetic Controls,Joseph Fry,"A common approach to constructing a Synthetic Control unit is to fit on the outcome variable and covariates in pre-treatment time periods, but it has been shown by Ferman and Pinto (2021) that this approach does not provide asymptotic unbiasedness when the fit is imperfect and the number of controls is fixed. Many related panel methods have a similar limitation when the number of units is fixed. I introduce and evaluate a new method in which the Synthetic Control is constructed using a General Method of Moments approach where units not being included in the Synthetic Control are used as instruments. I show that a Synthetic Control Estimator of this form will be asymptotically unbiased as the number of pre-treatment time periods goes to infinity, even when pre-treatment fit is imperfect and the number of units is fixed. Furthermore, if both the number of pre-treatment and post-treatment time periods go to infinity, then averages of treatment effects can be consistently estimated. I provide a model selection procedure for deciding whether a unit should be used as an instrument or as a control. I also conduct simulations and an empirical application to compare the performance of this method with existing approaches in the literature.",http://www.sciencedirect.com/science/article/pii/S030440762400191X
Journal of Econometrics,2024,Empirical risk minimization for time series: Nonparametric performance bounds for prediction,Christian Brownlees and Jordi Llorens-Terrazas,Empirical risk minimization is a standard principle for choosing algorithms in learning theory. In this paper we study the properties of empirical risk minimization for time series. The analysis is carried out in a general framework that covers different types of forecasting applications encountered in the literature. We are concerned with 1-step-ahead prediction of a univariate time series belonging to a class of location-scale parameter-driven processes. A class of recursive algorithms is available to forecast the time series. The algorithms are recursive in the sense that the forecast produced in a given period is a function of the lagged values of the forecast and of the time series. The relationship between the generating mechanism of the time series and the class of algorithms is not specified. Our main result establishes that the algorithm chosen by empirical risk minimization achieves asymptotically the optimal predictive performance that is attainable within the class of algorithms.,http://www.sciencedirect.com/science/article/pii/S0304407624001945
Journal of Econometrics,2024,Large Bayesian SVARs with linear restrictions,Chenghan Hou,"This paper develops a Markov Chain Monte Carlo (MCMC) algorithm for Bayesian inference in large structural vector autoregressions (SVARs) with linear restrictions. Our proposed method is based on a novel parameter transformation scheme, which aims to facilitate sampling from the posterior distribution of model parameters when linear equality and inequality restrictions are imposed on contemporaneous impulse responses. A prominent feature of the proposed methodology is its applicability for inference in SVARs with over-identifying restrictions. In our empirical application, we demonstrate the usefulness of our method by employing a large Proxy-SVAR with multiple proxy variables to simultaneously identify multiple macroeconomic shocks and investigate their contributions to the 2007–09 Recession.",http://www.sciencedirect.com/science/article/pii/S0304407624001957
Journal of Econometrics,2024,High-dimensional model-assisted inference for treatment effects with multi-valued treatments,Wenfu Xu and Zhiqiang Tan,"Consider estimation of average treatment effects with multi-valued treatments using augmented inverse probability weighted (IPW) estimators, depending on outcome regression and propensity score models in high-dimensional settings. These regression models are often fitted by regularized likelihood-based estimation, while ignoring how the fitted functions are used in the subsequent inference about the treatment parameters. Such separate estimation can be associated with known difficulties in existing methods. We develop regularized calibrated estimation for fitting propensity score and outcome regression models, where sparsity-including penalties are employed to facilitate variable selection but the loss functions are carefully chosen such that valid confidence intervals can be obtained under possible model misspecification. Unlike in the case of binary treatments, the usual augmented IPW estimator is generalized to ensure just-identification of parameters from new calibration equations. For propensity score estimation, the new loss function and estimating functions are directly tied to achieving covariate balance between weighted treatment groups. We develop practical algorithms for computing the regularized calibrated estimators with group Lasso by innovatively exploiting Fisher scoring, and provide rigorous high-dimensional analysis for the resulting augmented IPW estimators under suitable sparsity conditions, while tackling technical issues absent or overlooked in previous analyses. We present simulation studies and an empirical application to estimate the effects of maternal smoking on birth weights. The proposed methods are implemented in the R package mRCAL.",http://www.sciencedirect.com/science/article/pii/S0304407624001970
Journal of Econometrics,2024,GMM estimation for high-dimensional panel data models,"Tingting Cheng, Chaohua Dong, Jiti Gao and Oliver Linton","In this paper, we study a class of high dimensional moment restriction panel data models with interactive effects, where the factors are unobserved and these factor loadings are nonparametrically unknown smooth functions of individual characteristic variables. We allow the dimension of the parameter vector and the number of moment conditions to diverge with the sample size. This is a very general framework and is closely related to many existing linear and nonlinear panel data models. In order to estimate the unknown parameters, factors and factor loadings, we propose a sieve-based generalized method of moments estimation method and we show that under a set of simple identification conditions, all those unknown quantities can be consistently estimated. Further we establish asymptotic distributions of the proposed estimators. In addition, we propose tests for over-identification, specification of factor loading functions, and establish their large sample properties. Moreover, a number of simulation studies are conducted to examine the performance of the proposed estimators and test statistics in finite samples. An empirical example on stock return prediction is studied to demonstrate both the empirical relevance and the applicability of the proposed framework and corresponding estimation and testing methods.",http://www.sciencedirect.com/science/article/pii/S0304407624001982
Journal of Econometrics,2024,Identification in discrete choice models with imperfect information,Cristina Gualdani and Shruti Sinha,"We study identification of preferences in static single-agent discrete choice models where decision makers may be imperfectly informed about the state of the world. Leveraging the notion of one-player Bayes Correlated Equilibrium by Bergemann and Morris (2016), we provide a tractable characterisation of the sharp identified set. We develop a procedure to practically construct the sharp identified set following a sieve approach, and provide sharp bounds on counterfactual outcomes of interest. Using our methodology and data on the 2017 UK general election, we estimate a spatial voting model under weak assumptions on agents’ information about the returns to voting. Counterfactual exercises quantify the consequences of imperfect information on the well-being of voters and parties.",http://www.sciencedirect.com/science/article/pii/S0304407624001994
Journal of Econometrics,2024,Heterogeneous treatment effect bounds under sample selection with an application to the effects of social media on political polarization,Phillip Heiler,"We propose a method for estimation and inference for bounds for heterogeneous causal effect parameters in general sample selection models where the treatment can affect whether an outcome is observed and no exclusion restrictions are available. The method provides conditional effect bounds as functions of policy relevant pre-treatment variables. It allows for conducting valid statistical inference on the unidentified conditional effects. We use a flexible debiased/double machine learning approach that can accommodate non-linear functional forms and high-dimensional confounders. Easily verifiable high-level conditions for estimation, misspecification robust confidence intervals, and uniform confidence bands are provided as well. We re-analyze data from a large scale field experiment on Facebook on counter-attitudinal news subscription with attrition. Our method yields substantially tighter effect bounds compared to conventional methods and suggests depolarization effects for younger users.",http://www.sciencedirect.com/science/article/pii/S030440762400201X
Journal of Econometrics,2024,Identification and estimation of unconditional policy effects of an endogenous binary treatment: An unconditional MTE approach,Julian Martinez-Iriarte and Yixiao Sun,"This paper studies the identification and estimation of policy effects when treatment status is binary and endogenous. We introduce a new class of marginal treatment effects (MTEs) based on the influence function of the functional underlying the policy target. We show that an unconditional policy effect can be represented as a weighted average of the newly defined MTEs over the individuals who are indifferent about their treatment status. We provide conditions for point identification of the unconditional policy effects. When a quantile is the functional of interest, we introduce the UNconditional Instrumental Quantile Estimator (UNIQUE) and establish its consistency and asymptotic distribution. In the empirical application, we estimate the effect of changing college enrollment status, induced by higher tuition subsidy, on the quantiles of the wage distribution.",http://www.sciencedirect.com/science/article/pii/S0304407624002033
Journal of Econometrics,2024,A gentle introduction to matrix calculus,Jan R. Magnus,"Matrix calculus is an important tool when we wish to optimize functions involving matrices or perform sensitivity analyses. This tutorial is designed to make matrix calculus more accessible to graduate students and young researchers. It contains the theory that would suffice in most applications, many fully worked-out exercises and examples, and presents some of the ‘tacit knowledge’ that is prevalent in this field.",http://www.sciencedirect.com/science/article/pii/S0304407624002070
Journal of Econometrics,2024,Estimating option pricing models using a characteristic function-based linear state space representation,"H. Peter Boswijk, Roger Laeven and Evgenii Vladimirov","We develop a novel filtering and estimation procedure for parametric option pricing models driven by general affine jump-diffusions. Our procedure is based on the comparison between an option-implied, model-free representation of the conditional log-characteristic function and the model-implied conditional log-characteristic function, which is functionally affine in the model’s state vector. We formally derive an associated linear state space representation and the asymptotic properties of the corresponding measurement errors. The state space representation allows us to use a suitably modified Kalman filtering technique to learn about the latent state vector and a quasi-maximum likelihood estimator of the model parameters, for which we establish asymptotic inference results. Accordingly, the filtering and estimation procedure brings important computational advantages. We analyze the finite-sample behavior of our procedure in Monte Carlo simulations. The applicability of our procedure is illustrated in two case studies that analyze S&P 500 option prices and the impact of exogenous state variables capturing Covid-19 reproduction and economic policy uncertainty.",http://www.sciencedirect.com/science/article/pii/S0304407624002094
Journal of Econometrics,2024,On uniform confidence intervals for the tail index and the extreme quantile,Yuya Sasaki and Yulong Wang,"This paper presents two results concerning uniform confidence intervals for the tail index and the extreme quantile. First, we show that there exists a lower bound of the length for confidence intervals that satisfy the correct uniform coverage over a nonparametric family of tail distributions. Second, in light of the impossibility result, we construct honest confidence intervals that are uniformly valid by incorporating the worst-case bias in the nonparametric family. The proposed method is applied to simulated data and real data of financial time series.",http://www.sciencedirect.com/science/article/pii/S0304407624002100
Journal of Econometrics,2024,Earnings dynamics and intergenerational transmission of skill,Lance Lochner and Youngmin Park,"This paper develops and estimates a two-factor model of intergenerational skill transmission when earnings inequality reflects differences in individual skills and other non-skill shocks. We consider heterogeneity in both initial skills and skill growth rates, allowing variation in skill growth to change over the lifecycle. Using administrative tax data on two linked generations of Canadians covering 37 years, we exploit covariances in log earnings (at different ages) both across and within generations to identify and estimate the intergenerational correlation structure for initial skills and skill growth rates, lifecycle skill growth profiles, and the dynamics of non-skill earnings shocks.",http://www.sciencedirect.com/science/article/pii/S0304407622000343
Journal of Econometrics,2024,Some children left behind: Variation in the effects of an educational intervention,"Julie Buhl-Wiggers, Jason Kerwin, Juan Munoz-Morales, Jeffrey Smith and Rebecca Thornton","We document substantial variation in the effects of a highly-effective literacy program in northern Uganda. The program increases test scores by 1.4 SDs on average, but standard statistical bounds show that the impact standard deviation exceeds 1.0 SD. This implies that the variation in effects across our students is wider than the spread of mean effects across all randomized evaluations of developing country education interventions in the literature. This very effective program does indeed leave some students behind. At the same time, we do not learn much from our analyses that attempt to determine which students benefit more or less from the program. We reject rank preservation, and the weaker assumption of stochastic increasingness leaves wide bounds on quantile-specific average treatment effects. Neither conventional nor machine-learning approaches to estimating systematic heterogeneity capture more than a small fraction of the variation in impacts given our available candidate moderators.",http://www.sciencedirect.com/science/article/pii/S0304407622000355
Journal of Econometrics,2024,You are what your parents expect: Height and local reference points,"Fan Wang, Esteban Puentes, Jere Behrman and Flávio Cunha","Recent estimates are that about 150 million children under five years of age are stunted, with substantial negative consequences for their schooling, cognitive skills, health, and economic productivity. Therefore, understanding what determines such growth retardation is significant for designing public policies that aim to address this issue. We build a model for nutritional choices and health with reference-dependent preferences. Parents care about the health of their children relative to some reference population. In our empirical model, we use height as the health outcome that parents target. Reference height is an equilibrium object determined by earlier cohorts’ parents’ nutritional choices in the same village. We explore the exogenous variation in reference height produced by a protein-supplementation experiment in Guatemala to estimate our model’s parameters. We use our model to decompose the impact of the protein intervention on height into price and reference-point effects. We find that the changes in reference points account for 65% of the height difference between two-year-old children in experimental and control villages in the sixth annual cohort born after the initiation of the intervention.",http://www.sciencedirect.com/science/article/pii/S0304407622000562
Journal of Econometrics,2024,Gender pension gaps in a private retirement accounts system: A dynamic model of household labor supply and savings,Clement Joubert and Petra Todd,"This paper develops and estimates a dynamic model of individuals’ and couples’ labor supply, savings, and retirement decisions to analyze how the design of Chile’s privatized pension system and a reform undertaken in 2008 affect gender pension gaps and old-age poverty. Chile has one of the longest-running private retirements accounts systems in the world, which has served as a model for many countries. The paper estimates the dynamic model using pre-reform data and compares the model’s short-term forecasts with reduced form estimates of the reform’s causal impacts. The model provides accurate forecasts, so it is used to evaluate how actual and counterfactual changes in the pension system design affect men’s and women’s labor supply and savings decisions, pension receipts, and program costs over a longer time horizon. The results show that three design features significantly reduce gender pension gaps: expanding minimum pension benefit eligibility, providing a per-child pension bonus, and increasing women’s retirement age to be equal to men’s. Overall, the 2008 pension reform largely achieved its goals of reducing gender gaps and old age poverty, although the new system costs double that of the old system.",http://www.sciencedirect.com/science/article/pii/S0304407622001622
Journal of Econometrics,2024,Sample selection models without exclusion restrictions: Parameter heterogeneity and partial identification,Bo E. Honoré and Luojia Hu,"This paper studies semiparametric versions of the classical sample selection model (Heckman, 1976, 1979) without exclusion restrictions. We extend the analysis in Honoré and Hu (2020) by allowing for parameter heterogeneity and derive implications of this model. We also consider models that allow for heteroskedasticity and briefly discuss other extensions. The key ideas are illustrated in a simple wage regression for females. We find that the derived implications of a semiparametric version of Heckman’s classical sample selection model are consistent with the data for women with no college education, but strongly rejected for women with a college degree or more.",http://www.sciencedirect.com/science/article/pii/S0304407622001932
Journal of Econometrics,2024,Eliciting willingness-to-pay to decompose beliefs and preferences that determine selection into competition in lab experiments,"Yvonne Chen, Deniz Dutz, Li Li, Sarah Moon, Edward Vytlacil and Songfa Zhong","This paper develops a partial-identification methodology for analyzing self-selection into alternative compensation schemes in a laboratory environment. We formulate a model of self-selection in which individuals select the compensation scheme with the largest expected valuation, which depends on individual- and scheme-specific beliefs and non-monetary preferences. We characterize the resulting sharp identified sets for individual-specific willingness-to-pay, subjective beliefs, and preferences, and develop conditions on the experimental design under which these identified sets are informative. We apply our methods to examine gender differences in preference for winner-take-all compensation schemes. We find that what has commonly been attributed to a gender difference in preference for performing in a competition is instead explained by men being more confident than women in their probability of winning a future (though not necessarily a past) competition.",http://www.sciencedirect.com/science/article/pii/S0304407623003688
Journal of Econometrics,2024,Robust inference for moment condition models without rational expectations,"Xiaohong Chen, Lars Hansen and Peter G. Hansen","Applied researchers using structural models under rational expectations (RE) often confront empirical evidence of misspecification. In this paper we consider a generic dynamic model that is posed as a vector of unconditional moment restrictions. We suppose that the model is globally misspecified under RE, and thus empirically flawed in a way that is not econometrically subtle. We relax the RE restriction by allowing subjective beliefs to differ from the data-generating probability (DGP) model while still maintaining that the moment conditions are satisfied under the subjective beliefs of economic agents. We use statistical measures of divergence relative to RE to bound the set of subjective probabilities. This form of misspecification alters econometric identification and inferences in a substantial way, leading us to construct robust confidence sets for various set identified functionals.",http://www.sciencedirect.com/science/article/pii/S030440762300369X
Journal of Econometrics,2024,Dealing with imperfect randomization: Inference for the highscope perry preschool program,"James Heckman, Rodrigo Pinto and Azeem Shaikh","This paper considers the problem of making inferences about the effects of a program on multiple outcomes when the assignment of treatment status is imperfectly randomized. By imperfect randomization we mean that treatment status is reassigned after an initial randomization on the basis of characteristics that may be observed or unobserved by the analyst. We develop a partial identification approach to this problem that makes use of information limiting the extent to which randomization is imperfect to show that it is still possible to make nontrivial inferences about the effects of the program in such settings. We consider a family of null hypotheses in which each null hypothesis specifies that the program has no effect on one of many outcomes of interest. Under weak assumptions, we construct a procedure for testing this family of null hypotheses in a way that controls the familywise error rate – the probability of even one false rejection – in finite samples. We develop our methodology in the context of a reanalysis of the HighScope Perry Preschool program. We find statistically significant effects of the program on a number of different outcomes of interest, including outcomes related to criminal activity for males and females, even after accounting for imperfections in the randomization and the multiplicity of null hypotheses.",http://www.sciencedirect.com/science/article/pii/S0304407624000290
Journal of Econometrics,2024,Policy evaluation with multiple instrumental variables,"Magne Mogstad, Alexander Torgovitsky and Christopher Walters","Marginal treatment effect methods are widely used for causal inference and policy evaluation with instrumental variables. However, they fundamentally rely on the well-known monotonicity (threshold-crossing) condition on treatment choice behavior. This condition cannot hold with multiple instruments unless treatment choice is effectively homogeneous. We develop a new marginal treatment effect framework under a weaker, partial monotonicity condition. The partial monotonicity condition is implied by standard choice theory and allows for rich unobserved heterogeneity even in the presence of multiple instruments. The new framework can be viewed as having multiple different choice models for the same observed treatment variable, all of which must be consistent with the data and with each other. Using this framework, we develop a methodology for partial identification of clearly stated, policy-relevant target parameters while allowing for a wide variety of nonparametric shape restrictions and parametric functional form assumptions. We show how the methodology can be used to combine multiple instruments together to yield more informative empirical conclusions than one would obtain by using each instrument separately. The methodology provides a blueprint for extracting and aggregating information from multiple controlled or natural experiments while still allowing for rich unobserved heterogeneity in both treatment effects and choice behavior.",http://www.sciencedirect.com/science/article/pii/S0304407624000642
Journal of Econometrics,2024,Econometric causality: The central role of thought experiments,James Heckman and Rodrigo Pinto,"This paper examines the econometric causal model and the interpretation of empirical evidence based on thought experiments that was developed by Ragnar Frisch and Trygve Haavelmo. We compare the econometric causal model with two currently popular causal frameworks: the Neyman–Rubin causal model and the Do-Calculus. The Neyman–Rubin causal model is based on the language of potential outcomes and was largely developed by statisticians. Instead of being based on thought experiments, it takes statistical experiments as its foundation. The Do-Calculus, developed by Judea Pearl and co-authors, relies on Directed Acyclic Graphs (DAGs) and is a popular causal framework in computer science and applied mathematics. We make the case that economists who uncritically use these frameworks often discard the substantial benefits of the econometric causal model to the detriment of more informative analyses. We illustrate the versatility and capabilities of the econometric framework using causal models developed in economics.",http://www.sciencedirect.com/science/article/pii/S0304407624000654
Journal of Econometrics,2024,Human capital and migration: A cautionary tale,Salvador Navarro and Jin Zhou,"We analyze the impact that the option of migration might have on human capital accumulation. We show that, when the return to education for migrants is lower in the destination than in the origin, the overall incentive to accumulate human capital is reduced as restrictions on migration are relaxed. We use panel data from the Chinese Household Income Project to document that the return to education for rural individuals is lower in urban areas than in rural areas. We then use a difference-in-differences design to show that the 1983 reform that eliminated the strong restriction that existed for rural–urban migration resulted in a reduction of 0.4 years of schooling for rural people in China. Guided by these results, we estimate a life-cycle dynamic discrete choice model of education decisions and circular rural–urban migration in which individuals differ in observable characteristics and unobservable cognitive and non-cognitive skills. We validate the model by showing it is able to replicate the impact estimates from the difference-in-differences specification. Our simulations show that, while it would take a small subsidy (conditional on staying in school) to undo about 40% of the negative effects of the 1983 policy on rural education, the annual subsidy would need to be half of annual earnings to undo the effects entirely.",http://www.sciencedirect.com/science/article/pii/S0304407624000666
Journal of Econometrics,2024,Introduction to the annals issue in honor of James J Heckman,Xiaohong Chen and Edward Vytlacil,,http://www.sciencedirect.com/science/article/pii/S0304407624001635
Journal of Econometrics,2024,Reprint of: Profiling the plight of disconnected youth in America,"Thomas MaCurdy, David Glick, Sonam Sherpa and Sriniketh Nagavarapu","In a successful transition from youth to adulthood, individuals pass through a sequence of roles involving school, work, and family formation that culminate in their becoming self-sufficient adults. However, some “disconnected” youth spend extended periods of time outside of any role that constitutes an element of the pathway towards adult independence. Assisting these youth requires a systematic understanding of what “disconnection” means, how many disconnected youth there are, who these youth are, and how the scale of the problem has evolved over time. Using the National Longitudinal Surveys of Youth for 1997 and 1979, we address these issues by creating concrete definitions of “disconnection spells” using rich data on youths’ enrollment, work, and personal histories. We estimate a multi-state duration model to account for right censoring and to understand differences across salient sub-groups. Our estimates imply that in the early 2000s, almost 19% and 25% of young men and young women, respectively, experienced a disconnection spell by age 23 using our basic definition. These rates are substantially higher for certain sub-groups defined by race/ethnicity, parental education, and government aid receipt, rising as high as 30+% by age 23. Approximately 60% of youth with a disconnection spell have it last longer than a year, and close to 10% have it last longer than 4 years. However, once reconnected, a majority of youth go at least three years without a re-disconnection spell. Patterns of initial disconnection changed markedly from the 1980s to the 2000s, as young women saw a 12 percentage point decline over time. Moreover, the Black-White gap in disconnection has fallen for women, but increased for men. Our profile of disconnection experiences provides a starting point for government agencies aiming to understand where, how, and with whom to intervene to prevent lengthy disconnection spells.",http://www.sciencedirect.com/science/article/pii/S0304407624001659
Journal of Econometrics,2024,A Correlated Random Coefficient panel model with time-varying endogeneity,Louise Laage,"This paper studies a class of linear panel models with random coefficients. We do not restrict the joint distribution of the time-invariant unobserved heterogeneity and the covariates. We investigate identification of the average partial effect (APE) when fixed-effect techniques cannot be used to control for the correlation between the regressors and the time-varying disturbances. Relying on control variables, we develop a constructive two-step identification argument. The first step identifies nonparametrically the conditional expectation of the disturbances given the regressors and the control variables, and the second step uses “between-group” variation, correcting for endogeneity, to identify the APE. We propose a natural semiparametric estimator of the APE, show its n asymptotic normality and compute its asymptotic variance. The estimator is computationally easy to implement, and Monte Carlo simulations show favorable finite sample properties. As an empirical illustration, we estimate the average elasticity of intertemporal substitution in a labor supply model with random coefficients.",http://www.sciencedirect.com/science/article/pii/S0304407624001507
Journal of Econometrics,2024,Nonparametric identification and estimation of stochastic block models from many small networks,Koen Jochmans,"This paper concerns the analysis of network data when unobserved node-specific heterogeneity is present. We postulate a weighted version of the classic stochastic block model, where nodes belong to one of a finite number of latent communities and the placement of edges between them and any weight assigned to these depend on the communities to which the nodes belong. A simple rank condition is presented under which we establish that the number of latent communities, their distribution, and the conditional distribution of edges and weights given community membership are all nonparametrically identified from knowledge of the joint (marginal) distribution of edges and weights in graphs of a fixed size. The identification argument is constructive and we present a computationally-attractive nonparametric estimator based on it. Limit theory is derived under asymptotics where we observe a growing number of independent networks of a fixed size. The results of a series of numerical experiments are reported on.",http://www.sciencedirect.com/science/article/pii/S0304407624001519
Journal of Econometrics,2024,Identification and estimation of dynamic structural models with unobserved choices,Yingyao Hu and Yi Xin,"This paper develops identification and estimation methods for dynamic discrete choice models when agents’ actions are unobserved by econometricians. We provide conditions under which choice probabilities and latent state transition rules are nonparametrically identified with a continuous state variable in a single-agent dynamic discrete choice model. Our identification strategy from the baseline model can extend to models with serially correlated unobserved heterogeneity, cases in which choices are partially unavailable, and dynamic discrete games. We propose a sieve maximum likelihood estimator for primitives in agents’ utility functions and state transition rules. Monte Carlo simulation results support the validity of the proposed approach.",http://www.sciencedirect.com/science/article/pii/S0304407624001520
Journal of Econometrics,2024,On LASSO for high dimensional predictive regression,Ziwei Mei and Zhentao Shi,"This paper examines LASSO, a widely-used L1-penalized regression method, in high dimensional linear predictive regressions, particularly when the number of potential predictors exceeds the sample size and numerous unit root regressors are present. The consistency of LASSO is contingent upon two key components: the deviation bound of the cross product of the regressors and the error term, and the restricted eigenvalue of the Gram matrix. We present new probabilistic bounds for these components, suggesting that LASSO’s rates of convergence are different from those typically observed in cross-sectional cases. When applied to a mixture of stationary, nonstationary, and cointegrated predictors, LASSO maintains its asymptotic guarantee if predictors are scale-standardized. Leveraging machine learning and macroeconomic domain expertise, LASSO demonstrates strong performance in forecasting the unemployment rate, as evidenced by its application to the FRED-MD database.",http://www.sciencedirect.com/science/article/pii/S0304407624001556
Journal of Econometrics,2024,Change-point analysis of time series with evolutionary spectra,Alessandro Casini and Pierre Perron,"This paper develops change-point methods for the spectrum of a locally stationary time series. We focus on series with a bounded spectral density that change smoothly under the null hypothesis but exhibits change-points or becomes less smooth under the alternative. We address two local problems. The first is the detection of discontinuities (or breaks) in the spectrum at unknown dates and frequencies. The second involves abrupt yet continuous changes in the spectrum over a short time period at an unknown frequency without signifying a break. Both problems can be cast into changes in the degree of smoothness of the spectral density over time. We consider estimation and minimax-optimal testing. We determine the optimal rate for the minimax distinguishable boundary, i.e., the minimum break magnitude such that we are able to uniformly control type I and type II errors. We propose a novel procedure for the estimation of the change-points based on a wild sequential top-down algorithm and show its consistency under shrinking shifts and possibly growing number of change-points.",http://www.sciencedirect.com/science/article/pii/S030440762400157X
Journal of Econometrics,2024,Semiparametrically optimal cointegration test,Bo Zhou,"This paper aims to address the issue of semiparametric efficiency for cointegration rank testing in finite-order vector autoregressive models, where the innovation distribution is considered an infinite-dimensional nuisance parameter. Our asymptotic analysis relies on Le Cam’s theory of limit experiment, which in this context is of the Locally Asymptotically Brownian Functional (LABF) type likelihood ratios. By exploiting the structural representation of LABF, an Ornstein–Uhlenbeck experiment, we develop the asymptotic power envelopes of asymptotically invariant tests for both cases with and without time trends. We propose feasible tests based on a nonparametrically estimated density and demonstrate that their power can achieve the semiparametric power envelopes, making them semiparametrically optimal. We validate the theoretical results through large-sample simulations and illustrate satisfactory size control and excellent power performance of our tests under small samples. In both cases with and without time trends, we show that a remarkable amount of additional power can be obtained from non-Gaussian distributions.",http://www.sciencedirect.com/science/article/pii/S0304407624001611
Journal of Econometrics,2024,Modeling long cycles,Da Natasha Kang and Vadim Marmer,"Recurrent boom-and-bust cycles are a salient feature of economic and financial history. Cycles found in the data are stochastic, often highly persistent, and span substantial fractions of the sample size. We refer to such cycles as “long”. In this paper, we develop a novel approach to modeling cyclical behavior specifically designed to capture long cycles. We show that existing inferential procedures may produce misleading results in the presence of long cycles and propose a new econometric procedure for the inference on the cycle length. Our procedure is asymptotically valid regardless of the cycle length. We apply our methodology to a set of macroeconomic and financial variables for the U.S. We find evidence of long stochastic cycles in the standard business cycle variables, as well as in credit and house prices. However, we rule out the presence of stochastic cycles in asset market data. Moreover, according to our result, financial cycles, as characterized by credit and house prices, tend to be twice as long as business cycles.",http://www.sciencedirect.com/science/article/pii/S0304407624000976
Journal of Econometrics,2024,Better the devil you know: Improved forecasts from imperfect models,Dong Hwan Oh and Andrew J. Patton,"Many important economic decisions are based on a parametric forecasting model that is known to be good but imperfect. We propose methods to improve out-of-sample forecasts from a misspecified model by estimating its parameters using a form of local M estimation (thereby nesting local OLS and local MLE), drawing on information from a state variable that is correlated with the misspecification of the model. We theoretically consider the forecast environments in which our approach is likely to offer improvements over standard methods, and we find significant forecast improvements from applying the proposed method across four distinct empirical analyses including volatility forecasting, risk management, and yield curve forecasting.",http://www.sciencedirect.com/science/article/pii/S0304407624001131
Journal of Econometrics,2024,Nonlinear and nonseparable structural functions in regression discontinuity designs with a continuous treatment,Haitian Xie,"Many empirical examples of regression discontinuity (RD) designs concern a continuous treatment variable, but the theoretical aspects of such models are less studied. This study examines the identification and estimation of the structural function in fuzzy RD designs with a continuous treatment variable. The structural function fully describes the causal impact of the treatment on the outcome. We show that the nonlinear and nonseparable structural function can be nonparametrically identified at the RD cutoff under shape restrictions, including monotonicity and smoothness conditions. Based on the nonparametric identification equation, we propose a three-step semiparametric estimation procedure and establish the asymptotic normality of the estimator. The semiparametric estimator achieves the same convergence rate as in the case of a binary treatment variable. As an application of the method, we estimate the causal effect of sleep time on health status by using the discontinuity in natural light timing at time zone boundaries.",http://www.sciencedirect.com/science/article/pii/S0304407624001301
Journal of Econometrics,2024,2SLS with multiple treatments,Manudeep Bhuller and Henrik Sigstad,"We study what two-stage least squares (2SLS) identifies in models with multiple treatments under treatment effect heterogeneity. Two conditions are shown to be necessary and sufficient for the 2SLS to identify positively weighted sums of agent-specific effects of each treatment: average conditional monotonicity and no cross effects. Our identification analysis allows for any number of treatments, any number of continuous or discrete instruments, and the inclusion of covariates. We provide testable implications and present characterizations of choice behavior implied by our identification conditions.",http://www.sciencedirect.com/science/article/pii/S0304407624001313
Journal of Econometrics,2024,A simple specification test for models with many conditional moment inequalities,"Mathieu Marcoux, Thomas M. Russell and Yuanyuan Wan","This paper proposes a simple specification test for partially identified models with a large or possibly uncountably infinite number of conditional moment (in)equalities. The approach is valid under weak assumptions, allowing for both weak identification and non-differentiable moment conditions. Computational simplifications are obtained by reusing certain expensive-to-compute components of the test statistic when constructing the critical values. Because of the weak assumptions, the procedure faces a new set of interesting theoretical issues which we show can be addressed by an unconventional sample-splitting procedure that runs multiple tests of the same null hypothesis. The resulting specification test controls size uniformly over a large class of data generating processes, has power tending to 1 for fixed alternatives, and has power against certain local alternatives which we characterize. Finally, the testing procedure is demonstrated in three simulation exercises.",http://www.sciencedirect.com/science/article/pii/S0304407624001349
Journal of Econometrics,2024,Maximum likelihood estimation of a spatial autoregressive model for origin–destination flow variables,Hanbat Jeong and Lung-fei Lee,"We introduce a spatial autoregressive hurdle model for nonnegative origin–destination flows yN,ij. The model incorporates a hurdle formulation to elucidate the different data-generating processes for zero and positive flows. Our model specifies three types of spatial influences on flow yN,ij that quantify the impact of third-party characteristics on the flow yN,ij: (i) the effect of outflows from origin j, (ii) the effect of inflows to destination i, and (iii) the effect of flows among third-party units. We account for two-way fixed effects in the model to capture the inherent characteristics of both origins and destinations. We employ maximum likelihood estimation to estimate the model parameters. To address statistical inference issues, we analyze the asymptotic properties of the ML estimator using the spatial near-epoch dependence concept. We confirm the presence of an asymptotic bias that arises from the fixed effects, whose dimensions grow with the sample size. Applying our model to migration flows among U.S. states, we estimate significant spatial influences, particularly from inflows to destinations and outflows from origins. Our findings support the notion that zero and positive flow formations are distinct. Consequently, our proposed model outperforms the spatial autoregressive Tobit specification for origin–destination flows, thus providing a better fit to the data.",http://www.sciencedirect.com/science/article/pii/S0304407624001362
Journal of Econometrics,2024,On the performance of the Neyman Allocation with small pilots,Yong Cai and Ahnaf Rafi,"The Neyman Allocation is used in many papers on experimental design, which typically assume that researchers have access to large pilot studies. This may be unrealistic. To understand the properties of the Neyman Allocation with small pilots, we study its behavior in an asymptotic framework that takes pilot size to be fixed even as the size of the main wave tends to infinity. Our analysis shows that the Neyman Allocation can lead to estimates of the ATE with higher asymptotic variance than with (non-adaptive) balanced randomization. In particular, this happens when the outcome variable is relatively homoskedastic with respect to treatment status or when it exhibits high kurtosis. We provide a series of empirical examples showing that such situations can arise in practice. Our results suggest that researchers with small pilots should not use the Neyman Allocation if they believe that outcomes are homoskedastic or heavy-tailed. Finally, we examine some potential methods for improving the finite sample performance of the FNA via simulations.",http://www.sciencedirect.com/science/article/pii/S0304407624001398
Journal of Econometrics,2024,Prewhitened long-run variance estimation robust to nonstationarity,Alessandro Casini and Pierre Perron,"We introduce a nonparametric nonlinear VAR prewhitened long-run variance (LRV) estimator for the construction of standard errors robust to autocorrelation and heteroskedasticity that can be used for hypothesis testing in a variety of contexts including the linear regression model. Existing methods either are theoretically valid only under stationarity and have poor finite-sample properties under nonstationarity (i.e., fixed-b methods), or are theoretically valid under the null hypothesis but lead to tests that are not consistent under nonstationary alternative hypothesis (i.e., both fixed-b and traditional HAC estimators). The proposed estimator accounts explicitly for nonstationarity, unlike previous prewhitened procedures which are known to be unreliable, and leads to tests with accurate null rejection rates and good monotonic power. We also establish MSE bounds for LRV estimation that are sharper than previously established and use them to determine the data-dependent bandwidths.",http://www.sciencedirect.com/science/article/pii/S0304407624001404
Journal of Econometrics,2024,Correcting attrition bias using changes-in-changes,"Dalia Ghanem, Sarojini Hirshleifer, Desire Kedagni and Karen Ortiz-Becerra","Attrition is a common and potentially important threat to internal validity in treatment effect studies. We extend the changes-in-changes approach to identify the average treatment effect for respondents and the entire study population in the presence of attrition. Our method, which exploits baseline outcome data, can be applied to randomized experiments as well as quasi-experimental difference-in-difference designs. A formal comparison highlights that while widely used corrections typically impose restrictions on whether or how response depends on treatment, our proposed attrition correction exploits restrictions on the outcome model. We further show that the conditions required for our correction can accommodate a broad class of response models that depend on treatment in an arbitrary way. We illustrate the implementation of the proposed corrections in an application to a large-scale randomized experiment.",http://www.sciencedirect.com/science/article/pii/S0304407624000836
Journal of Econometrics,2024,Hybrid unadjusted Langevin methods for high-dimensional latent variable models,"Rubén Loaiza-Maya, Didier Nibbering and Dan Zhu","The exact estimation of latent variable models with big data is known to be challenging. The latents have to be integrated out numerically, and the dimension of the latent variables increases with the sample size. This paper develops a novel approximate Bayesian method based on the Langevin diffusion process. The method employs the Fisher identity to integrate out the latent variables, which makes it accurate and computationally feasible when applied to big data. In contrast to other approximate estimation methods, it does not require the choice of a parametric distribution for the unknowns, which often leads to inaccuracies. In an empirical discrete choice example with a million observations, the proposed method accurately estimates the posterior choice probabilities using only 2% of the computation time of exact MCMC.",http://www.sciencedirect.com/science/article/pii/S0304407624000873
Journal of Econometrics,2024,Dynamic partial correlation models,"D’Innocenzo, Enzo and Andre Lucas","We introduce a new scalable model for dynamic conditional correlation matrices based on a recursion of dynamic bivariate partial correlation models. By exploiting the model’s recursive structure and the theory of perturbed stochastic recurrence equations, we establish stationarity, ergodicity, and filter invertibility in the multivariate setting using conditions for bivariate slices of the data only. From this, we establish consistency and asymptotic normality of the maximum likelihood estimator for the model’s static parameters. The new model outperforms benchmarks like the t-cDCC and the multivariate t-GAS, both in simulations and in an in-sample and out-of-sample asset pricing application to US stock returns.",http://www.sciencedirect.com/science/article/pii/S0304407624000939
Journal of Econometrics,2024,Parametric risk-neutral density estimation via finite lognormal-Weibull mixtures,"Yifan Li, Ingmar Nolte and Manh Cuong Pham","This paper proposes a new parametric risk-neutral density (RND) estimator based on a finite lognormal-Weibull mixture (LWM) density. We establish the consistency and asymptotic normality of the LWM method in a general misspecified parametric framework. Based on the theoretical results, we propose a sequential test procedure to evaluate the goodness-of-fit of the LWM model, which leads to an adaptive choice for the number and type of mixture components. Our simulation results show that, in finite samples with various observation error specifications, the LWM method can approximate complex RNDs generated by state-of-the-art multi-factor stochastic volatility models with a few (typically less than 4) mixtures. Application of the LWM model on index options confirms its reliability in recovering empirical RNDs with a heavy left tail or bimodality, which can be incorrectly identified as bimodality or a heavy left tail by existing (semi)-nonparametric methods if the goodness-of-fit to the observed data is ignored.",http://www.sciencedirect.com/science/article/pii/S0304407624000940
Journal of Econometrics,2024,Estimation and inference for high dimensional factor model with regime switching,Giovanni Urga and Fa Wang,"This paper proposes maximum (quasi)likelihood estimation for high dimensional factor models with regime switching in the loadings. The model parameters are estimated jointly by the EM (expectation maximization) algorithm, which in the current context only requires iteratively calculating regime probabilities and principal components of the weighted sample covariance matrix. When regime dynamics are taken into account, smoothed regime probabilities are calculated using a recursive algorithm. Consistency, convergence rates and limit distributions of the estimated loadings and the estimated factors are established under weak cross-sectional and temporal dependence as well as heteroscedasticity. It is worth noting that due to high dimension, regime switching can be identified consistently after the switching point with only one observation period. Simulation results show good performance of the proposed method. An application to the FRED-MD dataset illustrates the potential of the proposed method for detection of business cycle turning points.",http://www.sciencedirect.com/science/article/pii/S0304407624000988
Journal of Econometrics,2024,Robust inference of panel data models with interactive fixed effects under long memory: A frequency domain approach,"Shuyao Ke, Peter Phillips and Liangjun Su","This paper studies a linear panel data model with interactive fixed effects wherein regressors, factors and idiosyncratic error terms are all stationary but with potential long memory. The setup involves a new formulation of panel data models, where weakly dependent regressors, factors and idiosyncratic errors are embedded as a special case. Standard methods based on principal component decomposition and least squares estimation, as in Bai (2009), are found to be biased and distorted in inference. To cope with this failure and to provide a simple implementable estimation procedure, a frequency domain least squares estimation is proposed. The limit distribution of the frequency domain estimator is established and a self-normalized approach to inference without the need for plug-in estimation of memory parameters is developed. Simulations show that the frequency domain estimator performs robustly under short memory and outperforms the time domain estimator when long range dependence is present. An empirical illustration is provided, examining the long-run relationship between stock returns and realized volatility.",http://www.sciencedirect.com/science/article/pii/S0304407624001076
Journal of Econometrics,2024,The local to unity dynamic Tobit model,Anna Bykhovskaya and James A. Duffy,"This paper considers highly persistent time series that are subject to nonlinearities in the form of censoring or an occasionally binding constraint, such as are regularly encountered in macroeconomics. A tractable candidate model for such series is the dynamic Tobit with a root local to unity. We show that this model generates a process that converges weakly to a non-standard limiting process, that is constrained (regulated) to be positive. Surprisingly, despite the presence of censoring, the OLS estimators of the model parameters are consistent. We show that this allows OLS-based inferences to be drawn on the overall persistence of the process (as measured by the sum of the autoregressive coefficients), and for the null of a unit root to be tested in the presence of censoring. Our simulations illustrate that the conventional ADF test substantially over-rejects when the data is generated by a dynamic Tobit with a unit root, whereas our proposed test is correctly sized. We provide an application of our methods to testing for a unit root in the Swiss franc/euro exchange rate, during a period when this was subject to an occasionally binding lower bound.",http://www.sciencedirect.com/science/article/pii/S0304407624001106
Journal of Econometrics,2024,Identifiability and estimation of possibly non-invertible SVARMA Models: The normalised canonical WHF parametrisation,Bernd Funovits,"This article focuses on the parametrisation, identifiability, and (quasi-) maximum likelihood (QML) estimation of possibly non-invertible structural vector autoregressive moving average (SVARMA) models. SVAR models are routinely adopted due to their well-known implementation strategy. However, for various economic and statistical reasons, multivariate SVARMA settings are often more suitable. These settings introduce complexity in the analysis, primarily due to the presence of the moving average (MA) polynomial. We propose a novel representation of the MA polynomial matrix using the Wiener–Hopf factorization (WHF). A significant advantage of the WHF is its ability to handle possible non-invertibility and thus models with informational asymmetry between economic agents and outside observers. Since solutions of Dynamic Stochastic General Equilibrium (DSGE) models often involve this informational asymmetry, SVARMA models in WHF parametrisation can be considered data-driven alternatives to DSGE models and used for their evaluation. Furthermore, we provide low-level conditions for the asymptotic normality of the (Q)ML estimator and analytic expressions for the score and information matrix. As application, we estimate the Blanchard and Quah model, and compare our results and implied impulse response function with the ones in the SVAR model by Blanchard and Quah and a non-invertible SVARMA model by Gouriéroux and co-authors. Importantly, we have implemented this novel method in a well-documented R-package, making it readily accessible for researchers and practitioners.",http://www.sciencedirect.com/science/article/pii/S030440762400112X
Journal of Econometrics,2024,Measuring tail risk,"Maik Dierkes, Fabian Hollstein, Marcel Prokopczuk and Christoph Matthias Würsig","We comprehensively investigate the usefulness of tail risk measures proposed in the literature. We evaluate their statistical as well as their economic validity. The option-implied measure of Bollerslev and Todorov (2011b) (BT11Q) performs best overall. While some other tail risk measures excel at specialized tasks, BT11Q performs well in all tests: First, BT11Q can predict both future tail events and future tail volatility. Second, it has predictive power for returns in both the time series and the cross-section, as well as for real economic activity. Finally, a simulation analysis shows that the main driver of performance is measurement error.",http://www.sciencedirect.com/science/article/pii/S0304407624001155
Journal of Econometrics,2024,Extreme expectile estimation for short-tailed data,"Abdelaati Daouia, Simone A. Padoan and Gilles Stupfler","The use of expectiles in risk management has recently gathered remarkable momentum due to their excellent axiomatic and probabilistic properties. In particular, the class of elicitable law-invariant coherent risk measures only consists of expectiles. While the theory of expectile estimation at central levels is substantial, tail estimation at extreme levels has so far only been considered when the tail of the underlying distribution is heavy. This article is the first work to handle the short-tailed setting where the loss (e.g. negative log-returns) distribution of interest is bounded to the right and the corresponding extreme value index is negative. This is motivated by the assessment of long-term market risk carried by low-frequency (e.g. weekly) returns of equities that show evidence of being generated from short-tailed distributions. We derive an asymptotic expansion of tail expectiles in this challenging context under a general second-order extreme value condition, which allows to come up with two semiparametric estimators of extreme expectiles, and with their asymptotic properties in a general model of strictly stationary but weakly dependent observations. We also extend the applicability of the proposed method to the regression setting. A simulation study and a real data analysis from a forecasting perspective are performed to compare the proposed competing estimation procedures.",http://www.sciencedirect.com/science/article/pii/S0304407624001167
Journal of Econometrics,2024,Spectral clustering with variance information for group structure estimation in panel data,"Lu Yu, Jiaying Gu and Stanislav Volgushev","Consider a panel data setting where repeated observations on individuals are available. Often it is reasonable to assume that there exist groups of individuals that share similar effects of observed characteristics, but the grouping is typically unknown in advance. We first conduct a local analysis which reveals that the variances of the individual coefficient estimates contain useful information for the estimation of group structure. We then propose a method to estimate unobserved groupings for general panel data models that explicitly accounts for the variance information. Our proposed method remains computationally feasible with a large number of individuals and/or repeated measurements on each individual. The developed ideas can also be applied even when individual-level data are not available and only parameter estimates together with some quantification of estimation uncertainty are given to the researcher. A thorough simulation study demonstrates superior performance of our method than existing methods and we apply the method to two empirical applications.",http://www.sciencedirect.com/science/article/pii/S0304407624000551
Journal of Econometrics,2024,Predictive ability tests with possibly overlapping models,"Valentina Corradi, Jack Fosten and Daniel Gutknecht","This paper provides novel tests for comparing out-of-sample predictive ability of two or more competing models that are possibly overlapping. The tests do not require pre-testing, they allow for dynamic misspecification and are valid under different estimation schemes and loss functions. In pairwise model comparisons, the test is constructed by adding a random perturbation to both the numerator and denominator of a standard Diebold–Mariano test statistic. This prevents degeneracy in the presence of overlapping models but becomes asymptotically negligible otherwise. The test is shown to control the Type I error probability asymptotically at the nominal level, uniformly over all null data generating processes. A similar idea is used to develop a superior predictive ability test for the comparison of multiple models against a benchmark. Monte Carlo simulations demonstrate that our tests exhibit very good size control in finite samples reducing both over- and under-rejection relative to its competitors. Finally, an application to forecasting U.S. excess bond returns provides evidence in favour of models using macroeconomic factors.",http://www.sciencedirect.com/science/article/pii/S0304407624000629
Journal of Econometrics,2024,No star is good news: A unified look at rerandomization based on p-values from covariate balance tests,Anqi Zhao and Peng Ding,"Randomized experiments balance all covariates on average and are considered the gold standard for estimating treatment effects. Chance imbalances are nonetheless common in realized treatment allocations. To inform readers of the comparability of treatment groups at baseline, contemporary scientific publications often report covariate balance tables with not only covariate means by treatment group but also the associated p-values from significance tests of their differences. The practical need to avoid small p-values as indicators of poor balance motivates balance check and rerandomization based on these p-values from covariate balance tests (ReP) as an attractive tool for improving covariate balance in designing randomized experiments. Despite the intuitiveness of such strategy and its possibly already widespread use in practice, the literature lacks results about its implications on subsequent inference, subjecting many effectively rerandomized experiments to possibly inefficient analyses. To fill this gap, we examine a variety of potentially useful schemes for ReP and quantify their impact on subsequent inference. Specifically, we focus on three estimators of the average treatment effect from the unadjusted, additive, and interacted linear regressions of the outcome on treatment, respectively, and derive their asymptotic sampling properties under ReP. The main findings are threefold. First, the estimator from the interacted regression is asymptotically the most efficient under all ReP schemes examined, and permits convenient regression-assisted inference identical to that under complete randomization. Second, ReP, in contrast to complete randomization, improves the asymptotic efficiency of the estimators from the unadjusted and additive regressions. Standard regression analyses are accordingly still valid but in general overconservative. Third, ReP reduces the asymptotic conditional biases of the three estimators and improves their coherence in terms of mean squared difference. These results establish ReP as a convenient tool for improving covariate balance in designing randomized experiments, and we recommend using the interacted regression for analyzing data from ReP designs.",http://www.sciencedirect.com/science/article/pii/S0304407624000708
Journal of Econometrics,2024,Bayesian estimation of cluster covariance matrices of unknown form,Drew Creal and Jaeho Kim,"We develop a flexible Bayesian model for cluster covariance matrices in large dimensions where the number of clusters and the assignment of cross-sectional units to a cluster are a-priori unknown and estimated from the data. In a cluster covariance matrix, the variances and covariances are equal within each diagonal block, while the covariances are equal in each off-diagonal block. This reduces the number of parameters by pooling those parameters together that are in the same cluster. In order to treat the number of clusters and the cluster assignments as unknowns, we build a random partition model which assigns a prior distribution over the space of partitions of the data into clusters. Sampling from the posterior over the space of partitions creates a flexible estimator because it averages across a wide set of cluster covariance matrices. We illustrate our methods on linear factor models and large vector autoregressions.",http://www.sciencedirect.com/science/article/pii/S030440762400071X
Journal of Econometrics,2024,Wild bootstrap inference for instrumental variables regressions with weak and few clusters,Wenjie Wang and Yichong Zhang,"We study the wild bootstrap inference for instrumental variable regressions under an alternative asymptotic framework that the number of independent clusters is fixed, the size of each cluster diverges to infinity, and the within cluster dependence is sufficiently weak. We first show that the wild bootstrap Wald test controls size asymptotically up to a small error as long as the parameters of endogenous variables are strongly identified in at least one of the clusters. Second, we establish the conditions for the bootstrap tests to have power against local alternatives. We further develop a wild bootstrap Anderson–Rubin test for the full-vector inference and show that it controls size asymptotically even under weak identification in all clusters. We illustrate their good performance using simulations and provide an empirical application to a well-known dataset about US local labor markets.",http://www.sciencedirect.com/science/article/pii/S0304407624000733
Journal of Econometrics,2024,Estimation and inference of seller’s expected revenue in first-price auctions,Federico Zincenko,"I propose an estimator for the seller’s expected revenue function in a first-price sealed-bid auction with independent private values and symmetric bidders, who can exhibit constant relative risk aversion and bid according to the Bayesian Nash equilibrium. I build the proposed estimator from pseudo-private values, which can be estimated from observed bids, and show that it is pointwise and uniformly consistent: the corresponding optimal nonparametric rates of convergence can be achieved. Then I construct asymptotically valid confidence intervals and uniform confidence bands. Suggestions for critical values are based on first-order asymptotics, as well as on the bootstrap method.",http://www.sciencedirect.com/science/article/pii/S0304407624000800
Journal of Econometrics,2024,A vector monotonicity assumption for multiple instruments,Leonard Goff,"When a researcher combines multiple instrumental variables for a single binary treatment, the monotonicity assumption of the local average treatment effects (LATE) framework can become restrictive: it requires that all units share a common direction of response even when separate instruments are shifted in opposing directions. What I call vector monotonicity, by contrast, simply assumes treatment uptake to be monotonic in all instruments. I characterize the class of causal parameters that are point identified under vector monotonicity, when the instruments are binary. This class includes, for example, the average treatment effect among units that are in any way responsive to the collection of instruments, or those that are responsive to a given subset of them. The identification results are constructive and yield a simple estimator for the identified treatment effect parameters. An empirical application revisits the labor market returns to college.",http://www.sciencedirect.com/science/article/pii/S0304407624000812
Journal of Econometrics,2024,Testing identification conditions of LATE in fuzzy regression discontinuity designs,"Yu-Chin Hsu, Ji-Liang Shiu and Yuanyuan Wan",This paper derives testable implications of the identifying conditions for the local average treatment effect in fuzzy regression discontinuity designs. We show that the testable implications of these identifying conditions are a finite number of inequality restrictions on the observed data distribution. We then propose a specification test for the testable implications and show that the proposed test controls the size and is asymptotically consistent. We apply our test to several fuzzy regression discontinuity designs in the literature.,http://www.sciencedirect.com/science/article/pii/S0304407624000848
Journal of Econometrics,2024,Covariate adjustment in experiments with matched pairs,"Yuehao Bai, Liang Jiang, Joseph P. Romano, Azeem Shaikh and Yichong Zhang","This paper studies inference for the average treatment effect (ATE) in experiments in which treatment status is determined according to “matched pairs” and it is additionally desired to adjust for observed, baseline covariates to gain further precision. By a “matched pairs” design, we mean that units are sampled i.i.d. from the population of interest, paired according to observed, baseline covariates, and finally, within each pair, one unit is selected at random for treatment. Importantly, we presume that not all observed, baseline covariates are used in determining treatment assignment. We study a broad class of estimators based on a “doubly robust” moment condition that permits us to study estimators with both finite-dimensional and high-dimensional forms of covariate adjustment. We find that estimators with finite-dimensional, linear adjustments need not lead to improvements in precision relative to the unadjusted difference-in-means estimator. This phenomenon persists even if the adjustments interact with treatment; in fact, doing so leads to no changes in precision. However, gains in precision can be ensured by including fixed effects for each of the pairs. Indeed, we show that this adjustment leads to the minimum asymptotic variance of the corresponding ATE estimator among all finite-dimensional, linear adjustments. We additionally study an estimator with a regularized adjustment, which can accommodate high-dimensional covariates. We show that this estimator leads to improvements in precision relative to the unadjusted difference-in-means estimator and also provides conditions under which it leads to the “optimal” nonparametric, covariate adjustment. A simulation study confirms the practical relevance of our theoretical analysis, and the methods are employed to reanalyze data from an experiment using a “matched pairs” design to study the effect of macroinsurance on microenterprise.",http://www.sciencedirect.com/science/article/pii/S0304407624000861
Journal of Econometrics,2024,The law of large numbers for large stable matchings,Jacob Schwartz and Kyungchul Song,"In many empirical studies of a large two-sided matching market (such as in a college admissions problem), the researcher performs statistical inference under the assumption that they observe a random sample from a large matching market. In this paper, we consider a setting in which the researcher observes either all or a nontrivial fraction of outcomes from a stable matching. We establish a concentration inequality for empirical matching probabilities assuming strong correlation among the colleges’ preferences while allowing students’ preferences to be fully heterogeneous. Our concentration inequality yields laws of large numbers for the empirical matching probabilities and other statistics commonly used in empirical analyses of a large matching market. To illustrate the usefulness of our concentration inequality, we prove consistency for estimators of conditional matching probabilities and measures of positive assortative matching.",http://www.sciencedirect.com/science/article/pii/S0304407624000885
Journal of Econometrics,2024,Standard errors for panel data models with unknown clusters,"Jushan Bai, Sung Hoon Choi and Yuan Liao","This paper develops a new standard-error estimator for linear panel data models. The proposed estimator is robust to heteroskedasticity, serial correlation, and cross-sectional correlation of unknown forms. The serial correlation is controlled by the Newey–West method. To control for cross-sectional correlations, we propose to use the thresholding method, without assuming the clusters to be known. We establish the consistency of the proposed estimator. Monte Carlo simulations show the method works well. An empirical application is considered.",http://www.sciencedirect.com/science/article/pii/S0304407620303341
Journal of Econometrics,2024,Maximum likelihood estimation of latent Markov models using closed-form approximations,"Yacine Ait-Sahalia, Chenxu Li and Chen Xu Li","This paper proposes and implements an efficient and flexible method to compute maximum likelihood estimators of continuous-time models when part of the state vector is latent. Stochastic volatility and term structure models are typical examples. Existing methods integrate out the latent variables using either simulations as in MCMC, or replace the latent variables by observable proxies. By contrast, our approach relies on closed-form approximations to estimate parameters and simultaneously infer the distribution of filters, i.e., that of the latent states conditioning on observations. Without any particular assumption on the filtered distribution, we approximate in closed form a coupled iteration system for updating the likelihood function and filters based on the transition density of the state vector. Our procedure has a linear computational cost with respect to the number of observations, as opposed to the exponential cost implied by the high dimensional integral nature of the likelihood function. We establish the theoretical convergence of our method as the frequency of observation increases and conduct Monte Carlo simulations to demonstrate its performance.",http://www.sciencedirect.com/science/article/pii/S0304407620303389
Journal of Econometrics,2024,Network and panel quantile effects via distribution regression,"Victor Chernozhukov, Ivan Fernandez-Val and Martin Weidner","This paper provides a method to construct simultaneous confidence bands for quantile functions and quantile effects in nonlinear network and panel models with unobserved two-way effects, strictly exogenous covariates, and possibly discrete outcome variables. The method is based upon projection of simultaneous confidence bands for distribution functions constructed from fixed effects distribution regression estimators. These fixed effects estimators are debiased to deal with the incidental parameter problem. Under asymptotic sequences where both dimensions of the data set grow at the same rate, the confidence bands for the quantile functions and effects have correct joint coverage in large samples. An empirical application to gravity models of trade illustrates the applicability of the methods to network data.",http://www.sciencedirect.com/science/article/pii/S0304407620303390
Journal of Econometrics,2024,Financially adaptive clinical trials via option pricing analysis,Shomesh E. Chaudhuri and Andrew W. Lo,"The regulatory approval process for new therapies involves costly clinical trials that can span multiple years. When valuing a candidate therapy from a financial perspective, industry sponsors may terminate a program early if clinical evidence suggests market prospects are not as favorable as originally forecasted. Intuition suggests that clinical trials that can be modified as new data are observed, i.e., adaptive trials, are more valuable than trials without this flexibility. To quantify this value, we propose modeling the accrual of information in a clinical trial as a sequence of real options, allowing us to systematically design early-stopping decision boundaries that maximize the economic value to the sponsor. In an empirical analysis of selected disease areas, we find that when a therapy is ineffective, our adaptive financing method can decrease the expected cost incurred by the sponsor in terms of total expenditures, number of patients, and trial length by up to 46%. Moreover, by amortizing the large fixed costs associated with a clinical trial over time, financing these projects becomes less risky, resulting in lower costs of capital and larger valuations when the therapy is effective.",http://www.sciencedirect.com/science/article/pii/S030440762030364X
Journal of Econometrics,2024,A comparison of the GB2 and skewed generalized log-t distributions with an application in finance,Joshua D. Higbee and James McDonald,"Several families of statistical distributions have been used to model financial data. The four-parameter generalized beta of the second kind (GB2) and five-parameter skewed generalized t (SGT) have been fit to return and log-return data, respectively. We introduce the skewed generalized log-t (SGLT) distribution and note that the GB2 and SGLT share such distributions as the asymmetric log-Laplace (ALL), log-Laplace (LL), and log-normal (LN). We then compare the relative performance of the GB2 and SGLT in modeling the distribution of daily, weekly, and monthly stock return data. We find that the GB2 and SGLT perform similarly and that the three-parameter log-t (LT) distribution is quite robust.",http://www.sciencedirect.com/science/article/pii/S0304407621000154
Journal of Econometrics,2024,Local regression distribution estimators,"Matias Cattaneo, Michael Jansson and Xinwei Ma","This paper investigates the large sample properties of local regression distribution estimators, which include a class of boundary adaptive density estimators as a prime example. First, we establish a pointwise Gaussian large sample distributional approximation in a unified way, allowing for both boundary and interior evaluation points simultaneously. Using this result, we study the asymptotic efficiency of the estimators, and show that a carefully crafted minimum distance implementation based on “redundant” regressors can lead to efficiency gains. Second, we establish uniform linearizations and strong approximations for the estimators, and employ these results to construct valid confidence bands. Third, we develop extensions to weighted distributions with estimated weights and to local L2 estimation. Finally, we illustrate our methods with two applications in program evaluation: counterfactual density testing, and IV specification and heterogeneity density analysis. Companion software packages in Stata and R are available.",http://www.sciencedirect.com/science/article/pii/S0304407621000427
Journal of Econometrics,2024,Testing and relaxing the exclusion restriction in the control function approach,"D’Haultfœuille, Xavier, Stefan Hoderlein and Yuya Sasaki","The control function approach which employs an instrumental variable excluded from the outcome equation is a very common solution to deal with the problem of endogeneity in nonseparable models. Exclusion restrictions, however, are frequently controversial. We first argue that, in a nonparametric triangular structure typical of the control function literature, one can actually test this exclusion restriction provided the instrument satisfies a local irrelevance condition. Second, we investigate identification without such exclusion restrictions, i.e., if the “instrument” that is independent of the unobservables in the outcome equation also directly affects the outcome variable. In particular, we show that identification of average causal effects can be achieved in the two most common special cases of the general nonseparable model: linear random coefficients models and single index models.",http://www.sciencedirect.com/science/article/pii/S0304407621000439
Journal of Econometrics,2024,Using Wasserstein Generative Adversarial Networks for the design of Monte Carlo simulations,"Susan Athey, Guido W. Imbens, Jonas Metzger and Evan Munro","When researchers develop new econometric methods it is common practice to compare the performance of the new methods to those of existing methods in Monte Carlo studies. The credibility of such Monte Carlo studies is often limited because of the discretion the researcher has in choosing the Monte Carlo designs reported. To improve the credibility we propose using a class of generative models that has recently been developed in the machine learning literature, termed Generative Adversarial Networks (GANs) which can be used to systematically generate artificial data that closely mimics existing datasets. Thus, in combination with existing real data sets, GANs can be used to limit the degrees of freedom in Monte Carlo study designs for the researcher, making any comparisons more convincing. In addition, if an applied researcher is concerned with the performance of a particular statistical method on a specific data set (beyond its theoretical properties in large samples), she can use such GANs to assess the performance of the proposed method, e.g. the coverage rate of confidence intervals or the bias of the estimator, using simulated data which closely resembles the exact setting of interest. To illustrate these methods we apply Wasserstein GANs (WGANs) to the estimation of average treatment effects. In this example, we find that (i) there is not a single estimator that outperforms the others in all three settings, so researchers should tailor their analytic approach to a given setting, (ii) systematic simulation studies can be helpful for selecting among competing methods in this situation, and (iii) the generated data closely resemble the actual data.",http://www.sciencedirect.com/science/article/pii/S0304407621000440
Journal of Econometrics,2024,Nonseparable sample selection models with censored selection rules,"Ivan Fernandez-Val, Aico van Vuuren and Francis Vella","We consider identification and estimation of nonseparable sample selection models with censored selection rules. We employ a control function approach and discuss different objects of interest based on (1) local effects conditional on the control function, and (2) global effects obtained from integration over ranges of values of the control function. We derive conditions for identification of these different objects and suggest strategies for estimation. Moreover, we provide the associated asymptotic theory. These strategies are illustrated in an empirical investigation of the determinants of female wages in the United Kingdom.",http://www.sciencedirect.com/science/article/pii/S0304407621000567
Journal of Econometrics,2024,"Testing underidentification in linear models, with applications to dynamic panel and asset pricing models",Frank Windmeijer,"This paper develops the links between overidentification tests, underidentification tests, score tests and the Cragg and Donald (1993, 1997) and Kleibergen and Paap (2006) rank tests in linear instrumental variable (IV) models. For the structural linear model y=Xβ+u, with the endogenous explanatory variables partitioned as X=x1X2, this general framework shows that standard underidentification tests are tests for overidentification in an auxiliary linear model, x1=X2δ+ɛ, estimated by IV estimation methods using the same instruments as for the original model. This simple structure makes it possible to establish valid robust underidentification tests for linear IV models where these have not been proposed or used before, like clustered dynamic panel data models estimated by GMM. The framework also applies to tests for the rank of general parameter matrices. Invariant rank tests are based on the LIML or continuously updated GMM estimators of both structural and first-stage parameters. This insight leads to the proposal of new two-step invariant asymptotically efficient GMM estimators, and a new iterated GMM estimator that, if it converges, converges to the continuously updated GMM estimator.",http://www.sciencedirect.com/science/article/pii/S030440762100097X
Journal of Econometrics,2024,On uniform inference in nonlinear models with endogeneity,Shakeeb Khan and Denis Nekipelov,"This paper explores the uniformity of inference for parameters of interest in nonlinear econometric models with endogeneity. Here the notion of uniformity arises because the behavior of estimators of parameters of interest is shown to vary with where either they or nuisance parameters lie in the parameter space. As a result, inference becomes nonstandard in a fashion that is loosely analogous to inference complications found in the unit root and weak instruments literature, as well as the models recently studied in Andrews and Cheng (2012), Chen et al. (2014), Han and McCloskey (2019). Our main illustrative example is the standard sample selection model, where the parameter of interest is the intercept term as in Heckman (1990), Andrews and Schafgans (1998) and Lewbel (2007). We show here there is a discontinuity in the limiting distribution for an estimator of this parameter despite it being uniformly consistent. This discontinuity prevents standard inference procedures from being valid, and motivates the development of new methods, for which we establish asymptotic properties. Finite sample properties of the procedure are explored through a simulation study and an empirical illustration using the Mroz (1987) data set as in Newey, Powell, and Walker (1990).",http://www.sciencedirect.com/science/article/pii/S0304407622000409
Journal of Econometrics,2024,Testing unconditional and conditional independence via mutual information,"Chunrong Ai, Li-Hsien Sun, Zheng Zhang and Liping Zhu","Testing independence has garnered increasing attention in the econometric and statistical literature. Many tests have been proposed, most of which are inconsistent against all departures from independence. Few of those tests, though consistent, suffer a significant loss of local power. This study proposes a mutual information test for testing independence. The proposed test is simple to implement and, with a slight loss of local power, is consistent against all departures from independence. The key driving factor is that we estimate the density ratio directly. This value is constant in a state of independence. This is in contrast with related studies that estimate the joint and marginal density functions to form the density ratio. A small-scale simulation study indicates that the proposed test outperforms the existing alternatives in various dependence structures.",http://www.sciencedirect.com/science/article/pii/S0304407622001609
Journal of Econometrics,2024,Kernel density estimation for undirected dyadic data,"Bryan S. Graham, Fengshi Niu and James Powell","We study nonparametric estimation of density functions for undirected dyadic random variables (i.e., random variables defined for all n≡defN2 unordered pairs of agents/nodes in a weighted network of order N). These random variables satisfy a local dependence property: any random variables in the network that share one or two indices may be dependent, while those sharing no indices in common are independent. In this setting, we show that density functions may be estimated by an application of the kernel estimation method of Rosenblatt (1956) and Parzen (1962). We suggest an estimate of their asymptotic variances inspired by a combination of (i) Newey’s (1994) method of variance estimation for kernel estimators in the “monadic” setting and (ii) a variance estimator for the (estimated) density of a simple network first suggested by Holland and Leinhardt (1976). More unusual are the rates of convergence and asymptotic (normal) distributions of our dyadic density estimates. Specifically, we show that they converge at the same rate as the (unconditional) dyadic sample mean: the square root of the number, N, of nodes. This differs from the results for nonparametric estimation of densities and regression functions for monadic data, which generally have a slower rate of convergence than their corresponding sample mean.",http://www.sciencedirect.com/science/article/pii/S0304407622001610
Journal of Econometrics,2024,One instrument to rule them all: The bias and coverage of just-ID IV,Joshua Angrist and Michal Kolesár,"We revisit the finite-sample behavior of single-variable just-identified instrumental variables (just-ID IV) estimators, arguing that in most microeconometric applications, the usual inference strategies are likely reliable. Three widely-cited applications are used to explain why this is so. We then consider pretesting strategies of the form t1>c, where t1 is the first-stage t-statistic, and the first-stage sign is given. Although pervasive in empirical practice, pretesting on the first-stage F-statistic exacerbates bias and distorts inference. We show, however, that median bias is both minimized and roughly halved by setting c=0, that is by screening on the sign of the estimated first stage. This bias reduction is a free lunch: conventional confidence interval coverage is unchanged by screening on the estimated first-stage sign. To the extent that IV analysts sign-screen already, these results strengthen the case for a sanguine view of the finite-sample behavior of just-ID IV.",http://www.sciencedirect.com/science/article/pii/S0304407623000295
Journal of Econometrics,2024,Is Newey–West optimal among first-order kernels?,"Thomas Kolokotrones, James H. Stock and Christopher D. Walker","Newey–West (1987) standard errors are the dominant standard errors used for heteroskedasticity and autocorrelation robust (HAR) inference in time series regression. The Newey–West estimator uses the Bartlett kernel, which is a first-order kernel, meaning that its characteristic exponent, q, is equal to 1, where q is defined as the largest value of r for which the quantity k[r](0)=limt→0|t|−r(k(0)−k(t)) is defined and finite. This raises the apparently uninvestigated question of whether the Bartlett kernel is optimal among first-order kernels. We demonstrate that, for q<2, there is no optimal qth-order kernel for HAR testing in the Gaussian location model or for minimizing the MSE in spectral density estimation. In fact, for any q<2, the space of qth-order positive-semidefinite kernels is not closed and, moreover, all continuous qth-order kernels can be decomposed into a weighted sum of qth and second-order kernels, which suggests that there is no meaningful notion of ‘pure’ qth-order kernels for q<2. Nevertheless, it is possible to rank any given collection of qth-order kernels using the functional Iq[k]=k[q](0)1/q∫k2(t)dt with smaller values corresponding to better asymptotic performance. We examine the value of Iq[k] for a wide variety of first-order estimators and find that none improve upon the Bartlett kernel. These comparisons provide additional justification for the continued use of the Newey–West estimator with testing-optimal smoothing parameters and fixed-b critical values despite the lack of optimality of Bartlett among first-order kernels.",http://www.sciencedirect.com/science/article/pii/S0304407623000301
Journal of Econometrics,2024,Instrumental variable estimation with first-stage heterogeneity,"Alberto Abadie, Jiaying Gu and Shu Shen","We propose a simple data-driven procedure that exploits heterogeneity in the first-stage correlation between an instrument and an endogenous variable to improve the asymptotic mean squared error (MSE) of instrumental variable estimators. We show that the resulting gains in asymptotic MSE can be quite large in settings where there is substantial heterogeneity in the first-stage parameters. We also show that a naive procedure used in some applied work, which consists of selecting the composition of the sample based on the value of the first-stage t-statistic, may cause substantial over-rejection of a null hypothesis on a second-stage parameter. We apply the methods to study (1) the return to schooling using the minimum school leaving age as the exogenous instrument and (2) the effect of local economic conditions on voter turnout using energy supply shocks as the source of identification.",http://www.sciencedirect.com/science/article/pii/S0304407623000702
Journal of Econometrics,2024,Heterogeneity of consumption responses to income shocks in the presence of nonlinear persistence,"Manuel Arellano, Richard Blundell, Stéphane Bonhomme and Jack Light","In this paper we use the enhanced consumption data in the Panel Survey of Income Dynamics (PSID) from 2005–2017 to explore the transmission of income shocks to consumption. We build on the nonlinear quantile framework introduced in Arellano et al. (2017). Our focus is on the estimation of consumption responses to persistent nonlinear income shocks in the presence of unobserved heterogeneity. To reliably estimate heterogeneous responses in our unbalanced panel, we develop Sequential Monte Carlo computational methods. We find substantial heterogeneity in consumption responses, and uncover latent types of households with different life-cycle consumption behavior. Ordering types according to their average log-consumption, we find that low-consumption types respond more strongly to income shocks at the beginning of the life cycle and when their assets are low, as standard life-cycle theory would predict. In contrast, high-consumption types respond less on average, and in a way that changes little with age or assets. We examine various mechanisms that might explain this heterogeneity.",http://www.sciencedirect.com/science/article/pii/S0304407623001434
Journal of Econometrics,2024,Assumption-lean falsification tests of rate double-robustness of double-machine-learning estimators,"Lin Liu, Rajarshi Mukherjee and James M. Robins","The class of doubly robust (DR) functionals studied by Rotnitzky et al. (2021) is of central importance in economics and biostatistics. It strictly includes both (i) the class of mean-square continuous functionals that can be written as an expectation of an affine functional of a conditional expectation studied by Chernozhukov et al. (2022b) and the class of functionals studied by Robins et al. (2008). The present state-of-the-art estimators for DR functionals ψ are double-machine-learning (DML) estimators (Chernozhukov et al., 2018a). A DML estimator ψ̂1 of ψ depends on estimates p̂(x) and b̂(x) of a pair of nuisance functions p(x) and b(x), and is said to satisfy “rate double-robustness” if the Cauchy–Schwarz upper bound of its bias is o(n−1/2). Rate double-robustness implies that the bias is o(n−1/2), but the converse is false. Were it achievable, our scientific goal would have been to construct valid, assumption-lean (i.e. no complexity-reducing assumptions on b or p) tests of the validity of a nominal (1−α) Wald confidence interval (CI) centered at ψ̂1. But this would require a test of the bias to be o(n−1/2), which can be shown not to exist. We therefore adopt the less ambitious goal of falsifying, when possible, an analyst’s justification for her claim that the reported (1−α) Wald CI is valid. In many instances, an analyst justifies her claim by imposing complexity-reducing assumptions on b and p to ensure “rate double-robustness”. Here we exhibit valid, assumption-lean tests of H0: “rate double-robustness holds”, with non-trivial power against certain alternatives. If H0 is rejected, we will have falsified her justification. However, no assumption-lean test of H0, including ours, can be a consistent test. Thus, the failure of our test to reject is not meaningful evidence in favor of H0.",http://www.sciencedirect.com/science/article/pii/S0304407623002166
Journal of Econometrics,2024,Likelihood approach to dynamic panel models with interactive effects,Jushan Bai,"This paper studies dynamic panel models with a factor error structure that is correlated with the regressors. Both short panels (small T) and long panels (large T) are considered. A dynamic panel forms a simultaneous-equation system, and under the factor error structure, there exist constraints between the mean and the covariance matrix. We explore the constraints through a quasi-FIML (full information maximum likelihood) approach. The quasi-FIML approach does not estimate individual effects, even if they are fixed constants, thus circumventing the incidental parameters problem in the cross-sectional dimension.",http://www.sciencedirect.com/science/article/pii/S0304407623003524
Journal of Econometrics,2024,Locally robust inference for non-Gaussian linear simultaneous equations models,Adam Lee and Geert Mesters,"All parameters in linear simultaneous equations models can be identified (up to permutation and sign) if the underlying structural shocks are independent and at most one of them is Gaussian. Unfortunately, existing inference methods that exploit such identifying assumptions suffer from size distortions when the true distributions of the shocks are close to Gaussian. To address this weak non-Gaussian problem we develop a locally robust semi-parametric inference method which is simple to implement, improves coverage and retains good power properties. The finite sample properties of the methodology are illustrated in a large simulation study and an empirical study for the returns to schooling.",http://www.sciencedirect.com/science/article/pii/S0304407623003639
Journal of Econometrics,2024,Cross-section bootstrap for CCE regressions,Ignace De Vos and Ovidijus Stauskas,"The Common Correlated Effects (CCE) methodology is now well established for the analysis of factor-augmented panel data models. Yet, it is often neglected that the pooled variant is biased unless the cross-section dimension (N) of the dataset dominates the time series length (T). This is problematic for inference with typical macroeconomic datasets, where T is often equal or larger than N. In response, we establish in this paper the theoretical foundation of the cross-section (CS) bootstrap for inference with CCE estimators in large N and T panels with TN−1→τ<∞. This resampling scheme is often used to estimate standard errors, yet without theoretical justification, and with unused potential, as we show it also provides a solution to the bias problem. We derive conditions under which the scheme replicates the distribution of the CCE estimators, such that bias can be eliminated and asymptotically valid inference can ensue. In so doing, we also spend attention to the case where factors need not be common across the dependent and explanatory variables, or when slopes are heterogeneous. Since we find that the CS-bootstrap applies in each case, researchers can stay agnostic on these issues. Simulation experiments show that the asymptotic properties also translate well to finite samples.",http://www.sciencedirect.com/science/article/pii/S0304407623003640
Journal of Econometrics,2024,Bias in local projections,Edward P. Herbst and Benjamin K. Johannsen,"Local projections (LPs) are a popular tool in macroeconomic research. We show that LPs are often used with very small samples in the time dimension and, consequently, that LP point estimates can be severely biased. Under regularity conditions, we derive simple expressions to approximate this bias and propose a way to correct for bias in LPs. Using a medium-scale macroeconomic time-series model, we demonstrate that the bias in point estimates can be economically meaningful. We also show that the same small-sample bias issue can lead some autocorrelation-robust standard errors to understate sampling uncertainty.",http://www.sciencedirect.com/science/article/pii/S0304407624000010
Journal of Econometrics,2024,Volatility of volatility and leverage effect from options,Carsten H. Chong and Viktor Todorov,"We propose model-free (nonparametric) estimators of the volatility of volatility and leverage effect using high-frequency observations of short-dated options. At each point in time, we integrate available options into estimates of the conditional characteristic function of the price increment until the options’ expiration and we use these estimates to recover spot volatility. Our volatility of volatility estimator is then formed from the sample variance and first-order autocovariance of the spot volatility increments, with the latter correcting for the bias in the former due to option observation errors. The leverage effect estimator is the sample covariance between price increments and the estimated volatility increments. The rate of convergence of the estimators depends on the diffusive innovations in the latent volatility process as well as on the observation error in the options with strikes in the vicinity of the current spot price. Feasible inference is developed in a way that does not require prior knowledge of the source of estimation error that is asymptotically dominating.",http://www.sciencedirect.com/science/article/pii/S0304407624000150
Journal of Econometrics,2024,Identification of a rational inattention discrete choice model,Moyu Liao,"This paper studies the non-parametric identification and estimation of an empirical rational inattention discrete choice model. Decision-makers do not observe their realized utility perfectly but can obtain a costly signal to inform them about the utility. This model nests the standard discrete choice model as a special case. We characterize the identified set of decision-makers’ prior beliefs based on cross-sectional market-level choice probabilities. We show that the standard discrete choice model is always observationally equivalent to some rational inattention discrete choice models with costly information. We then characterize the moment conditions that can be used to estimate the model parameters. When an exogenous variation of the choice set is available, we show that we can distinguish the standard discrete choice model from the rational inattention discrete choice model with costly information. We apply the model to study the Fox News effect on the 2000 U.S. presidential election. We compare the voters’ prior beliefs with and without the Fox News influence. The result shows that the presence of Fox News has heterogeneous effects for voters with different educational backgrounds.",http://www.sciencedirect.com/science/article/pii/S0304407624000162
Journal of Econometrics,2024,Time-varying multivariate causal processes,"Jiti Gao, Bin Peng, Wei Biao Wu and Yayi Yan","In this paper, we consider a wide class of time-varying multivariate causal processes that nests many classical and new examples as special cases. We first show the existence of a weakly dependent stationary approximation to initiate our theoretical investigation. We then consider a quasi-maximum likelihood estimation (QMLE), and provide both point-wise and uniform inferences to coefficient functions of interest. The theoretical findings are further examined through extensive simulations. Finally, we show empirical relevance of our study by evaluating both temporal and contemporaneous connectedness between the stock markets of China and U.S.",http://www.sciencedirect.com/science/article/pii/S0304407624000174
Journal of Econometrics,2024,Panel quantile regression for extreme risk,"Yanxi Hou, Xuan Leng, Liang Peng and Yinggang Zhou","Panel quantile regression models play an essential role in finance, insurance, and risk management applications. However, a direct application of panel regression for extreme conditional quantiles may suffer from a significant estimation uncertainty due to data sparsity on the far tail. We introduce a two-stage method to predict extreme conditional quantiles over cross-sections, which uses panel quantile regression at a selected intermediate level and then extrapolates the intermediate level to an extreme level with extreme value theory. This combination of panel quantile regression at an intermediate level and extreme value theory relies on a set of second-order conditions for heteroscedastic extremes. A metric called Average Absolute Relative Error is proposed to evaluate the prediction performance of both intermediate and extreme conditional quantiles. Allowing individual fixed effects in panel quantile regressions challenges the asymptotic analysis of the two-stage method and prediction metric. We demonstrate the finite sample performance of the extreme conditional quantile prediction compared to the direct use of panel quantile regression. Finally, an application of the two-stage method to the macroeconomic and housing price data finds strong evidence of housing bubbles and common economic factors.",http://www.sciencedirect.com/science/article/pii/S0304407624000204
Journal of Econometrics,2024,Enhanced pricing and management of bundled insurance risks with dependence-aware prediction using pair copula construction,Peng Shi and Zifeng Zhao,"We propose a dependence-aware predictive modeling framework for multivariate risks stemmed from an insurance contract with bundling features — an important type of policy increasingly offered by major insurance companies. The bundling feature naturally leads to longitudinal measurements of multiple insurance risks, and correct pricing and management of such risks is of fundamental interest to financial stability of the macroeconomy. We build a novel predictive model that fully captures the dependence among the multivariate repeated risk measurements. Specifically, the longitudinal measurement of each individual risk is first modeled using pair copula construction with a D-vine structure, and the multiple D-vines are then integrated by a flexible copula. While our analysis mainly focuses on multivariate insurance risks, the proposed model indeed contributes to the broad research area of longitudinal data analysis. In particular, it provides a unified modeling framework for multivariate longitudinal data that can accommodate different scales of measurements, including continuous, discrete, and mixed observations, and thus can be potentially useful for various economic studies. A computationally efficient sequential method is proposed for model estimation and inference, and its performance is investigated both theoretically and via simulation studies. In the application, we examine multivariate bundled risks in multi-peril property insurance using proprietary data from a commercial property insurance provider. The proposed model is found to provide improved decision making for several key insurance operations. For underwriting, we show that the experience rate priced by the proposed model leads to a 9% lift in the insurer’s net revenue. For reinsurance, we show that the insurer underestimates the risk of the retained insurance portfolio by 10% when ignoring the dependence among bundled insurance risks.",http://www.sciencedirect.com/science/article/pii/S0304407624000228
Journal of Econometrics,2024,Classical p-values and the Bayesian posterior probability that the hypothesis is approximately true,Brendan Kline,"This paper relates p-values for the hypothesis that θ=c to the Bayesian posterior probability that the hypothesis is approximately true, in the sense that θ∈[c−ϵ,c+ϵ] for a selected ϵ>0. In a setup with a continuous prior for θ, the results show that a larger (respectively, smaller) p-value does not necessarily correspond to a higher (respectively, lower) probability that θ is close to c. Therefore, the results suggest caution about common ways of using p-values, specifically the use of small p-values as a key standard in empirical research.",http://www.sciencedirect.com/science/article/pii/S030440762400023X
Journal of Econometrics,2024,Identifying the effects of a program offer with an application to Head Start,Vishal Kamat,"I propose a treatment selection model that introduces unobserved heterogeneity in both choice sets and preferences to evaluate the average effects of a program offer. I show how to exploit the model structure to define parameters capturing these effects and then computationally characterize their identified sets under instrumental variable variation in choice sets. I illustrate these tools by analyzing the effects of providing an offer to the Head Start preschool program using data from the Head Start Impact Study. I find that such a policy affects a large number of children who take up the offer, and that they subsequently have positive effects on test scores. These effects arise from children who do not have any preschool as an outside option. A cost–benefit analysis reveals that the earning benefits associated with the test score gains can be large and outweigh the net costs associated with offer take up.",http://www.sciencedirect.com/science/article/pii/S0304407624000253
Journal of Econometrics,2024,A computational approach to identification of treatment effects for policy evaluation,Sukjin Han and Shenshen Yang,"For counterfactual policy evaluation, it is important to ensure that treatment parameters are relevant to policies in question. This is especially challenging under unobserved heterogeneity, as is well featured in the definition of the local average treatment effect (LATE). Being intrinsically local, the LATE is known to lack external validity in counterfactual environments. This paper investigates the possibility of extrapolating local treatment effects to different counterfactual settings when instrumental variables can be only binary. We propose a novel framework to systematically calculate sharp nonparametric bounds on various policy-relevant treatment parameters that are defined as weighted averages of the marginal treatment effect (MTE). Our framework is flexible enough to fully incorporate statistical independence (rather than mean independence) of instruments and a large menu of identifying assumptions beyond the shape restrictions on the MTE that have been considered in prior studies. We apply our method to understand the effects of medical insurance policies on the use of medical services.",http://www.sciencedirect.com/science/article/pii/S0304407624000265
Journal of Econometrics,2024,The variance of regression coefficients when the population is finite,Richard Startz and Douglas Steigerwald,"Recent work has returned attention to the role of finite-population corrections in empirical settings. It is well established that if the only source of variation arises from the sampling design, then the asymptotic variance of regression estimators must include the proportion of the finite population that is sampled. If there is, in addition, a random shock to each element of the finite population, then it is commonly observed that the resulting super-population renders the finite-population correction moot. We explore this setting and find that this common observation does not fully capture the richness of the result. The fraction of the finite population that is sampled defines bounds on the variance of regression estimators. Ignoring the finite-population correction yields the upper bound, which can be quite conservative.",http://www.sciencedirect.com/science/article/pii/S0304407624000277
Journal of Econometrics,2024,Inference for low-rank completion without sample splitting with application to treatment effect estimation,"Jungjun Choi, Hyukjun Kwon and Yuan Liao","This paper studies the inferential theory for estimating low-rank matrices. It also provides an inference method for the average treatment effect as an application. We show that the least square estimation of eigenvectors following the nuclear norm penalization attains the asymptotic normality. The key contribution of our method is that it does not require sample splitting. In addition, this paper allows dependent observation patterns and heterogeneous observation probabilities. Empirically, we apply the proposed procedure to estimating the impact of the presidential vote on allocating the U.S. federal budget to the states.",http://www.sciencedirect.com/science/article/pii/S0304407624000289
Journal of Econometrics,2024,Confidence intervals of treatment effects in panel data models with interactive fixed effects,"Xingyu Li, Yan Shen and Qiankun Zhou",We augment the factor-based estimation of treatment effects proposed by Bai and Ng (2021) with easy-to-implement and nonparametric confidence intervals of treatment effects on every treated unit at every post-treatment time. The construction of confidence intervals entails a residual-based bootstrap resampling procedure. This method does not rely on any parametric assumption on the distribution of idiosyncratic errors and it is robust to weak cross-sectional and time-series dependence among idiosyncratic errors. We prove the asymptotic validity of the proposed confidence intervals as the numbers of control units and pre-treatment times go to infinity. We also extend this method to cases where the common factors and covariates (if any) are unit root processes. Monte Carlo experiments show that the proposed confidence intervals are well-behaved in finite samples and outperform confidence intervals based on normal quantiles. Empirical applications with two classical datasets add informative confidence intervals to existing point estimates of treatment effects.,http://www.sciencedirect.com/science/article/pii/S0304407624000307
Journal of Econometrics,2024,Panel data models with time-varying latent group structures,"Yiren Wang, Peter Phillips and Liangjun Su","This paper considers a linear panel model with interactive fixed effects and unobserved individual and time heterogeneities that are captured by some latent group structures and an unknown structural break, respectively. To enhance realism, the model may have different numbers of groups and/or different group memberships before and after the break. With preliminary nuclear norm regularized estimation followed by row- and column-wise linear regressions, we estimate the break point based on the idea of binary segmentation and the latent group structures together with the number of groups before and after the break by sequential testing K-means algorithm simultaneously. It is shown that the break point, the number of groups and the group memberships can each be estimated correctly with probability approaching one. Asymptotic distributions of the estimators of the slope coefficients are established. Monte Carlo simulations demonstrate excellent finite sample performance for the proposed estimation algorithm. An empirical application to real house price data across 377 Metropolitan Statistical Areas in the US from 1975 to 2014 suggests the presence both of structural breaks and of changes in group membership.",http://www.sciencedirect.com/science/article/pii/S0304407624000319
Journal of Econometrics,2024,Non-representative sampled networks: Estimation of network structural properties by weighting,"Chih-Sheng Hsieh, Yu-Chin Hsu, Stanley I.M. Ko, Jaromír Kovářík and Trevon Logan","This paper analyzes statistical issues arising from non-representative samples of a network. Sampled network data could systematically bias the network properties and generate non-classical measurement error problems. Apart from the sampling rate and the elicitation procedure, the biases on network structural measures depend non-trivially on which subpopulations of nodes are missing with higher probability. We propose a methodology, adapting weighted estimators to networked contexts, which enables researchers to recover several network-level statistics and reduce the biases in the estimated network effects. The proposed weighted estimators are consistent and asymptotically normally distributed and have good performance in finite samples. Notably, our approach does not require users to assume any network formation model and is straightforward to implement.",http://www.sciencedirect.com/science/article/pii/S0304407624000356
Journal of Econometrics,2024,Nonparametric estimation for high-frequency data incorporating trading information,"Wenhao Cui, Jie Hu and Jiandong Wang","We propose nonparametric estimators for the explicative part of the noise in a model where the market microstructure noise is an unknown function of the trading information while allowing for the presence of an additional residual noise component. Our method allows for dependence in the observable trading information and accommodates the presence of infinite variation jumps in the efficient price process. We establish the convergence and asymptotic normality of the proposed estimators. We also propose a two-step Laplace estimator of integrated volatility where we replace the observed price with the estimated price by removing the explicative part of the market microstructure noise. The finite sample properties of both the nonparametric estimators and the two-step Laplace estimator are examined through Monte Carlo simulations. We find that our method is robust to misspecification of the unknown functional form given finite sample size. Furthermore, an empirical application using high-frequency data demonstrates that our method outperforms commonly employed parametric methods.",http://www.sciencedirect.com/science/article/pii/S0304407624000368
Journal of Econometrics,2024,Robust inference on correlation under general heterogeneity,"Liudas Giraitis, Yufei Li and Peter Phillips","Considerable evidence in past research shows size distortion in standard tests for zero autocorrelation or zero cross-correlation when time series are not independent identically distributed random variables, pointing to the need for more robust procedures. Recent tests for serial correlation and cross-correlation in Dalla, Giraitis, and Phillips (2022) provide a more robust approach, allowing for heteroskedasticity and dependence in uncorrelated data under restrictions that require a smooth, slowly-evolving deterministic heteroskedasticity process. The present work removes those restrictions and validates the robust testing methodology for a wider class of innovations and regression residuals allowing for heteroscedastic uncorrelated and non-stationary data settings. The updated analysis given here enables more extensive use of the methodology in practical applications. Monte Carlo experiments confirm excellent finite sample performance of the robust test procedures even for extremely complex white noise processes. The empirical examples show that use of robust testing methods can materially reduce spurious evidence of correlations found by standard testing procedures.",http://www.sciencedirect.com/science/article/pii/S030440762400037X
Journal of Econometrics,2024,Finite underidentification,Enrique Sentana,"I adapt the Generalised Method of Moments to deal with nonlinear models in which a finite number of isolated parameter values satisfy the moment conditions. I also study the closely related class of first-order underidentified models, whose expected Jacobian is rank deficient but not necessarily zero. In both cases, my proposed procedures exploit the underidentification structure to yield parameter estimators and underidentification tests within a standard asymptotically normal GMM framework. I study nonlinear models with and without separation of data and parameters. I also illustrate my proposed inference procedures with applications to production function estimation and dynamic panel data models.",http://www.sciencedirect.com/science/article/pii/S0304407624000381
Journal of Econometrics,2024,Time-varying forecast combination for factor-augmented regressions with smooth structural changes,"Qitong Chen, Yongmiao Hong and Haiqi Li","This study proposes a time-varying forecast combination for factor-augmented (TVFCFA) regressions with smooth structural changes. First, we establish the limiting distribution of the estimators of the time-varying factor-augmented regressions. To estimate the optimal time-varying combination weights, we propose a local leave-l-out cross-validation (LLOCV) criterion that is asymptotically unbiased for the local mean squared forecast error (LMSFE). The TVFCFA method was shown to be asymptotically optimal in the sense that its LMSFE attains the infeasible lower bound. We establish the convergence rate of the selected weights and demonstrate that the TVFCFA method automatically assigns all weights to correctly specified models. Because the overfitted models have nonzero weights, the TVFCFA estimator asymptotically follows a nonstandard distribution. To obtain an asymptotic normal distribution, we propose a penalized LLOCV criterion such that the weights for the overfitted models asymptotically converge to zero. The TVFCFA estimator, with weights that minimize the penalized LLOCV, asymptotically follows a normal distribution, and the convergence rate of the weights assigned to the overfitted models is inversely proportional to the penalized factor. A Monte Carlo simulation shows that the TVFCFA method outperforms competing model averaging and selection methods that are popular in the literature. Moreover, an empirical application of the TVFCFA method to inflation forecasts demonstrates its superiority.",http://www.sciencedirect.com/science/article/pii/S0304407624000393
Journal of Econometrics,2024,"High frequency principal component analysis based on correlation matrix that is robust to jumps, microstructure noise and asynchronous observation times",Dachuan Chen,"This paper developed the high frequency estimation for the principal component analysis (PCA) based on correlation matrix. This estimation methodology is robust to jumps, microstructure noise and asynchronous observation times simultaneously, which is enabled by the newly proposed Truncated and Smoothed Two-Scales Realized Volatility (Truncated S-TSRV) estimator. The general framework of our methodology is constructed based on the estimation of realized spectral functions with respect to the spot correlation matrix. A new asymptotic representation for the element-wise estimation error of the spot correlation matrix estimate has been derived, resulting in a new bias correction term which is much more complex than that of the PCA based covariance matrix. Central limit theorem and rate of convergence have been developed for the bias-corrected estimator. The standard error estimator has also been proposed. As the empirical study of our methodology, we have constructed the first eigen-portfolio based on the eigenvector estimate corresponding to the largest eigenvalue in the spot correlation matrix. We regress the returns of first eigen-portfolio against that of the market ETF, which obtained non-significant alpha estimate and significant beta estimate which is very close to one.",http://www.sciencedirect.com/science/article/pii/S0304407624000472
Journal of Econometrics,2024,Testing specification of distribution in stochastic frontier analysis,"Ming-Yen Cheng, Shouxia Wang, Lucy Xia and Xibin Zhang","Stochastic frontier analysis is regularly used in empirical studies to evaluate the productivity and efficiency of companies. A typical stochastic frontier model involves a parametric frontier subject to a composite error term consisting of an inefficiency and a random error. We develop new tests for specification of distribution of the inefficiency. We focus on simultaneous relaxation of two common assumptions: (1) parametric frontier which may lead to false conclusions when misspecified, and (2) homoscedasticity which can be easily violated when working with real data. While these two issues have been extensively studied in prior research exploring the estimation of a stochastic frontier and inefficiencies, they have not been properly addressed in the considered testing problem. We propose novel bootstrap and asymptotic distribution-free tests with neither parametric frontier nor homoscedasticity assumptions, in both cross-sectional and panel settings. Our tests are asymptotically consistent, simple to implement and widely applicable. Their powers against general fixed alternatives tend to one as sample size increases, and they can detect root-n order local alternatives. We demonstrate their efficacies through extensive simulation studies. When applied to a banking panel dataset, our tests provide sound justification for the commonly used exponential specification for banking data. The findings also show that a new parametric frontier model is more plausible than the conventional translog frontier.",http://www.sciencedirect.com/science/article/pii/S0304407622000677
Journal of Econometrics,2024,Testing equality of several distributions in separable metric spaces: A maximum mean discrepancy based approach,"Jin-Ting Zhang, Jia Guo and Bu Zhou","A new test for equal distributions of several high-dimensional samples in separable metric spaces, with its test statistic constructed based on maximum mean discrepancy, is proposed and studied. The asymptotic null and alternative distributions of the test statistic are established under some mild conditions. The new test is implemented via a three-cumulant matched chi-square approximation with the associated approximation parameters consistently estimated from the data. A new data-adaptive Gaussian kernel width selection method is also suggested. Good performance of the new test is illustrated by intensive simulation studies and a real data example of Gini index curves.",http://www.sciencedirect.com/science/article/pii/S0304407622000859
Journal of Econometrics,2024,Asset splitting algorithm for ultrahigh dimensional portfolio selection and its theoretical property,"Zhanrui Cai, Changcheng Li, Jiawei Wen and Songshan Yang","The presence of a huge number of assets poses challenges to classical portfolio selection algorithms. Constrained l1 minimization approaches have been proposed to directly estimate effective parameters in the optimal portfolio. Linear programming method and alternating direction method of multiplier (ADMM) algorithm is used to solve the corresponding minimization problems. However, these two algorithms may fail due to the limitations of computing time and computing memory when a huge number of assets are considered in the portfolio optimization. This article proposes an asset splitting ADMM (AS-ADMM for short), a parallel computing algorithm, to tackle such challenges, and establishes the convergence property of the new algorithm. Furthermore, we develop a new regularization method for estimating the effective parameters with the folded-concave penalty and establish its oracle property. The local linear approximation (LLA) algorithm is used to redirect the new method to a weighted l1 regularization method. We conduct simulation studies to investigate the advantage of the proposed algorithm and regularized model in solving the high dimensional portfolio selection problems. A real data example is also included to demonstrate the applicability of the proposed algorithms and regularization methods.",http://www.sciencedirect.com/science/article/pii/S0304407622000902
Journal of Econometrics,2024,A Multi-Kink quantile regression model with common structure for panel data analysis,"Yan Sun, Chuang Wan, Wenyang Zhang and Wei Zhong","Stimulated by the analysis of a data set on financial portfolio returns, we propose a multi-kink quantile regression (MKQR) model with latent homogeneous structure for panel data analysis. The proposed model accounts for both homogeneity and heterogeneity among individuals and parameters in panel data analysis. From statistical modeling point of view, it well balances the risk of misspecification and the model parsimony. From practical point of view, it is able to reveal not only the impacts of covariates in the global sense, but also individual attributes. An estimation procedure is presented to estimate both the unknown parameters and the latent homogeneous structure in the proposed model. Computational issues with the implementation of the estimation procedure are also discussed. Asymptotic theory of the estimators is established. It shows the necessity of taking into account both homogeneity and heterogeneity in panel data analysis. Monte Carlo simulation studies are conducted to demonstrate the finite sample performance of the proposed estimation and the risk of ignoring the homogeneity or heterogeneity among individuals. Finally, we apply the proposed model and the estimation procedure to the data set which stimulates this work and reveal some interesting findings.",http://www.sciencedirect.com/science/article/pii/S0304407622001178
Journal of Econometrics,2024,Optimal covariance matrix estimation for high-dimensional noise in high-frequency data,"Jinyuan Chang, Qiao Hu, Cheng Liu and Cheng Yong Tang","We consider high-dimensional measurement errors with high-frequency data. Our objective is on recovering the high-dimensional cross-sectional covariance matrix of the random errors with optimality. In this problem, not all components of the random vector are observed at the same time and the measurement errors are latent variables, leading to major challenges besides high data dimensionality. We propose a new covariance matrix estimator in this context with appropriate localization and thresholding, and then conduct a series of comprehensive theoretical investigations of the proposed estimator. By developing a new technical device integrating the high-frequency data feature with the conventional notion of α-mixing, our analysis successfully accommodates the challenging serial dependence in the measurement errors. Our theoretical analysis establishes the minimax optimal convergence rates associated with two commonly used loss functions; and we demonstrate with concrete cases when the proposed localized estimator with thresholding achieves the minimax optimal convergence rates. Considering that the variances and covariances can be small in reality, we conduct a second-order theoretical analysis that further disentangles the dominating bias in the estimator. A bias-corrected estimator is then proposed to ensure its practical finite sample performance. We also extensively analyze our estimator in the setting with jumps, and show that its performance is reasonably robust. We illustrate the promising empirical performance of the proposed estimator with extensive simulation studies and a real data analysis.",http://www.sciencedirect.com/science/article/pii/S0304407622001543
Journal of Econometrics,2024,A generalized knockoff procedure for FDR control in structural change detection,"Jingyuan Liu, Ao Sun and Yuan Ke","Controlling false discovery rate (FDR) is crucial for variable selection, multiple testing, among other signal detection problems. In literature, there is certainly no shortage of FDR control strategies when selecting individual features, but the relevant works for structural change detection, such as profile analysis for piecewise constant coefficients and integration analysis with multiple data sources, are limited. In this paper, we propose a generalized knockoff procedure (GKnockoff) for FDR control under such problem settings. We prove that the GKnockoff possesses pairwise exchangeability, and is capable of controlling the exact FDR under finite sample sizes. We further explore GKnockoff under high dimensionality, by first introducing a new screening method to filter the high-dimensional potential structural changes. We adopt a data splitting technique to first reduce the dimensionality via screening and then conduct GKnockoff on the refined selection set. Furthermore, the powers of proposed methods are systematically studied. Numerical comparisons with other methods show the superior performance of GKnockoff, in terms of both FDR control and power. We also implement the proposed methods to analyze a macroeconomic dataset for detecting changes of driven effects of economic development on the secondary industry.",http://www.sciencedirect.com/science/article/pii/S0304407622001567
Journal of Econometrics,2024,Time-varying minimum variance portfolio,"Qingliang (Michael) Fan, Ruike Wu, Yanrong Yang and Wei Zhong","This paper proposes a new time-varying minimum variance portfolio (TV-MVP) in a large investment universe of assets. Our method extends the existing literature on minimum variance portfolios by allowing for time-varying factor loadings, which facilitates the capture of the dynamics of the covariance structure of asset returns (and hence, the optimal investment strategy in a dynamic setting). We also use a shrinkage estimation method based on a quasi-likelihood function to regularize the residual covariances further. We establish the desired theoretical properties of proposed time-varying covariance and the optimal portfolio estimators under a more realistic heavy-tailed distribution. Specifically, we provide consistency of the optimal Sharpe ratio of the TV-MVP and the sharp risk consistency. Moreover, we offer a test of constant covariance structure and show the asymptotic distribution of the test statistic. Simulation and empirical studies suggest that the performance of the proposed TV-MVP is superior, in terms of estimation accuracy and out-of-sample Sharpe ratio, compared with that of other popular contemporary methods.",http://www.sciencedirect.com/science/article/pii/S0304407622001646
Journal of Econometrics,2024,A latent class Cox model for heterogeneous time-to-event data,"Youquan Pei, Heng Peng and Jinfeng Xu","Credit risk plays a vital role in the era of digital finance and it is one of primary interests to identify customers with similar types of risk categories so that personalized financial services can be offered accordingly. Motivated by the bourgeoning need for default risk modeling in finance, we propose herein a latent class Cox model for heterogeneous time-to-event data. The proposed model naturally extends the Cox proportional hazards model to flexibly take into account the heterogeneity of covariate effects as often manifested in real data. Without a priori specification of the number of latent classes, it simultaneously incorporates the commonalities and disparities of individual customers’ risk behaviors and provides a more refined modeling technique than existing approaches. We further propose a penalized maximum likelihood approach to identify the number of latent classes and estimate the model parameters. A modified expectation–maximization algorithm is then developed for its numerical implementation. Simulation studies are conducted to assess the finite-sample performance of the proposed approach. Its illustration with a real credit card data set is also provided.",http://www.sciencedirect.com/science/article/pii/S0304407622001841
Journal of Econometrics,2024,A post-screening diagnostic study for ultrahigh dimensional data,"Yaowu Zhang, Yeqing Zhou and Liping Zhu","We propose a consistent lack-of-fit test to assess whether replacing the original ultrahigh dimensional covariates with a given number of linear combinations results in a loss of regression information. To attenuate the spurious correlations that may inflate type-I error rates in high dimensions, we suggest to randomly split the observations into two parts. In the first part, we screen out as many irrelevant covariates as possible. This screening step helps to reduce the ultrahigh dimensionality to a moderate scale. In the second part, we perform a lack-of-fit test for conditional independence in the context of sufficient dimension reduction. In case that some important covariates are missed with a non-ignorable probability in the first screening stage, we introduce a multiple splitting procedure. We further propose a new statistic to test for conditional independence, which is shown to be n-consistent under the null and root-n-consistent under the alternative. We develop a consistent bootstrap procedure to approximate the asymptotic null distribution. The performances of our proposal are evaluated through comprehensive simulations and an empirical analysis of GDP data.",http://www.sciencedirect.com/science/article/pii/S0304407622001877
Journal of Econometrics,2024,Mixed membership estimation for social networks,"Jiashun Jin, Zheng Tracy Ke and Shengming Luo","In economics and social science, network data are regularly observed, and a thorough understanding of the network community structure facilitates the comprehension of economic patterns and activities. Consider an undirected network with n nodes and K communities. We model the network using the Degree-Corrected Mixed-Membership (DCMM) model, where for each node i=1,2,…,n, there exists a membership vector πi=(πi(1),πi(2),…,πi(K))′, where πi(k) is the weight that node i puts on community k, 1≤k≤K. In comparison to the well-known stochastic block model (SBM), the DCMM permits both severe degree heterogeneity and mixed memberships, making it more realistic and general. We present an efficient approach, Mixed-SCORE, for estimating the mixed membership vectors of all nodes and the other DCMM parameters. This approach is inspired by the discovery of a delicate simplex structure in the spectral domain. We derive explicit error rates for the Mixed-SCORE algorithm and demonstrate that it is rate-optimal over a broad parameter space. Our findings provide a novel statistical tool for network community analysis, which can be used to understand network formations, extract nodal features, identify unobserved covariates in dyadic regressions, and estimate peer effects. We applied Mixed-SCORE to a political blog network, two trade networks, a co-authorship network, and a citee network, and obtained interpretable results.",http://www.sciencedirect.com/science/article/pii/S0304407622002081
Journal of Econometrics,2024,An autocovariance-based learning framework for high-dimensional functional time series,"Jinyuan Chang, Cheng Chen, Xinghao Qiao and Qiwei Yao","Many scientific and economic applications involve the statistical learning of high-dimensional functional time series, where the number of functional variables is comparable to, or even greater than, the number of serially dependent functional observations. In this paper, we model observed functional time series, which are subject to errors in the sense that each functional datum arises as the sum of two uncorrelated components, one dynamic and one white noise. Motivated from the fact that the autocovariance function of observed functional time series automatically filters out the noise term, we propose a three-step framework by first performing autocovariance-based dimension reduction, then formulating a novel autocovariance-based block regularized minimum distance estimation to produce block sparse estimates, and based on which obtaining the final functional sparse estimates. We investigate theoretical properties of the proposed estimators, and illustrate the proposed estimation procedure with the corresponding convergence analysis via three sparse high-dimensional functional time series models. We demonstrate via both simulated and real datasets that our proposed estimators significantly outperform their competitors.",http://www.sciencedirect.com/science/article/pii/S0304407623000167
Journal of Econometrics,2024,Mining the factor zoo: Estimation of latent factor models with sufficient proxies,"Runzhe Wan, Yingying Li, Wenbin Lu and Rui Song","Latent factor model estimation typically relies on either using domain knowledge to manually pick several observed covariates as factor proxies, or purely conducting multivariate analysis such as principal component analysis. However, the former approach may suffer from the bias while the latter cannot incorporate additional information. We propose to bridge these two approaches while allowing the number of factor proxies to diverge, and hence make the latent factor model estimation robust, flexible, and statistically more accurate. As a bonus, the number of factors is also allowed to grow. At the heart of our method is a penalized reduced rank regression to combine information. To further deal with heavy-tailed data, a computationally attractive penalized robust reduced rank regression method is proposed. We establish faster rates of convergence compared with the benchmark. Extensive simulations and real examples are used to illustrate the advantages.",http://www.sciencedirect.com/science/article/pii/S0304407623000179
Journal of Econometrics,2024,Robustifying Markowitz,"Alla Petukhina, Yegor Klochkov, Wolfgang Karl Härdle and Nikita Zhivotovskiy","Markowitz mean–variance portfolios with sample mean and covariance as input parameters feature numerous issues in practice. They perform poorly out of sample due to estimation error, they experience extreme weights together with high sensitivity to change in input parameters. The heavy-tail characteristics of financial time series are in fact the cause for these erratic fluctuations of weights that consequently create substantial transaction costs. In robustifying the weights we present a toolbox for stabilizing costs and weights for global minimum Markowitz portfolios. Utilizing a projected gradient descent (PGD) technique, we avoid the estimation and inversion of the covariance operator as a whole and concentrate on robust estimation of the gradient descent increment. Using modern tools of robust statistics we construct a computationally efficient estimator with almost Gaussian properties based on median-of-means uniformly over weights. This robustified Markowitz approach is confirmed by empirical studies on equity markets. We demonstrate that robustified portfolios reach the lowest turnover compared to shrinkage-based and constrained portfolios while preserving or slightly improving out-of-sample performance.",http://www.sciencedirect.com/science/article/pii/S0304407623000180
Journal of Econometrics,2024,"Spherical autoregressive models, with application to distributional and compositional time series",Changbo Zhu and Hans-Georg Müller,"We introduce a new class of autoregressive models for spherical time series. The dimension of the spheres on which the observations of the time series are situated may be finite-dimensional or infinite-dimensional, where in the latter case we consider the Hilbert sphere. Spherical time series arise in various settings. We focus here on distributional and compositional time series. Applying a square root transformation to the densities of the observations of a distributional time series maps the distributional observations to the Hilbert sphere, equipped with the Fisher–Rao metric. Likewise, applying a square root transformation to the components of the observations of a compositional time series maps the compositional observations to a finite-dimensional sphere, equipped with the geodesic metric on spheres. The challenge in modeling such time series lies in the intrinsic non-linearity of spheres and Hilbert spheres, where conventional arithmetic operations such as addition or scalar multiplication are no longer available. To address this difficulty, we consider rotation operators to map observations on the sphere. Specifically, we introduce a class of skew-symmetric operators such that the associated exponential operators are rotation operators that for each given pair of points on the sphere map the first point of the pair to the second point of the pair. We exploit the fact that the space of skew-symmetric operators is Hilbertian to develop autoregressive modeling of geometric differences that correspond to rotations of spherical and distributional time series. Differences expressed in terms of rotations can be taken between the Fréchet mean and the observations or between consecutive observations of the time series. We derive theoretical properties of the ensuing autoregressive models and showcase these approaches with several motivating data. These include a time series of yearly observations of bivariate distributions of the minimum/maximum temperatures for a period of 120 days during each summer for the years 1990-2018 at Los Angeles (LAX) and John F. Kennedy (JFK) international airports. A second data application concerns a compositional time series with annual observations of compositions of energy sources for power generation in the U.S..",http://www.sciencedirect.com/science/article/pii/S0304407623000209
Journal of Econometrics,2024,The nonparametric Box–Cox model for high-dimensional regression analysis,He Zhou and Hui Zou,"The mainstream theory for high-dimensional regression assumes that the underlying true model is a low-dimensional linear regression model. On the other hand, a standard technique in regression analysis, even in the traditional low-dimensional setting, is to employ the Box–Cox transformation for reducing anomalies such as non-additivity and heteroscedasticity in linear regression. In this paper, we propose a new high-dimensional regression method based on a nonparametric Box–Cox model with an unspecified monotone transformation function. Model fitting and computation become much more challenging than the usual penalized regression method, and a two-step method is proposed for the estimation of this model in high-dimensional settings. First, we propose a novel technique called composite probit regression (CPR) and use the folded concave penalized CPR for estimating the regression parameters. The strong oracle property of the estimator is established without knowing the nonparametric transformation function. Next, the nonparametric function is estimated by conducting univariate monotone regression. The computation is done efficiently by using a coordinate-majorization-descent algorithm. Extensive simulation studies show that the proposed method performs well in various settings. Our analysis of the supermarket data demonstrates the superior performance of the proposed method over the standard high-dimensional regression method.",http://www.sciencedirect.com/science/article/pii/S0304407623000568
Journal of Econometrics,2024,Stock co-jump networks,"Yi Ding, Yingying Li, Guoli Liu and Xinghua Zheng","We propose a Degree-Corrected Block Model with Dependent Multivariate Poisson edges (DCBM-DMP) to study stock co-jump dependence. To estimate the community structure, we extend the SCORE algorithm in Jin (2015) and develop a Spectral Clustering On Ratios-of-Eigenvectors for networks with Dependent Multivariate Poisson edges (SCORE-DMP) algorithm. We prove that SCORE-DMP enjoys strong consistency in community detection. Empirically, using high-frequency data of S&P 500 constituents, we construct two co-jump networks according to whether the market jumps and find that they exhibit different community features than GICS. We further show that the co-jump networks help in stock return prediction.",http://www.sciencedirect.com/science/article/pii/S030440762300057X
Journal of Econometrics,2024,High frequency market making: The role of speed,Yacine Ait-Sahalia and Mehmet Sağlam,"We propose a model where a strategic high frequency market maker exploits his speed and informational advantages to place quotes that interact with the market orders of low frequency traders. We characterize the optimal market making policy and the equilibrium that results. The model has testable implications regarding the impact of speed on the provision of liquidity. We test these implications by taking advantage of a natural experiment on the NYSE American stock exchange, which implemented an intentional delay in 2017 then removed it in 2019. We find broad agreement between the empirical evidence and the implications of the model.",http://www.sciencedirect.com/science/article/pii/S0304407623000581
Journal of Econometrics,2024,Inferential theory for generalized dynamic factor models,"Matteo Barigozzi, Marc Hallin, Matteo Luciani and Paolo Zaffaroni","We provide the asymptotic distributional theory for the so-called General or Generalized Dynamic Factor Model (GDFM), laying the foundations for an inferential approach in the GDFM analysis of high-dimensional time series. By exploiting the duality between common shocks and dynamic loadings, we derive the asymptotic distribution and associated standard errors for a class of estimators for common shocks, dynamic loadings, common components, and impulse response functions. We present an empirical application aimed at constructing a “core” inflation indicator for the U.S. economy, which demonstrates the superiority of the GDFM-based indicator over the most common approaches, particularly the one based on Principal Components.",http://www.sciencedirect.com/science/article/pii/S0304407623000593
Journal of Econometrics,2024,Realized regression with asynchronous and noisy high frequency and high dimensional data,"Dachuan Chen, Per A. Mykland and Lan Zhang","We develop regression for high frequency data. This regression is novel in that it can be for both fixed and increasing dimension. Also, the data may have microstructure noise, and observations (trades, or quotes) can be asynchronous, (i.e., the observations do not need to be synchronized across dimensions). As is customary for high-frequency inference methods, we refer to our method as “realized” regression.",http://www.sciencedirect.com/science/article/pii/S030440762300132X
Journal of Econometrics,2024,Power enhancement for testing multi-factor asset pricing models via Fisher’s method,"Xiufan Yu, Jiawei Yao and Lingzhou Xue","Testing multi-factor asset pricing models is instrumental for asset pricing theory and practice. However, due to the accumulation of errors in estimating high-dimensional parameters, traditional quadratic-form tests such as the Wald test perform poorly against the sparse alternative hypothesis, i.e., a few mispriced assets. Fan et al. (2015b) introduced a powerful testing procedure by adding a power enhancement component to the Wald test statistic and proved power enhancement properties. To provide an alternative to their methodology, we first instantiate the power enhancement component by introducing a new maximum-form test statistic and then study the asymptotic joint distribution of the Wald test statistic and the maximum test statistic. We prove that these two test statistics are asymptotically independent. Given their asymptotic independence, we propose a new power-enhanced testing procedure to combine their respective power based on Fisher’s method (Fisher, 1925). Theoretically, we prove that the new power-enhanced test retains the desired nominal significance level and achieves asymptotically consistent power against more general alternatives. Furthermore, we demonstrate the finite-sample performance of our proposed power-enhanced test in both simulation studies and an empirical study of testing market efficiency using asset returns of the Russel-2000 portfolio.",http://www.sciencedirect.com/science/article/pii/S0304407623001525
Journal of Econometrics,2024,Retire: Robust expectile regression in high dimensions,"Rebeka Man, Kean Ming Tan, Zian Wang and Wen-Xin Zhou","High-dimensional data can often display heterogeneity due to heteroscedastic variance or inhomogeneous covariate effects. Penalized quantile and expectile regression methods offer useful tools to detect heteroscedasticity in high-dimensional data. The former is computationally challenging due to the non-smooth nature of the check loss, and the latter is sensitive to heavy-tailed error distributions. In this paper, we propose and study (penalized) robust expectile regression (retire), with a focus on iteratively reweighted ℓ1-penalization which reduces the estimation bias from ℓ1-penalization and leads to oracle properties. Theoretically, we establish the statistical properties of the retire estimator under two regimes: (i) low-dimensional regime in which d≪n; (ii) high-dimensional regime in which s≪n≪d with s denoting the number of significant predictors. In the high-dimensional setting, we thoroughly analyze the statistical properties of the solution path of iteratively reweighted ℓ1-penalized retire estimation, adapted from the local linear approximation algorithm for folded-concave regularization. Under a mild minimum signal strength condition, we demonstrate that with as few as log(logd) iterations, the final iterate of our proposed approach achieves the oracle convergence rate. At each iteration, we solve the weighted ℓ1-penalized convex program using a semismooth Newton coordinate descent algorithm. Numerical studies demonstrate the promising performance of the proposed procedure in comparison to both non-robust and quantile regression based alternatives.",http://www.sciencedirect.com/science/article/pii/S0304407623001537
Journal of Econometrics,2024,Inference on the best policies with many covariates,"Waverly Wei, Yuqing Zhou, Zeyu Zheng and Jingshen Wang","Understanding the impact of the most effective policies or treatments on a response variable of interest is desirable in many empirical works in economics, statistics and other disciplines. Due to the widespread winner’s curse phenomenon, conventional statistical inference assuming that the top policies are chosen independent of the random sample may lead to overly optimistic evaluations of the best policies. In recent years, given the increased availability of large datasets, such an issue can be further complicated when researchers include many covariates to estimate the policy or treatment effects in an attempt to control for potential confounders. In this manuscript, to simultaneously address the above-mentioned issues, we propose a resampling-based procedure that not only lifts the winner’s curse in evaluating the best policies observed in a random sample, but also is robust to the presence of many covariates. The proposed inference procedure yields accurate point estimates and valid frequentist confidence intervals that achieve the exact nominal level as the sample size goes to infinity for multiple best policy effect sizes. We illustrate the finite-sample performance of our approach through Monte Carlo experiments and two empirical studies, evaluating the most effective policies in charitable giving and the most beneficial group of workers in the National Supported Work program.",http://www.sciencedirect.com/science/article/pii/S0304407623001549
Journal of Econometrics,2024,Bipartite network influence analysis of a two-mode network,"Yujia Wu, Wei Lan, Xinyan Fan and Kuangnan Fang","A two-mode network contains two types of nodes, and edges exist only between any two nodes that are associated with different entities. Owing to the network connections (i.e., edges) between the two types of network nodes, nodal responses are unlikely to be independently and identically distributed, resulting in possible nodal heterogeneity across the two types of nodes. This study proposes a novel bipartite network influence model (BNIM) to evaluate nodal heterogeneity from the perspective of nodal influence. To make the model estimable, we parameterize the influence indices with a set of nodal attributes through a prespecified link function, and employ the quasi-maximum likelihood approach to estimate the unknown parameters. Score tests are presented to examine the heterogeneity of nodal influences across the two types of nodes. To assess the adequacy of the link function, we carry out a quasi-likelihood ratio test and establish its asymptotic properties under the appropriate conditions. Simulation studies and the real data analysis of a fund-stock network are studied to assess the finite-sample performance of BNIM.",http://www.sciencedirect.com/science/article/pii/S0304407623002786
Journal of Econometrics,2024,Dynamic modeling for multivariate functional and longitudinal data,"Siteng Hao, Shu-Chin Lin, Jane-Ling Wang and Qixian Zhong","Dynamic interactions among several stochastic processes are common in many scientific fields. It is crucial to model these interactions to understand the dynamic relationship of the corresponding multivariate processes with their derivatives and to improve predictions. In reality, full observations of the multivariate processes are not feasible as measurements can only be taken at discrete locations or time points, and often only sparingly and intermittently in longitudinal studies. This results in multivariate longitudinal data that are measured at different times for different subjects. We propose a time-dynamic model to handle multivariate longitudinal data by modeling the derivatives of multivariate processes using the values of these processes. Starting with a linear concurrent model, we develop methods to estimate the regression coefficient functions, which can accommodate irregularly measured longitudinal data that are possibly contaminated with noise. Our approach can also be applied to settings when the observational times are the same for all subjects. We establish the convergence rates of our estimators with phase transitions and further illustrate our model through a simulation study and a real data application.",http://www.sciencedirect.com/science/article/pii/S0304407623002890
Journal of Econometrics,2024,Volatility prediction comparison via robust volatility proxies: An empirical deviation perspective,"Weichen Wang, Ran An and Ziwei Zhu","Volatility forecasting is crucial to risk management and portfolio construction. One particular challenge of assessing volatility forecasts is how to construct a robust proxy for the unknown true volatility. In this work, we show that the empirical loss comparison between two volatility predictors hinges on the deviation of the volatility proxy from the true volatility. We then establish non-asymptotic deviation bounds for three robust volatility proxies, two of which are based on clipped data, and the third of which is based on exponentially weighted Huber loss minimization. In particular, in order for the Huber approach to adapt to non-stationary financial returns, we propose to solve a tuning-free weighted Huber loss minimization problem to jointly estimate the volatility and the optimal robustification parameter at each time point. We then inflate this robustification parameter and use it to update the volatility proxy to achieve optimal balance between the bias and variance of the global empirical loss. We also extend this Huber method to construct volatility predictors. Finally, we exploit the proposed robust volatility proxy to compare different volatility predictors on the Bitcoin market data and calibrated synthetic data. It turns out that when the sample size is limited, applying the robust volatility proxy gives more consistent and stable evaluation of volatility forecasts.",http://www.sciencedirect.com/science/article/pii/S0304407623003494
Journal of Econometrics,2024,Reprint: Statistical inference for linear mediation models with high-dimensional mediators and application to studying stock reaction to COVID-19 pandemic,"Xu Guo, Runze Li, Jingyuan Liu and Mudong Zeng","Mediation analysis draws increasing attention in many research areas such as economics, finance and social sciences. In this paper, we propose new statistical inference procedures for high dimensional mediation models, in which both the outcome model and the mediator model are linear with high dimensional mediators. Traditional procedures for mediation analysis cannot be used to make statistical inference for high dimensional linear mediation models due to high-dimensionality of the mediators. We propose an estimation procedure for the indirect effects of the models via a partially penalized least squares method, and further establish its theoretical properties. We further develop a partially penalized Wald test on the indirect effects, and prove that the proposed test has a χ2 limiting null distribution. We also propose an F-type test for direct effects and show that the proposed test asymptotically follows a χ2-distribution under null hypothesis and a noncentral χ2-distribution under local alternatives. Monte Carlo simulations are conducted to examine the finite sample performance of the proposed tests and compare their performance with existing ones. We further apply the newly proposed statistical inference procedures to study stock reaction to COVID-19 pandemic via an empirical analysis of studying the mediation effects of financial metrics that bridge company’s sector and stock return.",http://www.sciencedirect.com/science/article/pii/S0304407623003664
Journal of Econometrics,2024,Reprint: Hypothesis testing on high dimensional quantile regression,"Zhao Chen, Vivian Xinyi Cheng and Xu Liu","Quantile regression has been an important analytical tool in econometrics since it was proposed in 1970s. Many advantages make it still popular in the era of big data. This paper focuses on the testing problems of high-dimensional quantile regression, supplementing to robust methods in the literature of high-dimensional hypothesis testing. We first construct a new test statistic based on the quantile regression score function. The new test statistic avoids the inverse operation of the covariance matrix, and hence becomes applicable to high-dimensional or even ultrahigh-dimensional settings. The proposed method retains robustness for non-Gaussian and heavy-tailed distributions. We then derive the limiting distributions of the proposed test statistic under both the null and the alternative hypotheses. We further investigate the case where the design matrix follows an elliptical distribution. We examined the finite sample performance of our proposed method through Monte Carlo simulations. The numerical comparisons exhibit that our proposed tests outperform some existing methods in terms of controlling Type I error rate and power, when the data deviate from the Gaussian assumptions or are heavy-tailed. We illustrate our proposed high-dimensional quantile testing in financial econometrics, through an empirical analysis of Chinese stock market data.",http://www.sciencedirect.com/science/article/pii/S0304407623003676
Journal of Econometrics,2024,Beyond RCP8.5: Marginal mitigation using quasi-representative concentration pathways,J. Miller and William A. Brock,"Assessments of decreases in economic damages from climate change mitigation typically rely on climate output from computationally expensive pre-computed runs of general circulation models under a handful of scenarios with discretely varying targets, such as the four representative concentration pathways for CO2 and other anthropogenically emitted gases. Although such analyses are valuable in informing scientists and policymakers about massive multilateral mitigation goals, we add to the literature by considering potential outcomes from more modest policy changes that may not be represented by any well-known concentration pathway. Specifically, we construct computationally efficient Quasi-representative Concentration Pathways (QCPs) to leverage concentration pathways of existing peer-reviewed scenarios. Computational efficiency allows for bootstrapping to assess uncertainty. We illustrate our methodology by considering the impact on the relative risk of mortality from heat stress in London from the United Kingdom’s net zero emissions goal. More than half of our interval estimate for the business-as-usual scenario covers an annual risk at least that of a COVID-19-like mortality event by 2100. Success of the UK’s policy alone would do little to mitigate the risk.",http://www.sciencedirect.com/science/article/pii/S0304407621001792
Journal of Econometrics,2024,Modelling cycles in climate series: The fractional sinusoidal waveform process,Tommaso Proietti and Federico Maddanu,"The paper proposes a novel model for time series displaying persistent stationary cycles, the fractional sinusoidal waveform process. The underlying idea is to allow the parameters that regulate the amplitude and phase to evolve according to fractional noise processes. Its advantages with respect to popular alternative specifications, such as the Gegenbauer process, are twofold: the autocovariance function is available in closed form, which opens the way to exact maximum likelihood estimation; secondly, the model encompasses deterministic cycles, so that discrete spectra arise as a limiting case. A generalization of the process, featuring multiple components, an additive ‘red noise’ component and exogenous variables, provides the basic model for climate time series with mixed spectra. Our illustrations deal with the change in amplitude and phase of the intra-annual component of carbon dioxide concentrations in Mauna Loa, and with the estimation and the quantification of the contribution of orbital cycles to the variability of paleoclimate time series.",http://www.sciencedirect.com/science/article/pii/S0304407622000987
Journal of Econometrics,2024,Sieve bootstrap inference for linear time-varying coefficient models,Marina Friedrich and Yicong Lin,"We propose a sieve bootstrap framework to conduct pointwise and simultaneous inference for time-varying coefficient regression models based on a local linear estimator. The asymptotic validity of the sieve bootstrap in the presence of autocorrelation is established. The bootstrap automatically produces a consistent estimate of nuisance parameters, both at the interior and boundary points. In addition, we develop a bootstrap-based test for parameter constancy and examine its asymptotic properties. An extensive simulation study demonstrates a good finite sample performance of our methods. The proposed methods are applied to assess the price development of CO2 certificates in the European Emissions Trading System. We find evidence of time variation in the relationship between allowance prices and their fundamental price drivers. The time variation might offer an explanation for previous contradicting findings using linear regression models with constant coefficients.",http://www.sciencedirect.com/science/article/pii/S0304407622001701
Journal of Econometrics,2024,The validity of bootstrap testing for threshold autoregression,"Simone Giannerini, Greta Goracci and Anders Rahbek","We consider bootstrap-based testing for threshold effects in non-linear threshold autoregressive (TAR) models. It is well-known that classic tests based on asymptotic theory tend to be biased in case of small, or even moderate sample sizes, especially when the estimated parameters indicate non-stationarity, or in presence of heteroskedasticity, as often witnessed in the analysis of financial or climate data. To address the issue we propose a supremum Lagrange Multiplier test statistic (sLM), where the null hypothesis specifies a linear autoregressive (AR) model against the alternative of a TAR model. We consider both the classical recursive residual i.i.d. bootstrap (sLMi) and a wild bootstrap (sLMw), applied to the sLM statistic, and establish their validity under the null hypothesis. The framework is new, and requires the proof of non-standard results for bootstrap analysis in time series models; this includes a uniform bootstrap law of large numbers and a bootstrap functional central limit theorem. The Monte Carlo evidence shows that the bootstrap tests have correct empirical size even for small samples; the wild bootstrap version (sLMw) is also robust against the presence of heteroskedasticity. Moreover, there is no loss of empirical power when compared to the asymptotic test and the size of the tests is not affected if the order of the tested model is selected through AIC. Finally, we use our results to analyse the time series of the Greenland ice sheet mass balance. We find a significant threshold effect and an appropriate specification that manages to reproduce the main non-linear features of the series, such as the asymmetric seasonal cycle, the main periodicities, and the multimodality of the probability density function.",http://www.sciencedirect.com/science/article/pii/S0304407623000040
Journal of Econometrics,2024,Modelling circular time series,"Andrew Harvey, Stan Hurn, Dario Palumbo and Stephen Thiele","Circular variables often play an important role in the construction of models for analysing and forecasting the consequences of climate change and its impact on the environment. Such variables pose special problems for time series modelling. This article shows how the score-driven approach, developed primarily in econometrics, provides a natural solution to the difficulties and leads to a coherent and unified methodology for estimation, model selection and testing. The new methods are illustrated with data on wind direction.",http://www.sciencedirect.com/science/article/pii/S0304407623001446
Journal of Econometrics,2024,Common volatility shocks driven by the global carbon transition,Susana Campos-Martins and David Hendry,"We propose a novel approach to measure the global effects of climate change news on financial markets. For that purpose, we first calculate the global common volatility of the oil and gas industry. Then we project it on climate-related shocks constructed using text-based proxies of climate change news. We show that rising concerns about the energy transition make oil and gas share prices move at the global scale, controlling for shocks to the oil price, US and world stock markets. Despite the clear exposure of oil and gas companies to carbon transition risk, not all geoclimatic shocks are alike. The signs and magnitudes of the impacts differ across climate risk drivers. Regarding sentiment, climate change news tends to create turmoil only when the news is negative. Moreover, the adverse effect is amplified by oil price movements but weakened by stock market shocks. Finally, our findings point out climate news materialises when it reaches the global scale, supporting the relevance of modelling geoclimatic volatility.",http://www.sciencedirect.com/science/article/pii/S0304407623001665
Journal of Econometrics,2024,Long monthly temperature series and the Vector Seasonal Shifting Mean and Covariance Autoregressive model,"Changli He, Jian Kang, Annastiina Silvennoinen and Timo Teräsvirta","We consider a vector version of the Shifting Seasonal Mean Autoregressive model. The model is used for describing dynamic behaviour of and contemporaneous dependence between a number of long monthly temperature series for 20 cities in Europe, extending from the second half of the 18th century until mid-2010s. The results indicate strong warming in the winter months, February excluded, and cooling followed by warming during the summer months. Error variances are mostly constant over time, but for many series there is systematic decrease between 1820 and 1850 in April. Error correlations are considered by selecting two small sets of series and modelling correlations within these sets. Some correlations do change over time, but a large majority remains constant. Not surprisingly, the correlations generally decrease with the distance between cities, but the precise geographical location also plays a role.",http://www.sciencedirect.com/science/article/pii/S0304407623002105
Journal of Econometrics,2024,On model selection criteria for climate change impact studies,"Xiaomeng Cui, Bulat Gafarov, Dalia Ghanem and Todd Kuffner","Climate change impact studies inform policymakers on the estimated damages of future climate change on economic, health and other outcomes. In most studies, an annual outcome variable is observed, e.g. agricultural yield, along with a higher-frequency regressor, e.g. daily temperature. Applied researchers then face a problem of selecting a model to characterize the nonlinear relationship between the outcome and the high-frequency regressor to make a policy recommendation based on the model-implied damage function. We show that existing model selection criteria are only suitable for the policy objective if one of the models under consideration nests the true model. If all models are seen as imperfect approximations of the true nonlinear relationship, the model that performs well in the historical climate conditions is not guaranteed to perform well at the projected climate. We therefore propose a new criterion, the proximity-weighted mean squared error (PWMSE) that directly targets precision of the damage function at the projected future climate. To make this criterion feasible, we assign higher weights to historical years that can serve as “weather analogs” to the projected future climate when evaluating competing models using the PWMSE. We show that our approach selects the best approximate regression model that has the smallest weighted squared error of predicted impacts for a projected future climate. A simulation study and an application revisiting the impact of climate change on agricultural production illustrate the empirical relevance of our theoretical analysis.",http://www.sciencedirect.com/science/article/pii/S0304407623002270
Journal of Econometrics,2024,Sparse generalized Yule–Walker estimation for large spatio-temporal autoregressions with an application to NO2 satellite data,Hanno Reuvers and Etienne Wijler,"We consider a high-dimensional model in which variables are observed over time and space. The model consists of a spatio-temporal regression containing a time lag and a spatial lag of the dependent variable. Unlike classical spatial autoregressive models, we do not rely on a predetermined spatial interaction matrix, but infer all spatial interactions from the data. Assuming sparsity, we estimate the spatial and temporal dependence fully data-driven by penalizing a set of Yule–Walker equations. This regularization can be left unstructured, but we also propose customized shrinkage procedures when observations originate from spatial grids (e.g. satellite images). Finite sample error bounds are derived and estimation consistency is established in an asymptotic framework wherein the sample size and the number of spatial units diverge jointly. Exogenous variables can be included as well. A simulation exercise shows strong finite sample performance compared to competing procedures. As an empirical application, we model satellite measured nitrogen dioxide (NO2) concentrations in London. Our approach delivers forecast improvements over a competitive benchmark and we discover evidence for strong spatial interactions.",http://www.sciencedirect.com/science/article/pii/S0304407623002361
Journal of Econometrics,2024,Testing for coefficient distortion due to outliers with an application to the economic impacts of climate change,"Xiyu Jiao, Felix Pretis and Moritz Schwarz","Outlying observations can bias regression estimates, requiring the use of outlier-robust estimators. Comparing robust estimates to those obtained using ordinary least squares (OLS) is a common robustness check, however, such comparisons have been mostly informal due to the lack of available tests. Here we introduce a formal test for coefficient distortion due to outliers in regression models. Our proposed test is based on the difference between OLS and robust estimates obtained using a class of Huber-skip M-type estimators (such as Impulse Indicator Saturation or Robustified Least Squares). We show that our distortion test has an asymptotic chi-squared distribution by establishing the asymptotics of the corresponding Huber-skip M-estimators using an empirical process Central Limit Theorem recently developed in the literature. The test is valid for cross-sectional, as well as panel, and stationary or deterministically-trending time series models. To improve finite sample performance and to alleviate concerns on distributional assumptions, we explore several bootstrap testing schemes. We apply our outlier distortion test to estimates of the macro-economic impacts of climate change allowing for adaptation.",http://www.sciencedirect.com/science/article/pii/S0304407623002634
Journal of Econometrics,2024,"Reprint of: When will Arctic sea ice disappear? Projections of area, extent, thickness, and volume","Francis Diebold, Glenn Rudebusch, Maximilian Göbel, Philippe Goulet Coulombe and Boyuan Zhang","Rapidly diminishing Arctic summer sea ice is a strong signal of the pace of global climate change. We provide point, interval, and density forecasts for four measures of Arctic sea ice: area, extent, thickness, and volume. Importantly, we enforce the joint constraint that these measures must simultaneously arrive at an ice-free Arctic. We apply this constrained joint forecast procedure to models relating sea ice to atmospheric carbon dioxide concentration and models relating sea ice directly to time. The resulting “carbon-trend” and “time-trend” projections are mutually consistent and predict a nearly ice-free summer Arctic Ocean by the mid-2030s with an 80% probability. Moreover, the carbon-trend projections show that global adoption of a lower carbon path would likely delay the arrival of a seasonally ice-free Arctic by only a few years.",http://www.sciencedirect.com/science/article/pii/S0304407623003615
Journal of Econometrics,2024,A residual bootstrap for conditional Value-at-Risk,"Eric Beutner, Alexander Heinemann and Stephan Smeekes","A fixed-design residual bootstrap method is proposed for the two-step estimator of Francq and Zakoïan(2015) associated with the conditional Value-at-Risk. The bootstrap’s consistency is proven for a general class of volatility models and intervals are constructed for the conditional Value-at-Risk. A simulation study reveals that the equal-tailed percentile bootstrap interval tends to fall short of its nominal value. In contrast, the reversed-tails bootstrap interval yields accurate coverage. We also compare the theoretically analyzed fixed-design bootstrap with the recursive-design bootstrap. It turns out that the fixed-design bootstrap performs equally well in terms of average coverage, yet leads on average to shorter intervals in smaller samples. An empirical application illustrates the interval estimation.",http://www.sciencedirect.com/science/article/pii/S0304407623002701
Journal of Econometrics,2024,Profiling the plight of disconnected youth in America,"Thomas MaCurdy, David Glick, Sonam Sherpa and Sriniketh Nagavarapu","In a successful transition from youth to adulthood, individuals pass through a sequence of roles involving school, work, and family formation that culminate in their becoming self-sufficient adults. However, some “disconnected” youth spend extended periods of time outside of any role that constitutes an element of the pathway towards adult independence. Assisting these youth requires a systematic understanding of what “disconnection” means, how many disconnected youth there are, who these youth are, and how the scale of the problem has evolved over time. Using the National Longitudinal Surveys of Youth for 1997 and 1979, we address these issues by creating concrete definitions of “disconnection spells” using rich data on youths’ enrollment, work, and personal histories. We estimate a multi-state duration model to account for right censoring and to understand differences across salient sub-groups. Our estimates imply that in the early 2000s, almost 19% and 25% of young men and young women, respectively, experienced a disconnection spell by age 23 using our basic definition. These rates are substantially higher for certain sub-groups defined by race/ethnicity, parental education, and government aid receipt, rising as high as 30+% by age 23. Approximately 60% of youth with a disconnection spell have it last longer than a year, and close to 10% have it last longer than 4 years. However, once reconnected, a majority of youth go at least three years without a re-disconnection spell. Patterns of initial disconnection changed markedly from the 1980s to the 2000s, as young women saw a 12 percentage point decline over time. Moreover, the Black-White gap in disconnection has fallen for women, but increased for men. Our profile of disconnection experiences provides a starting point for government agencies aiming to understand where, how, and with whom to intervene to prevent lengthy disconnection spells.",http://www.sciencedirect.com/science/article/pii/S0304407623002737
Journal of Econometrics,2024,Sharp bounds in the latent index selection model,Philip Marx,"A fundamental question underlying the literature on partial identification is: what can we learn about parameters that are relevant for policy but not necessarily point-identified by the exogenous variation we observe? This paper provides an answer in terms of sharp, analytic characterizations and bounds for an important class of policy-relevant treatment effects, consisting of marginal treatment effects and linear functionals thereof, in the latent index selection model as formalized in Vytlacil (2002). The sharp bounds use the full content of identified marginal distributions, and analytic derivations rely on the theory of stochastic orders. The proposed methods also make it possible to sharply incorporate new auxiliary assumptions on distributions into the latent index selection framework. Empirically, I apply the methods to study the effects of Medicaid on emergency room utilization in the Oregon Health Insurance Experiment, showing that the predictions from extrapolations based on a distribution assumption (rank similarity) differ substantively and consistently from existing extrapolations based on a parametric mean assumption (linearity). This underscores the value of utilizing the model’s full empirical content in combination with auxiliary assumptions.",http://www.sciencedirect.com/science/article/pii/S0304407623002774
Journal of Econometrics,2024,An information–Theoretic approach to partially identified auction models,Sung Jae Jun and Joris Pinkse,"We consider a situation in which we have data from ascending auctions with symmetric bidders, independent private values, and exogenous entry in which the bidders’ value distribution is partially identified. Focusing on the case in which the seller intends to use a second price auction, we discuss how to determine an optimal reserve price. We justify the use of maximum entropy, explore the properties of the estimand, determine the asymptotic properties of our maximum entropy estimator, evaluate its behavior in a simulation study, and demonstrate its use in a modest application. As an extension, we propose a maxmin decision rule with entropy regularization, which includes Aryal and Kim (2013) and the maximum entropy solution as extreme cases.",http://www.sciencedirect.com/science/article/pii/S0304407623002828
Journal of Econometrics,2024,Identification and estimation of sequential games of incomplete information with multiple equilibria,Jangsu Yoon,"This paper discusses the identification and estimation of game-theoretic models, mainly focusing on sequential games of incomplete information. In most empirical games, researchers cannot observe the exact order of actions played in the game and rely on the assumption of simultaneous actions. My structural modeling generalizes an empirical game to encompass simultaneous and sequential actions as special cases. I specify a sequential game allowing for multiple players in each stage and multiple Perfect Bayesian Nash Equilibria, showing that the structural parameters, including the payoff function parameters, the order of actions, and the equilibrium selection mechanism, are separately identified. The various exclusion restrictions in the finite mixture literature help attain point identification of structural parameters and provide a testable method to verify identification conditions. Next, I consider a Sieve Minimum Distance (SMD) estimator of Ai and Chen (2003) for the structural parameters and verify its asymptotic properties. The Monte Carlo simulations evaluate the performance of the proposed estimator and provide numerical evidence of potential bias under the misspecified order of actions. The empirical application of Walmart and Kmart’s entry game demonstrates that retailers compete sequentially in a significant portion of markets.",http://www.sciencedirect.com/science/article/pii/S0304407623002853
Journal of Econometrics,2024,Unconditional effects of general policy interventions,"Julian Martinez-Iriarte, Gabriel Montes-Rojas and Yixiao Sun","This paper studies the unconditional effects of a general policy intervention, which includes location-scale shifts and simultaneous shifts as special cases. The location-scale shift is intended to study a counterfactual policy aimed at changing not only the mean or location of a covariate but also its dispersion or scale. The simultaneous shift refers to the situation where shifts in two or more covariates take place simultaneously. For example, a shift in one covariate is compensated at a certain rate by a shift in another covariate. Not accounting for these possible scale or simultaneous shifts will result in an incorrect assessment of the potential policy effects on an outcome variable of interest. The unconditional policy parameters are estimated with simple semiparametric estimators, for which asymptotic properties are studied. Monte Carlo simulations are implemented to study their finite sample performances. The proposed approach is applied to a Mincer equation to study the effects of changing years of education on wages and to study the effect of smoking during pregnancy on birth weight.",http://www.sciencedirect.com/science/article/pii/S0304407623002865
Journal of Econometrics,2024,Role models and revealed gender-specific costs of STEM in an extended Roy model of major choice,"Marc Henry, Romuald Méango and Ismaël Mourifié","We derive sharp bounds on the non consumption utility component in an extended Roy model of sector selection. We interpret this non consumption utility component as a compensating wage differential. The bounds are derived under the assumption that potential utilities in each sector are (jointly) stochastically monotone with respect to an observed selection shifter. The research is motivated by the analysis of women’s choice of university major, their under representation in mathematics intensive fields, and the impact of role models on choices and outcomes. To illustrate our methodology, we investigate the cost of STEM fields with data from a German graduate survey, and using the mother’s education level and the proportion of women on the STEM faculty at the time of major choice as selection shifters.",http://www.sciencedirect.com/science/article/pii/S0304407623002877
Journal of Econometrics,2024,Estimation of complier expected shortfall treatment effects with a binary instrumental variable,"Bo Wei, Kean Ming Tan and Xuming He","Estimating the causal effect of a treatment or exposure for a subpopulation is of great interest in many biomedical and economical studies. Expected shortfall, also referred to as the super-quantile, is an attractive effect-size measure that can accommodate data heterogeneity and aggregate local information of effect over a certain region of interest of the outcome distribution. In this article, we propose the ComplieRExpected Shortfall Treatment Effect (CRESTE) model under an instrumental variable framework to quantity the CRESTE for a binary endogenous treatment variable. By utilizing the special characteristics of a binary instrumental variable and a specific formulation of Neyman-orthogonalization, we propose a two-step estimation procedure, which can be implemented by simply solving weighted least-squares regression and weighted quantile regression with estimated weights. We develop the asymptotic properties for the proposed estimator and use numerical simulations to confirm its validity and robust finite-sample performance. An illustrative analysis of a National Job Training Partnership Act study is presented to show the practical utility of the proposed method.",http://www.sciencedirect.com/science/article/pii/S0304407623002889
Journal of Econometrics,2024,Nested Pseudo likelihood estimation of continuous-time dynamic discrete games,Jason Blevins and Minhae Kim,"We introduce a sequential estimator for continuous time dynamic discrete choice models (single-agent models and games) by adapting the nested pseudo likelihood (NPL) estimator of Aguirregabiria and Mira (2002, 2007), developed for discrete time models with discrete time data, to the continuous time case with data sampled either discretely (i.e., uniformly-spaced snapshot data) or continuously. We establish conditions for consistency and asymptotic normality of the estimator, a local convergence condition, and, for single agent models, a zero Jacobian property assuring local convergence. We carry out a series of Monte Carlo experiments using an entry-exit game with five heterogeneous firms to confirm the large-sample properties and demonstrate finite-sample bias reduction via iteration. In our simulations we show that the convergence issues documented for the NPL estimator in discrete time models are less likely to affect comparable continuous-time models. We also show that there can be large bias in economically-relevant parameters, such as the competitive effect and entry cost, from estimating a misspecified discrete time model when in fact the data generating process is a continuous time model.",http://www.sciencedirect.com/science/article/pii/S0304407623002920
Journal of Econometrics,2024,What leads to measurement errors? Evidence from reports of program participation in three surveys,"Pablo Celhay, Bruce D. Meyer and Nikolas Mittag","Measurement errors are often a large source of bias in survey data. Lack of knowledge of the determinants of such errors makes it difficult to reduce the extent of errors when collecting data and to assess the validity of analyses using the data. We study the determinants of reporting error using high quality administrative data on government transfers linked to three major U.S. surveys. Our results support several theories of misreporting: Errors are related to event recall, forward and backward telescoping, salience of receipt, the stigma of reporting participation in welfare programs and respondent's degree of cooperation with the survey overall. We provide evidence on how survey design choices affect reporting errors. Our findings help survey users to gauge the reliability of their data and to devise estimation strategies that can correct for systematic errors, such as instrumental variable approaches. Understanding survey errors allows researchers collecting survey data to reduce them by improving survey design. Our results indicate that survey design should take into account that higher response rates as well as collecting more detailed information may have negative effects on survey accuracy.",http://www.sciencedirect.com/science/article/pii/S030440762300297X
Journal of Econometrics,2024,Quantile analysis of “hazard-rate” game models,Andreea Enache and Jean-Pierre Florens,"This paper consists of an econometric analysis of a broad class of games of incomplete information. In these games, a player’s action depends both on her unobservable characteristic (the private information), as well as on the ratio of the distribution of the unobservable characteristic and its density function (which we call the ”hazard-rate”). The goal is to use data on players’ actions to recover the distribution of private information. We show that the structural parameter (the quantile of the unobservable characteristic, denoted by L) can be related to the reduced form parameter (the quantile of the data, denoted by Q) through a differential equation Q(α)=a(L(α),αL′(α)). We analyse the local and global identification of this equation in L. At the same time, under suitable assumptions, we establish the local and global well-posedness of the inverse problem Q=T(L). We solve the differential equation and, as a consequence of the well-posedness, we estimate nonparametrically the quantile and the c.d.f. function of the unobserved variables at a root-n speed of convergence (conditional on the quantile of the data being estimated at root-n). Moreover as the transformation of Q in L is continuous with respect to a topology that implies also the derivatives, we estimate nonparametrically the quantile density and the density the unobserved variables (that are continuous functions with respect to Q) at root-n speed of convergence. We provide also several generalisations of the model accounting for exogenuous variables, implicit expression of the strategy function and integral expression of the strategy function. Our results have several policy applications, including better design of auctions, regulation models, estimation of war of attrition in patent races, and public good contracts.",http://www.sciencedirect.com/science/article/pii/S0304407623002981
Journal of Econometrics,2024,A conditional linear combination test with many weak instruments,"Dennis Lim, Wenjie Wang and Yichong Zhang","We consider a linear combination of jackknife Anderson-Rubin (AR), jackknife Lagrangian multiplier (LM), and orthogonalized jackknife LM tests for inference in IV regressions with many weak instruments and heteroskedasticity. Following I.Andrews (2016), we choose the weights in the linear combination based on a decision-theoretic rule that is adaptive to the identification strength. Under both weak and strong identifications, the proposed test controls asymptotic size and is admissible among certain class of tests. Under strong identification, our linear combination test has optimal power against local alternatives among the class of invariant or unbiased tests which are constructed based on jackknife AR and LM tests. Simulations and an empirical application to Angrist and Krueger’s (1991) dataset confirm the good power properties of our test.",http://www.sciencedirect.com/science/article/pii/S0304407623003184
Journal of Econometrics,2024,Kolmogorov–Smirnov type testing for structural breaks: A new adjusted-range based self-normalization approach,"Yongmiao Hong, Oliver Linton, Brendan McCabe, Jiajing Sun and Shouyang Wang","A popular self-normalization (SN) approach in time series analysis uses the variance of a partial sum as a self-normalizer. This is known to be sensitive to irregularities such as persistent autocorrelation, heteroskedasticity, unit roots and outliers. We propose a novel SN approach based on the adjusted-range of a partial sum, which is robust to these aforementioned irregularities. We develop an adjusted-range based Kolmogorov–Smirnov type test for structural breaks for both univariate and multivariate time series, and consider testing parameter constancy in a time series regression setting. Our approach can rectify the well-known power decrease issue associated with existing self-normalized KS tests without having to use backward and forward summations as in Shao and Zhang (2010), and can alleviate the “better size but less power” phenomenon when the existing SN approaches (Shao, 2010; Zhang et al., 2011; Wang and Shao, 2022) are used. Moreover, our proposed tests can cater for more general alternatives. Monte Carlo simulations and empirical studies demonstrate the merits of our approach.",http://www.sciencedirect.com/science/article/pii/S0304407623003196
Journal of Econometrics,2024,An identification and testing strategy for proxy-SVARs with weak proxies,"Giovanni Angelini, Giuseppe Cavaliere and Luca Fanelli","When proxies (external instruments) used to identify target structural shocks are weak, inference in proxy-SVARs (SVAR-IVs) is nonstandard and the construction of asymptotically valid confidence sets for the impulse responses of interest requires weak-instrument robust methods. In the presence of multiple target shocks, test inversion techniques require extra restrictions on the proxy-SVAR parameters other than those implied by the proxies that may be difficult to interpret and test. We show that frequentist asymptotic inference in these situations can be conducted through Minimum Distance estimation and standard asymptotic methods if the proxy-SVAR can be identified by using ‘strong’ instruments for the non-target shocks; i.e., the shocks which are not of primary interest in the analysis. The suggested identification strategy hinges on a novel pre-test for the null of instrument relevance, based on bootstrap resampling, which is not subject to pre-testing issues. Specifically, the validity of post-test asymptotic inferences remains unaffected by the test outcomes due to an asymptotic independence result between the bootstrap and non-bootstrap statistics. The test is robust to conditionally heteroskedastic and/or zero-censored proxies, is computationally straightforward and applicable regardless of the number of shocks being instrumented. Some illustrative examples show the empirical usefulness of the suggested identification and testing strategy.",http://www.sciencedirect.com/science/article/pii/S0304407623003202
Journal of Econometrics,2024,Estimation and variable selection for high-dimensional spatial dynamic panel data models,"Li Hou, Baisuo Jin and Yuehua Wu","Spatiotemporal modeling of networks is of great practical importance, with modern applications in epidemiology and social network analysis. Despite rapid methodological advances, how to effectively and efficiently estimate the parameters of spatial dynamic panel models remains a challenging problem. To tackle this problem, we construct consistent complex least-squares estimators by the eigendecomposition of a spatial weight matrix method originally proposed for undirected networks. We no longer require all eigenvalues and eigenvectors to be real, which is a remarkable achievement as it implies that the proposed method is now applicable to spatiotemporal data modeling of directed networks. Under mild, interpretable conditions, we show that the proposed parameter estimators are consistent and asymptotically normally distributed. We also present a complex orthogonal greedy algorithm for variable selection and rigorously investigate its convergence properties. Moreover, we incorporate fixed effects into the spatial dynamic panel models and provide a model transformation so that the proposed method can also be applied to the transformed model. Extensive simulation studies and data examples demonstrate the effectiveness of the proposed method.",http://www.sciencedirect.com/science/article/pii/S0304407623003214
Journal of Econometrics,2024,Tail behavior of ACD models and consequences for likelihood-based estimation,"Giuseppe Cavaliere, Thomas Mikosch, Anders Rahbek and Frederik Vilandt","We establish new results for estimation and inference in financial durations models, where events are observed over a given time span, such as a trading day, or a week. For the classical autoregressive conditional duration (ACD) models by Engle and Russell (1998), we show that the large sample behavior of likelihood estimators is highly sensitive to the tail behavior of the financial durations. In particular, even under stationarity, asymptotic normality breaks down for tail indices smaller than one or, equivalently, when the clustering behavior of the observed events is such that the unconditional distribution of the durations has no finite mean. Instead, we find that estimators are mixed Gaussian and have non-standard rates of convergence. The results are based on exploiting the crucial fact that for duration data the number of observations within any given time span is random. Our results apply to general econometric models where the number of observed events is random.",http://www.sciencedirect.com/science/article/pii/S0304407623003299
Journal of Econometrics,2024,High-dimensional IV cointegration estimation and inference,Peter Phillips and Igor L. Kheifets,"A semiparametric triangular systems approach shows how multicointegrating linkages occur naturally in an I(1) cointegrated regression model when the long run error variance matrix in the system is singular. Under such singularity, cointegrated I(1) systems embody a multicointegrated structure that makes them useful in many empirical settings. Earlier work shows that such systems may be analyzed and estimated without appealing to the associated I(2) system but with suboptimal convergence rates and potential asymptotic bias. The present paper develops a robust approach to estimation and inference of such systems using high dimensional IV methods that have appealing asymptotic properties like those known to apply in the optimal estimation of cointegrated systems (Phillips, 1991). The approach uses an extended version of high-dimensional trend IV (Phillips, 2006, 2014) estimation with deterministic orthonormal instruments. The methods and derivations involve new results on high-dimensional IV techniques and matrix normalization in the limit theory that are of independent interest. Wald tests of general linear restrictions are constructed using a fixed-b long run variance estimator that leads to robust pivotal HAR inference in both cointegrated and multicointegrated cases. Simulations show good properties of the estimation and inferential procedures in finite samples. An empirical illustration to housing stocks, starts and completions is provided.",http://www.sciencedirect.com/science/article/pii/S030440762300338X
Journal of Econometrics,2024,The fixed-b limiting distribution and the ERP of HAR tests under nonstationarity,Alessandro Casini,"We show that the limiting distribution of HAR test statistics under fixed-b asymptotics is not pivotal when the data are nonstationary (i.e., time-varying autocovariance structure). It takes the form of a complicated function of Gaussian processes and depends on the second moments of the relevant series (e.g., of the regressors and errors for the case of the linear regression model). Hence, fixed-b inference methods based on stationarity are not theoretically valid in general. The nuisance parameters entering the fixed-b limiting distribution can be consistently estimated under small-b asymptotics but only with nonparametric rate of convergence. We show that the error in rejection probability (ERP) is an order of magnitude larger than that under stationarity and can be also larger than that of HAR tests based on HAC estimators under conventional asymptotics. These theoretical results reconcile with recent finite-sample evidence showing that existing fixed-b HAR tests can perform poorly when the data are nonstationary. They can be conservative under the null hypothesis and have non-monotonic power under the alternative hypothesis irrespective of how large the sample size is. Based on the new nonstationary fixed-b distribution, we propose a feasible method that controls the null rejection rates well regardless of whether the data are stationary or not and of the strength of the serial dependence as verified for some representative data-generating processes in a simple location model.",http://www.sciencedirect.com/science/article/pii/S030440762300341X
Journal of Econometrics,2024,Robust testing for explosive behavior with strongly dependent errors,"Yiu Lim Lui, Peter Phillips and Jun Yu",A heteroskedasticity-autocorrelation robust (HAR) test statistic is proposed to test for the presence of explosive roots in financial or real asset prices when the equation errors are strongly dependent. Limit theory for the test statistic is developed and extended to heteroskedastic models. The new test has stable size properties unlike conventional test statistics that typically lead to size distortion and inconsistency in the presence of strongly dependent equation errors. The new procedure can be used to consistently time-stamp the origination and termination of an explosive episode under similar conditions of long memory errors. Simulations are conducted to assess the finite sample performance of the proposed test and estimators. An empirical application to the S&P 500 index highlights the usefulness of the proposed procedures in practical work.,http://www.sciencedirect.com/science/article/pii/S0304407623003421
Journal of Econometrics,2024,Distributed estimation and inference for spatial autoregression model with large scale networks,"Yimeng Ren, Zhe Li, Xuening Zhu, Yuan Gao and Hansheng Wang","The rapid growth of online network platforms generates large-scale network data and it poses great challenges for statistical analysis using the spatial autoregression (SAR) model. In this work, we develop a novel distributed estimation and statistical inference framework for the SAR model on a distributed system. We first propose a distributed network least squares approximation (DNLSA) method. This enables us to obtain a one-step estimator by taking a weighted average of local estimators on each worker. Afterwards, a refined two-step estimation is designed to further reduce the estimation bias. For statistical inference, we utilize a random projection method to reduce the expensive communication cost. Theoretically, we show the consistency and asymptotic normality of both the one-step and two-step estimators. In addition, we provide theoretical guarantee of the distributed statistical inference procedure. The theoretical findings and computational advantages are validated by several numerical simulations implemented on the Spark system. Lastly, an experiment on the Yelp dataset further illustrates the usefulness of the proposed methodology.",http://www.sciencedirect.com/science/article/pii/S0304407623003457
Journal of Econometrics,2024,Autoregressive conditional betas,"F. Blasques, Christian Francq and Sébastien Laurent","This paper introduces an autoregressive conditional beta (ACB) model that allows regressions with dynamic betas (or slope coefficients) and residuals with GARCH conditional volatility. The model fits in the (quasi) score-driven approach recently proposed in the literature, and it is semi-parametric in the sense that the distributions of the innovations are not necessarily specified. The time-varying betas are allowed to depend on past shocks and exogenous variables. We establish the existence of a stationary solution for the ACB model, the invertibility of the score-driven filter for the time-varying betas, and the asymptotic properties of one-step and multistep QMLEs for the new ACB model. The finite sample properties of these estimators are studied by means of an extensive Monte Carlo study. Finally, we also propose a strategy to test for the constancy of the conditional betas. In a financial application, we find evidence for time-varying conditional betas and highlight the empirical relevance of the ACB model in a portfolio and risk management empirical exercise.",http://www.sciencedirect.com/science/article/pii/S0304407623003469
Journal of Econometrics,2024,The likelihood ratio test for structural changes in factor models,"Jushan Bai, Jiangtao Duan and Xu Han","A factor model with a break in its factor loadings is observationally equivalent to a model without changes in the loadings but with a change in the variance of its factors. This approach effectively transforms a high-dimensional structural change problem into a low-dimensional problem. This paper considers the likelihood ratio (LR) test for a variance change in the estimated factors. The LR test implicitly explores a special feature of the estimated factors: the pre-break and post-break variances can be a singular matrix under the alternative hypothesis, making the LR test diverging faster and thus more powerful than Wald-type tests. The better power property of the LR test is also confirmed by simulations. We also consider mean changes and multiple breaks. We apply this procedure to the factor modeling of the US employment and study the structural change problem using monthly industry-level data.",http://www.sciencedirect.com/science/article/pii/S0304407623003470
Journal of Econometrics,2024,Bellman filtering and smoothing for state–space models,Rutger-Jan Lange,"This paper presents a new filter for state–space models based on Bellman’s dynamic-programming principle, allowing for nonlinearity, non-Gaussianity and degeneracy in the observation and/or state-transition equations. The resulting Bellman filter is a direct generalisation of the (iterated and extended) Kalman filter, enabling scalability to higher dimensions while remaining computationally inexpensive. It can also be extended to enable smoothing. Under suitable conditions, the Bellman-filtered states are stable over time and contractive towards a region around the true state at every time step. Static (hyper)parameters are estimated by maximising a filter-implied pseudo log-likelihood decomposition. In univariate simulation studies, the Bellman filter performs on par with state-of-the-art simulation-based techniques at a fraction of the computational cost. In two empirical applications, involving up to 150 spatial dimensions or highly degenerate/nonlinear state dynamics, the Bellman filter outperforms competing methods in both accuracy and speed.",http://www.sciencedirect.com/science/article/pii/S0304407623003482
Journal of Econometrics,2024,Advances in nowcasting economic activity: The role of heterogeneous dynamics and fat tails,"Juan Antolín-Díaz, Thomas Drechsel and Ivan Petrella","A key question for households, firms, and policy makers is: how is the economy doing now? This paper develops a Bayesian dynamic factor model that allows for nonlinearities, heterogeneous lead–lag patterns and fat tails in macroeconomic data. Explicitly modeling these features changes the way that different indicators contribute to the real-time assessment of the state of the economy, and substantially improves the out-of-sample performance of this class of models. In a formal evaluation, our nowcasting framework beats benchmark econometric models and professional forecasters at predicting US GDP growth in real time.",http://www.sciencedirect.com/science/article/pii/S0304407623003500
Journal of Econometrics,2024,Observation-driven filtering of time-varying parameters using moment conditions,"Drew Creal, Siem Jan Koopman, Andre Lucas and Marcin Zamojski","We develop a new and flexible semi-parametric approach for time-varying parameter models when the true dynamics are unknown. The time-varying parameters are estimated using a recursive updating scheme that is driven by the influence function of a conditional moments-based criterion. We show that the updates ensure local improvements of the conditional criterion function in expectation. The dynamics are observation driven, which yields a computationally efficient methodology that does not require advanced simulation techniques for estimation. We illustrate the new approach using both simulated and real empirical data and derive new, robust filters for time-varying scales based on characteristic functions.",http://www.sciencedirect.com/science/article/pii/S0304407623003512
Journal of Econometrics,2024,Identification of heterogeneous elasticities in gross-output production functions,Tong Li and Yuya Sasaki,"This paper presents the identification of heterogeneous elasticities in gross-output production functions with non-separable unobserved productivity. We propose that the ex-ante flexible input cost shares identify the heterogeneous output elasticities with respect to flexible inputs for each firm. Applying the proposed method to a panel of firms in the food production industry in Chile, we find that the extent of heterogeneity in the output elasticities with respect to intermediate inputs has significantly declined from 1985 to 1995, with its distribution tending to concentrate toward zero as time progresses.",http://www.sciencedirect.com/science/article/pii/S0304407623003536
Journal of Econometrics,2024,Estimation and inference by stochastic optimization,Jean-Jacques Forneron,"In non-linear estimations, it is common to assess sampling uncertainty by bootstrap inference. For complex models, this can be computationally intensive. This paper combines optimization with resampling: turning stochastic optimization into a fast resampling device. Two methods are introduced: a resampled Newton–Raphson (rnr) and a resampled quasi-Newton (rqn) algorithm. Both produce draws that can be used to compute consistent estimates, confidence intervals, and standard errors in a single run. The draws are generated by a gradient and Hessian (or an approximation) computed from batches of data that are resampled at each iteration. The proposed methods transition quickly from optimization to resampling when the objective is smooth and strictly convex. Simulated and empirical applications illustrate the properties of the methods on large scale and computationally intensive problems. Comparisons with frequentist and Bayesian methods highlight the features of the algorithms.",http://www.sciencedirect.com/science/article/pii/S0304407623003548
Journal of Econometrics,2024,Nonparametric estimation of stochastic frontier models with weak separability,Samuele Centorrino and Christopher F. Parmeter,"We propose a robust and versatile approach to estimate the stochastic frontier model which avoids parametric assumptions. Our approach requires a single continuous covariate which monotonically influences the conditional mean of inefficiency. Subject to these conditions, the frontier and the conditional mean of inefficiency can be estimated nonparametrically. The estimator we propose uses local least squares and marginal integration making it easy to implement across statistical software. A range of Monte Carlo simulations suggests that when our main identification condition holds, our proposed estimator outperforms other proposals that currently exist. Finally, we provide an application to the study of undercounting COVID-19 cases across the United States. Whereas our method indicates significant undercounting, consistent with existing evidence, other nonparametric methods suggest far less undercounting.",http://www.sciencedirect.com/science/article/pii/S0304407623003573
Journal of Econometrics,2024,Semiparametric Bayesian estimation of dynamic discrete choice models,Andriy Norets and Kenichi Shimizu,"We propose a tractable semiparametric estimation method for structural dynamic discrete choice models. The distribution of additive utility shocks in the proposed framework is modeled by location-scale mixtures of extreme value distributions with varying numbers of mixture components. Our approach exploits the analytical tractability of extreme value distributions in the multinomial choice settings and the flexibility of the location-scale mixtures. We implement the Bayesian approach to inference using Hamiltonian Monte Carlo and an approximately optimal reversible jump algorithm. In our simulation experiments, we show that the standard dynamic logit model can deliver misleading results, especially about counterfactuals, when the shocks are not extreme value distributed. Our semiparametric approach delivers reliable inference in these settings. We develop theoretical results on approximations by location-scale mixtures in an appropriate distance and posterior concentration of the set identified utility parameters and the distribution of shocks in the model.",http://www.sciencedirect.com/science/article/pii/S0304407623003585
Journal of Econometrics,2024,Systematic staleness,"Federico M. Bandi, Davide Pirino and Roberto Renò","Asset prices are stale. We define a measure of systematic (market-wide) staleness as the percentage of small price adjustments across multiple assets. A notion of idiosyncratic (asset-specific) staleness is also established. For both systematic and idiosyncratic staleness, we provide a limit theory based on joint asymptotics relying on increasingly-frequent observations over a fixed time span and an increasing number of assets. Using systematic and idiosyncratic staleness as moment conditions, we introduce novel structural estimates of systematic and idiosyncratic measures of liquidity obtained from transaction prices only. The economic signal contained in the structural estimates is assessed by virtue of suitable metrics.",http://www.sciencedirect.com/science/article/pii/S0304407623002385
Journal of Econometrics,2024,Hypothesis testing on high dimensional quantile regression,"Zhao Chen, Vivian Xinyi Cheng and Xu Liu","Quantile regression has been an important analytical tool in econometrics since it was proposed in 1970s. Many advantages make it still popular in the era of big data. This paper focuses on the testing problems of high-dimensional quantile regression, supplementing to robust methods in the literature of high-dimensional hypothesis testing. We first construct a new test statistic based on the quantile regression score function. The new test statistic avoids the inverse operation of the covariance matrix, and hence becomes applicable to high-dimensional or even ultrahigh-dimensional settings. The proposed method retains robustness for non-Gaussian and heavy-tailed distributions. We then derive the limiting distributions of the proposed test statistic under both the null and the alternative hypotheses. We further investigate the case where the design matrix follows an elliptical distribution. We examined the finite sample performance of our proposed method through Monte Carlo simulations. The numerical comparisons exhibit that our proposed tests outperform some existing methods in terms of controlling Type I error rate and power, when the data deviate from the Gaussian assumptions or are heavy-tailed. We illustrate our proposed high-dimensional quantile testing in financial econometrics, through an empirical analysis of Chinese stock market data.",http://www.sciencedirect.com/science/article/pii/S0304407623002415
Journal of Econometrics,2024,High-dimensional low-rank tensor autoregressive time series modeling,"Di Wang, Yao Zheng and Guodong Li","Modern technological advances have enabled an unprecedented amount of structured data with complex temporal dependence, urging the need for new methods to efficiently model and forecast high-dimensional tensor-valued time series. This paper provides a new modeling framework to accomplish this task via autoregression (AR). By considering a low-rank Tucker decomposition for the transition tensor, the proposed tensor AR can flexibly capture the underlying low-dimensional tensor dynamics, providing both substantial dimension reduction and meaningful multi-dimensional dynamic factor interpretations. For this model, we first study several nuclear-norm-regularized estimation methods and derive their non-asymptotic properties under the approximate low-rank setting. In particular, by leveraging the special balanced structure of the transition tensor, a novel convex regularization approach based on the sum of nuclear norms of square matricizations is proposed to efficiently encourage low-rankness of the coefficient tensor. To further improve the estimation efficiency under exact low-rankness, a non-convex estimator is proposed with a gradient descent algorithm, and its computational and statistical convergence guarantees are established. Simulation studies and an empirical analysis of tensor-valued time series data from multi-category import-export networks demonstrate the advantages of the proposed approach.",http://www.sciencedirect.com/science/article/pii/S0304407623002609
Journal of Econometrics,2024,Simultaneously Incomplete and Incoherent (SII) Dynamic LDV Models: With an Application to Financing Constraints and Firms’ Decision to Innovate,Vassilis Hajivassiliou and Frédérique Savignac,"We develop novel methods for establishing coherency and completeness conditions in Static and Dynamic Limited Dependent Variables (LDV) Models. We characterize the two distinct problems as “empty-region” incoherency and “overlap-region” incoherency or incompleteness and show that the two properties can co-exist. We focus on the class of models that can be Simultaneously Incomplete and Incoherent (SII). We propose estimation strategies based on Conditional Maximum Likelihood Estimation (CMLE) for simultaneous dynamic LDV models without imposing recursivity. Point identification is achieved through sign-restrictions on parameters or other prior assumptions that complete the underlying data process. Using as modelling framework the Panel Bivariate Probit model with State Dependence, we analyse the impact of financing constraints on innovation: ceteris paribus, a firm facing binding finance constraints is substantially less likely to undertake innovation, while the probability that a firm encounters a binding finance constraint more than doubles if the firm is innovative. In addition, a strong role for state dependence in dynamic versions of our models is established.",http://www.sciencedirect.com/science/article/pii/S0304407623002622
Journal of Econometrics,2024,Optimal nonparametric range-based volatility estimation,"Tim Bollerslev, Jia Li and Qiyuan Li","We present a general framework for optimal nonparametric spot volatility estimation based on intraday range data, comprised of the first, highest, lowest, and last price over a given time-interval. We rely on a decision-theoretic approach together with a coupling-type argument to directly tailor the form of the nonparametric estimator to the specific volatility measure of interest and relevant loss function. The resulting new optimal estimators offer substantial efficiency gains compared to existing commonly used range-based procedures.",http://www.sciencedirect.com/science/article/pii/S0304407623002646
Journal of Econometrics,2024,Local linearization based subvector inference in moment inequality models,Xinyue Bei,"This paper introduces a bootstrap-based profiling inference method for subvectors in moment inequality models following insights from Bugni et al. (2017). Compared to their paper, the new method calculates the critical value by searching over a local neighborhood of a pre-estimator, instead of the whole null parameter space, to profile out nuisance parameters. In this way, non-linear moment conditions are simplified by linear expansion and the bootstrap iterates over quadratic programming problems, which significantly simplifies and accelerates computation. This method controls asymptotic size uniformly over a large class of data generating processes. In the Monte Carlo simulations, the new procedure improves upon the computing time of Bugni et al. (2017) and Kaido et al. (2019) significantly. I provide an empirical illustration estimating an airline entry game.",http://www.sciencedirect.com/science/article/pii/S0304407623002658
Journal of Econometrics,2024,"Binary choice with misclassification and social interactions, with an application to peer effects in attitude",Zhongjian Lin and Yingyao Hu,"The interaction of economic agents is one of the most important elements in economic analyses. Social interactions on subjective outcomes, behavior, or decisions, are inherently difficult to identify and estimate because these variables are prone to misclassification errors. This paper puts forth a binary choice model with misclassification and social interactions to rectify the misclassification problems in social interactions studies. We achieve the identification of the conditional choice probability of the latent dependent variable by the technique of repeated measurements and a monotonicity condition. We construct the complete likelihood function from the two repeated measurements and propose a nested pseudo likelihood algorithm for estimation. Consistency and asymptotic normality results are shown for the proposed estimation method. We illustrate the finite sample performance of the model and the estimation method by three Monte Carlo experiments and an application to the study of peer effects among students in their attitudes towards learning.",http://www.sciencedirect.com/science/article/pii/S0304407623002671
Journal of Econometrics,2024,Detecting identification failure in moment condition models,Jean-Jacques Forneron,"This paper develops an approach to detect identification failure in moment condition models. This is achieved by introducing a quasi-Jacobian matrix computed as the slope of a linear approximation of the moments on an estimate of the identified set. It is asymptotically singular when local and/or global identification fails, and equivalent to the usual Jacobian matrix which has full rank when the model is globally and locally identified. Building on this property, a simple test with chi-squared critical values is introduced to conduct subvector inferences allowing for strong, semi-strong, and weak identification without a priori knowledge about the underlying identification structure. Monte-Carlo simulations and an empirical application to the Long-Run Risks model illustrate the results.",http://www.sciencedirect.com/science/article/pii/S0304407623002683
Journal of Econometrics,2024,Inference in models with partially identified control functions,Andres Aradillas-Lopez,"In multiple contributions to the literature, James L. Powell and coauthors have developed estimators for semiparametric models where sample selectivity and/or endogeneity can be handled through a “control function”. Their methods rely on pairwise comparisons of observations which match (asymptotically) the control functions. Conditional on this matching, a moment condition can identify the parameters of the model. However, there exist instances where the control functions are unobserved, but we have bounds for them which depend on observable covariates. These bounds can arise directly from the nature of the data available (e.g, with interval data), or they can be derived from an economic model. The inability to observe the control functions precludes the matching proposed in Powell’s methods. In this paper we show that, under certain conditions, testable implications can still be obtained through pairwise comparisons of observations for which the control-function bounds are disjoint. Testable implications now take the form of pairwise functional inequalities. We propose an inferential procedure based on these pairwise inequalities and we analyze its properties.",http://www.sciencedirect.com/science/article/pii/S0304407623002695
Journal of Econometrics,2024,Causal inference of general treatment effects using neural networks with a diverging number of confounders,"Xiaohong Chen, Ying Liu, Shujie Ma and Zheng Zhang","Semiparametric efficient estimation of various multi-valued causal effects, including quantile treatment effects, is important in economic, biomedical, and other social sciences. Under the unconfoundedness condition, adjustment for confounders requires estimating the nuisance functions relating outcome or treatment to confounders nonparametrically. This paper considers a generalized optimization framework for efficient estimation of general treatment effects using artificial neural networks (ANNs) to approximate the unknown nuisance function of growing-dimensional confounders. We establish a new approximation error bound for the ANNs to the nuisance function belonging to a mixed smoothness class without a known sparsity structure. We show that the ANNs can alleviate the “curse of dimensionality” under this circumstance. We establish the root-n consistency and asymptotic normality of the proposed general treatment effects estimators, and apply a weighted bootstrap procedure for conducting inference. The proposed methods are illustrated via simulation studies and a real data application.",http://www.sciencedirect.com/science/article/pii/S0304407623002713
Journal of Econometrics,2024,Bounding program benefits when participation is misreported,Denni Tommasi and Lina Zhang,"Instrumental variables (IV) are commonly used to estimate treatment effects in case of noncompliance. However, program participation is often misreported in survey data and standard techniques are not sufficient to point identify and consistently estimate the effects of interest. In this paper, we show that the identifiable IV estimand that ignores treatment misclassification is a weighted average of local average treatment effects with weights that can also be negative. This is troublesome because it may fail to deliver a correct causal interpretation, and this is true even if all the weights are non-negative. Therefore, we provide three IV strategies to bound the program benefits when both noncompliance and misreporting are present. We demonstrate the gain of identification power achieved by leveraging multiple exogenous variations when discrete or multiple-discrete IVs are available. At last, we use our new Stata command, ivbounds, to study the benefits of participating in the 401(k) pension plan on savings.",http://www.sciencedirect.com/science/article/pii/S0304407623002725
Journal of Econometrics,2024,Estimation and bootstrapping under spatiotemporal models with unobserved heterogeneity,"Xingdong Feng, Wenyu Li and Qianqian Zhu","Proposed herein is a novel spatiotemporal model to characterize the unobserved heterogeneity across individuals using quantile-function-based correlated random effects and heteroscedastic innovations in a general framework. This model can be used to explore the influence of space-specific factors on latent effects at different quantile levels by controlling for spatiotemporal effects. A two-stage estimation procedure is introduced in which (i) the method of moments is used to estimate spatiotemporal effects then (ii) quantile regression is used for individual effects. A hybrid double bootstrapping procedure is then proposed to approximate the asymptotic distributions of coefficient estimators. The validity of the estimation and bootstrapping is established theoretically and then confirmed by simulation studies, and the usefulness of the proposed model is demonstrated with a real example involving city air quality.",http://www.sciencedirect.com/science/article/pii/S0304407623002750
Journal of Econometrics,2024,Nonparametric Gini-Frisch bounds,Karim Chalak,"The Gini-Frisch bounds partially identify the constant slope coefficient in a linear equation when the explanatory variable suffers from classical measurement error. This paper generalizes these quintessential bounds to accommodate nonparametric heterogeneous effects. It provides suitable conditions under which the main insights that underlie the Gini-Frisch bounds apply to partially identify the average marginal effect of an error-laden variable in a nonparametric nonseparable equation. To this end, the paper puts forward a nonparametric analogue to the standard “forward” and “reverse” linear regression bounds. The nonparametric forward regression bound generalizes the linear regression “attenuation bias” due to classical measurement error.",http://www.sciencedirect.com/science/article/pii/S0304407623002762
Journal of Econometrics,2024,Identification of multi-valued treatment effects with unobserved heterogeneity,Koki Fusejima,"In this paper, we establish sufficient conditions for identifying treatment effects on continuous outcomes in endogenous and multi-valued discrete treatment settings with unobserved heterogeneity. We employ the monotonicity assumption for multi-valued discrete treatments and instruments, and our identification condition has a clear economic interpretation. In addition, we identify the local treatment effects in multi-valued treatment settings and derive closed-form expressions of the identified treatment effects. We provide examples to illustrate the usefulness of our result.",http://www.sciencedirect.com/science/article/pii/S0304407623002798
Journal of Econometrics,2024,Population interference in panel experiments,"Kevin Han, Guillaume Basse and Iavor Bojinov","The phenomenon of population interference, where a treatment assigned to one experimental unit affects another experimental unit’s outcome, has received considerable attention in standard randomized experiments. The complications produced by population interference in this setting are now readily recognized, and partial remedies are well known. Less understood is the impact of population interference in panel experiments where treatment is sequentially randomized in the population, and the outcomes are observed at each time step. This paper proposes a general framework for studying population interference in panel experiments and presents new finite population estimation and inference results. Our findings suggest that, under mild assumptions, the addition of a temporal dimension to an experiment alleviates some of the challenges of population interference for certain estimands. In contrast, we show that the presence of carryover effects — that is, when past treatments may affect future outcomes — exacerbates the problem. Our results are illustrated through both an empirical analysis and an extensive simulation study.",http://www.sciencedirect.com/science/article/pii/S0304407623002816
Journal of Econometrics,2024,Endogeneity in weakly separable models without monotonicity,"Songnian Chen, Shakeeb Khan and Xun Tang","We identify and estimate treatment effects when potential outcomes are weakly separable with a binary endogenous treatment. Vytlacil and Yildiz (2007) proposed an identification strategy that exploits the mean of observed outcomes, but their approach requires a monotonicity condition. In comparison, we exploit full information in the entire outcome distribution, instead of just its mean. As a result, our method does not require monotonicity and is also applicable to general settings with multiple indices. We provide examples where our approach can identify treatment effect parameters of interest whereas existing methods would fail. These include models where potential outcomes depend on multiple unobserved disturbance terms, such as a Roy model, a multinomial choice model, as well as a model with endogenous random coefficients. We establish consistency and asymptotic normality of our estimators.",http://www.sciencedirect.com/science/article/pii/S030440762300283X
Journal of Econometrics,2024,Tuning parameter-free nonparametric density estimation from tabulated summary data,"Ji Hyung Lee, Yuya Sasaki, Alexis Akira Toda and Yulong Wang","Administrative data are often easier to access as tabulated summaries than in the original format due to confidentiality concerns. Motivated by this practical feature, we propose a novel nonparametric density estimation method from tabulated summary data based on maximum entropy and prove its strong uniform consistency. Unlike existing kernel-based estimators, our estimator is free from tuning parameters and admits a closed-form density that is convenient for post-estimation analysis. We apply the proposed method to the tabulated summary data of the U.S. tax returns to estimate the income distribution.",http://www.sciencedirect.com/science/article/pii/S0304407623002841
Journal of Econometrics,2024,Asset pricing with neural networks: Significance tests,"Hasan Fallahgoul, Vincentius Franstianto and Xin Lin","This study proposes a novel hypothesis test for evaluating the statistical significance of input variables in multi-layer perceptron (MLP) regression models. Theoretical foundations are established through consistency results and estimation rate analysis using the sieves method. To validate the test’s performance in complex and realistic settings, an extensive Monte Carlo simulation is conducted. Results of the simulation reveal that the test has a high power and low rate of false positives, making it a powerful tool for detecting true effects in data. The test is further applied to identify the most influential predictors of equity risk premiums, with results indicating that only a small number of characteristics have statistical significance and all macroeconomic predictors are insignificant at the 1% level. These findings are consistent across a variety of neural network architectures.",http://www.sciencedirect.com/science/article/pii/S0304407623002907
Journal of Econometrics,2024,Maximum Likelihood Estimation for Non-Stationary Location Models with Mixture of Normal Distributions,"Francisco Blasques, Janneke van Brummelen, Paolo Gorgi and Siem Jan Koopman",We consider an observation-driven location model where the unobserved location variable is modeled as a random walk process and where the error variable is from a mixture of normal distributions. The time-varying location can be extended with a stationary process to account for cyclical and/or higher order autocorrelation. The mixed normal distribution can accurately approximate many continuous error distributions. We obtain a flexible modeling framework for the robust filtering and forecasting based on time-series models with non-stationary and nonlinear features. We provide sufficient conditions for strong consistency and asymptotic normality of the maximum likelihood estimator of the parameter vector in the specified model. The asymptotic properties are valid under correct model specification and can be generalized to allow for potential misspecification of the model. A simulation study is carried out to monitor the forecast accuracy improvements when extra mixture components are added to the model. In an empirical study we show that our approach is able to outperform alternative observation-driven location models in forecast accuracy for a time-series of electricity spot prices.,http://www.sciencedirect.com/science/article/pii/S0304407623002919
Journal of Econometrics,2024,Semi-parametric single-index predictive regression models with cointegrated regressors,"Weilun Zhou, Jiti Gao, David Harris and Hsein Kew","This paper considers the estimation of a semi-parametric single-index regression model that allows for nonlinear predictive relationships. This model is useful for predicting financial asset returns, whose observed behaviour resembles a stationary process, if the multiple nonstationary predictors are cointegrated. The presence of cointegrated regressors imposes a single-index structure in the model, and this structure not only balances the nonstationarity properties of the multiple predictors with the stationarity properties of asset returns but also avoids the curse of dimensionality associated with nonparametric regression function estimation. An orthogonal series expansion is used to approximate the unknown link function for the single-index component. We consider the constrained nonlinear least squares estimator of the single-index (or the cointegrating) parameters and the plug-in estimator of the link function, and derive their asymptotic properties. In an empirical application, we find some evidence of in-sample nonlinear predictability of U.S. stock returns using cointegrated predictors. We also find that the single-index model in general produces better out-of-sample forecasts than both the historical average benchmark and the linear predictive regression model.",http://www.sciencedirect.com/science/article/pii/S0304407623002932
Journal of Econometrics,2024,Rank-based max-sum tests for mutual independence of high-dimensional random vectors,"Hongfei Wang, Binghui Liu, Long Feng and Yanyuan Ma","We consider the problem of testing mutual independence of high-dimensional random vectors, and propose a series of high-dimensional rank-based max-sum tests, which are suitable for high-dimensional data and can be robust to distribution types of the variables, form of the dependence between variables and the sparsity of correlation coefficients. Further, we demonstrate the application of some representative members of the proposed tests on testing cross-sectional independence of the error vectors under fixed effects panel data regression models. We establish the asymptotic properties of the proposed tests under the null and alternative hypotheses, respectively, and then demonstrate the superiority of the proposed tests through extensive simulations, which suggest that they combine the advantages of both the max-type and sum-type high-dimensional rank-based tests. Finally, a real panel data analysis is performed to illustrate the application of the proposed tests.",http://www.sciencedirect.com/science/article/pii/S0304407623002944
Journal of Econometrics,2024,Matching points: Supplementing instruments with covariates in triangular models,Junlong Feng,"Models with a discrete endogenous variable and an instrument that takes on fewer values are common in economics. This paper presents a new method that matches pairs of covariates and instruments to restore the order condition in this scenario and to achieve point-identification of the outcome function. The outcome function must be monotonic in a scalar disturbance, but it can be nonseparable. The first stage for the discrete endogenous variable needs to have a multi-index structure but allows for multidimensional heterogeneity. This paper also provides estimators of the outcome function. Two empirical examples of the return to education and of selection into Head Start illustrate the usefulness and limitations of the method.",http://www.sciencedirect.com/science/article/pii/S0304407623002956
Journal of Econometrics,2024,"Mental health and abortions among young women: time-varying unobserved heterogeneity, health behaviors, and risky decisions",Lena Janys and Bettina Siflinger,"In this paper, we provide causal evidence on abortions and risky health behaviors as determinants of mental health development among young women. Using administrative in- and outpatient records from Sweden, we apply a novel grouped fixed-effects estimator proposed by Bonhomme and Manresa (2015) to allow for time-varying unobserved heterogeneity. We show that the positive association obtained from standard estimators shrinks to zero once we control for grouped unobserved heterogeneity that varies across ages. We estimate the group profiles of unobserved heterogeneity, which reflect differences in unobserved risk to be diagnosed with a mental health condition. In addition, we analyze mental health development and risky health behaviors other than unintended pregnancies across groups. Our results suggest that these are determined by the same type of unobserved heterogeneity, which we attribute to the same unobserved process of decision-making. We develop and estimate a theoretical model of risky choices and mental health that is consistent with our empirical results. In the model, mental health disparity across groups is generated by different degrees of self-control problems. Our findings imply that mental health concerns cannot be used to justify restrictive abortion policies.",http://www.sciencedirect.com/science/article/pii/S0304407623002968
